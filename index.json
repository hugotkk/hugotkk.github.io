[{"content":"","date":"2023-12-04","permalink":"/","section":"Hugo's IT journal","summary":"","title":"Hugo's IT journal"},{"content":"","date":"2023-12-04","permalink":"/tags/hypervisor/","section":"Tags","summary":"","title":"hypervisor"},{"content":"","date":"2023-12-04","permalink":"/tags/kubernetes/","section":"Tags","summary":"","title":"kubernetes"},{"content":"","date":"2023-12-04","permalink":"/tags/kvm/","section":"Tags","summary":"","title":"kvm"},{"content":"Purpose of this note: The existing online tutorials provide limited information about KVM.\nMany tutorials reference the use of virt-manager, which is akin to a GUI tool for creating virtual machines.\nHowever, my interest lies in a deeper understanding of KVM, which these tutorials do not adequately cover.\nI would like to begin with\na high-level overview break down KVM into different aspects and then combine all the details into a lab Setup Specification\u003e Setup Specification # Regarding the upcoming commands/guide, I am using Ubuntu 22.04 as my host operating system, and the guest OS is openSUSE 15.6.\nObjectives\u003e Objectives # Focus primarily on QEMU, though insights into libvirt are also valuable, as its underlying configurations can aid in verifying settings. Gain an understanding of the differences and uses of serial vs. monitor in QEMU, as well as the console. Learn about VNC and SPICE in QEMU. Comprehend networking aspects within KVM. Methodology\u003e Methodology # Creating VMs with KVM can be approached in two ways:\nLibvirt - Higher Level:\nUtilizes the libvirtd daemon, allowing VM and network definitions (in XML) to persist and enabling autostart on boot. The default bridge network default comes with bridge device named virbr0. QEMU - Lower Level:\nOperates solely through the command line. For networking, it involves using a helper tool to add a tap network to a bridge. Resources for reference\u003e Resources for reference # I\u0026rsquo;ve found two GitHub projects that have been tremendously helpful in my study from scratch. The labs I\u0026rsquo;ve worked on are essentially based on these projects:\ncreate-vm - Using libvirt vm-provision - Leveraging QEMU In addition to these projects, I\u0026rsquo;ve also come across three command builders for QEMU and virt-install that, despite not being updated for a while, are incredibly valuable for studying:\nCommand Line Libvirt Command Line QEMU QEMU Command Generator For in-depth information and documentation, you can refer to the following:\nThe man page of qemu-system-x86_64 The wiki page of QEMU provides a wealth of details and examples. On the Arch Linux wiki, you\u0026rsquo;ll find a plethora of resources. Additionally, if you\u0026rsquo;re looking for guidance on using virt-install, RedHat has a comprehensive guide available:\nRedHat\u0026rsquo;s Virt-Install Guide For installation guides, you can follow these resources:\nUbuntu: Virtualization with Libvirt CentOS: Creating Guest VMs on Linux KVM Prepartion\u003e Prepartion # Preparation for create VM with KVM:\nInstall related software packages Cloud image Disk image Cloud-init ISO First, ensure qemu and virt-install are installed.\nThen, obtain a cloud image to serve as the base disk.\nCreate a copy of this image (backed by the cloud image) to form the disk image for our VM.\nConfigure the user and network settings using cloud-init, which is packed into an ISO and attached to the VM as a CD-ROM.\nQEMU Shortcuts\u003e QEMU Shortcuts # In QEMU, there are many ways to achieve the same thing with different parameters and numerous shortcuts. This can be quite confusing as different tutorials, websites, and discussions often use varying methods.\nFor clarity, I prefer to convert the parameters to their shortcut forms:\n-hda = -drive -cdrom = -drive -drive = -blockdev -device -audio = -audiodev -device -nic = -netdev -device For example:\n-drive file=linux.img,format=qcow2,if=virtio is equivalent to:\n-hda linux.img And:\n-drive file=cloud-init.img,media=cdrom,if=virtio is the same as:\n-hda cloud-init.img Also:\n-chardev socket,id=char0,path=u1-serial,server=on,wait=off -serial chardev:char0 can be shortened to:\n-serial unix:u1-serial,server,nowait Furthermore:\n-netdev bridge,id=hn0 -device virtio-net-pci,netdev=hn0,id=nic1 tap,helper=/usr/local/libexec/qemu-bridge-helper,id=hn0 -device virtio-net-pci,netdev=hn0,id=nic1 are equivalent to:\n-nic bridge,model=virtio-net-pci Networking\u003e Networking # The common network types for VM access include:\nUser Bridge/Tap From my perspective, tap and bridge networks function similarly. When using a bridge, the qemu bridge-helper script creates a tap device and adds it to the bridge. In the case of a tap, up and down scripts achieve a similar outcome.\nThe user network type is akin to NAT. To SSH into the VM, setting up port forwarding is necessary.\nTo create a network using libvirt, configure it with a DHCP server using dnsmasq. The following commands are used:\nCreates the network temporarily\nvirsh create \u0026lt;network-config\u0026gt; Creates the network permanently and ensures the network starts automatically\nvirsh define \u0026lt;network-config\u0026gt; virsh net-autostart \u0026lt;network\u0026gt; Starts the defined network\nvirsh start \u0026lt;network\u0026gt; However, with qemu alone, no network is created automatically. The user must manually set up everything,\nTo set up a network on QEMU, you need to:\nCreate the network device. Set up a DHCP server (if needed), typically by creating a bridge. There are various methods for this:\nTemporary setup using iproute2, which is easy but not persistent. Persistent setups differ across operating systems and are not well documented. For instance, Ubuntu uses netplan, older versions or Debian use /etc/network/interfaces, and CentOS/RedHat uses /etc/sysconfig/network/ifcfg-\u0026lt;dev\u0026gt; A hack way is using libvirt for network creation. I\u0026rsquo;ve gathered some useful configurations from the web:\nUbuntu (netplan)\nbunch of example Centos / Redhat (/etc/sysconfig/network/)\nbasic bridge bond + bridge + vlan Opensuse - wicked (/etc/sysconfig/network/) - I can\u0026rsquo;t found a web page related to this\nbunch of examples Debian / Oldder Ubuntu (/etc/network/interfaces)\nbasic bridge vlan bond For DHCP, if an existing one is available, that\u0026rsquo;s good, but if you need one for testing, you can use libvirt to create a bridge network and DHCP server.\nFor those not using libvirt, here\u0026rsquo;s a snippet to create a temporary DHCP server with\ndnsmasq --interface=br0 --bind-interfaces --dhcp-range=172.20.0.2,172.20.255.254 Ref: QEMU Host-only Networking guide on the Arch Linux wiki.\nCloud Init\u003e Cloud Init # Since the cloud image doesn\u0026rsquo;t have a user password configuration, logging into the system via console isn\u0026rsquo;t possible.\nTherefore, SSH is necessary to access the system.\nThis requires the VM to have network connectivity and the user to have the SSH key or password set on the system. This setup can be achieved using cloud-init.\nTo facilitate this, at least three files are required:\nmeta-data: meta-data user-data: user-data network-config: network-config These files can either be packed into an ISO and attached to the VM, or served to the VM through a simple HTTP server.\nI note that for openSUSE, cloud-init\u0026rsquo;s network-config version 2 doesn\u0026rsquo;t work, so version 1 must be used.\nDisplay - VNC and Spice\u003e Display - VNC and Spice # Virt-viewer can be utilized for both VNC and Spice connections.\nEnsure that X11 forwarding is enabled on ssh server in the /etc/ssh/sshd_config file. On client, this can be activated with\nssh -X Since I\u0026rsquo;m using macOS, I have installed XQuartz as X11 Client.\nFor the lab purposes, I\u0026rsquo;m using the root account, so I use\nsudo -E bash to maintain the $DISPLAY variables for X11.\nIt\u0026rsquo;s crucial to set XQuartz\u0026rsquo;s X11 preference settings correctly.\nSpecifically, in the output settings \u0026gt; colors, select \u0026lsquo;from display\u0026rsquo; and avoid options like 256color.\nPreviously, I had set it to 256color to speed up the x2go redning, but this caused issues when connecting to Spice (the screen is blank).\nMost of the commands for Spice are mentioned in the official user manual, which can be found at Spice User Manual.\nFor instance, to start Spice, use the command:\n-vga qxl \\ -spice port=3001 \\ -soundhw hda \\ -device virtio-serial \\ -chardev spicevmc,id=vdagent,debug=0,name=vdagent \\ -device virtserialport,chardev=vdagent,name=com.redhat.spice.0 Then, connect to the Spice server using:\nremote-viewer spice://127.0.0.1:3001 Other features like password authentication, TLS, and SASL authentication add additional security to the connection, which is good to know.\nIn the future, I would also like to explore USB and audio redirection/pasteboard functionalities.\nSerial and Console\u003e Serial and Console # Serial refers to the VM console, which I frequently use. Console refers to the QEMU console, which I rarely use for myself. There are different ways to connect to serial/console, and I\u0026rsquo;ll list those I have experience with:\nsocket telnet pty stdio Commands for connecting to serial/console:\n-serial unix:u1-serial,server,nowait -serial stdio,server,nowait -serial pty,server,nowait -serial telnet::7000,server,nowait To connect to serial from socket:\nsocat - UNIX-CONNECT:u1-serial To connect to serial from telnet:\ntelnet host port To connect to serial from pty:\nscreen /dev/pts/2 However, this can be messy for me. The style is often broken as it doesn\u0026rsquo;t resolve colors and newlines correctly.\nSetting export TERM=screen-256color improves the color issue but doesn\u0026rsquo;t resolve the newline problem.\nStdio:\nUseful for running QEMU in the foreground (without --daemonize) for quick testing. Labs\u003e Labs # Preparation\u003e Preparation # Software Packages:\nFor QEMU:\napt install -y qemu-kvm For libvirt:\napt install -y qemu-kvm libvirt-daemon-system virtinst To create cloud-init ISO:\napt install -y genisoimage Cloud Image:\nwget https://download.opensuse.org/repositories/Cloud:/Images:/Leap_15.5/images/openSUSE-Leap-15.5.x86_64-1.0.0-NoCloud-Build1.143.qcow2 Disk Image:\nqemu-img create -b openSUSE-Leap-15.5.x86_64-1.0.0-NoCloud-Build1.143.qcow2 -f qcow2 -F qcow2 \u0026#34;linux.img\u0026#34; \u0026#34;20G\u0026#34; Cloud-init ISO:\nmkdir /lab/cloud-init -p cat \u0026lt;\u0026lt; EOF | tee /lab/cloud-init/network-config network: version: 1 config: - type: physical name: eth0 subnets: - type: dhcp - type: physical name: eth1 subnets: - type: dhcp EOF cat \u0026lt;\u0026lt; EOF | tee /lab/cloud-init/user-data #cloud-config users: - name: \u0026lt;user\u0026gt; sudo: [\u0026#34;ALL=(ALL) NOPASSWD:ALL\u0026#34;] groups: sudo shell: /bin/bash homedir: /home/hugotse ssh_authorized_keys: - \u0026#34;\u0026lt;SSH_PUBLIC_KEY\u0026gt;\u0026#34; EOF cat \u0026lt;\u0026lt; EOF | tee /lab/cloud-init/meta-data instance-id: u1 local-hostname: u1 EOF genisoimage -output \u0026#34;/lab/cloud-init.img\u0026#34; -volid cidata -rational-rock -joliet /lab/cloud-init/* Libvirt\u003e Libvirt # To create VM with libvirt:\nCreate a bridge network:\ncat \u0026lt;\u0026lt; EOF | tee /lab/bridge.xml \u0026lt;network connections=\u0026#39;2\u0026#39;\u0026gt; \u0026lt;name\u0026gt;br0\u0026lt;/name\u0026gt; \u0026lt;forward mode=\u0026#39;nat\u0026#39;/\u0026gt; \u0026lt;bridge name=\u0026#39;br0\u0026#39; stp=\u0026#39;on\u0026#39; delay=\u0026#39;0\u0026#39;/\u0026gt; \u0026lt;mac address=\u0026#39;52:54:00:8a:e9:7c\u0026#39;/\u0026gt; \u0026lt;ip address=\u0026#39;192.168.123.1\u0026#39; netmask=\u0026#39;255.255.255.0\u0026#39;\u0026gt; \u0026lt;dhcp\u0026gt; \u0026lt;range start=\u0026#39;192.168.123.2\u0026#39; end=\u0026#39;192.168.123.254\u0026#39;/\u0026gt; \u0026lt;/dhcp\u0026gt; \u0026lt;/ip\u0026gt; \u0026lt;/network\u0026gt; EOF virsh net-define bridge.xml virsh net-start br0 virsh net-autostart br0 virt-install --name=\u0026#34;u1\u0026#34; --import --disk \u0026#34;path=linux.img,format=qcow2\u0026#34; --disk \u0026#34;path=cloud-init.img,device=cdrom\u0026#34; --ram=\u0026#34;2048\u0026#34; --vcpus=\u0026#34;2\u0026#34; --osinfo opensuse15.0 --wait 0 --noautoconsole --autostart --network bridge=br0 Connect to console\nvirsh console u1 Connect to the display\nvirt- u1\nStop \u0026amp; Stop the VM\nvirsh start u1 virsh shutdown u1 Remove the VM\nvirsh destroy u1 virsh undefine u1 QEMU\u003e QEMU # To create a VM with QEMU, combine everything together:\nqemu-system-x86_64 \\ -name u1 \\ -enable-kvm \\ # Enable KVM -hda linux.img \\ # Add disk image -cdrom cloud-init.img \\ # Add cloud init -m 2048 \\ -smp 2 \\ # Memory and CPU -nic user,hostfwd=tcp:127.0.0.1:1025-:22 \\ -nic bridge,br=br0,model=virtio-net-pci \\ # First network - user network with port forward SSH to TCP 1025 # Second network - bridge network on br0 device -vga qxl \\ -spice port=3001,password=123 \\ -soundhw hda \\ -device virtio-serial \\ -chardev spicevmc,id=vdagent,debug=0,name=vdagent \\ -device virtserialport,chardev=vdagent,name=com.redhat.spice.0 # Spice ","date":"2023-12-04","permalink":"/posts/kvm-qemu-libvirt/","section":"Posts","summary":"Purpose of this note: The existing online tutorials provide limited information about KVM.\nMany tutorials reference the use of virt-manager, which is akin to a GUI tool for creating virtual machines.","title":"KVM - QEMU, libvirt"},{"content":"","date":"2023-12-04","permalink":"/tags/libvirt/","section":"Tags","summary":"","title":"libvirt"},{"content":"","date":"2023-12-04","permalink":"/posts/","section":"Posts","summary":"","title":"Posts"},{"content":"","date":"2023-12-04","permalink":"/tags/qemu/","section":"Tags","summary":"","title":"qemu"},{"content":"","date":"2023-12-04","permalink":"/tags/rancher/","section":"Tags","summary":"","title":"rancher"},{"content":"Drawbacks and Challenges\u003e Drawbacks and Challenges # High Resource Requirements in Harvester\u003e High Resource Requirements in Harvester # When initiating Harvester, the system immediately demands significant resources, utilizing at least 10GB of RAM and 4 CPUs. This level of resource usage is notably high, especially when compared to other hypervisors (eg: xen, vmware).\nThe official hardware requirements are even higher:\nCPU: min: 8-core; prefer: 16 cores or more Memory: min 32 GB; prefer: 64 GB or more Disk Capacity: min 140 GB; prefer: 500 GB or more This high demand for resources can largely be attributed to Harvester\u0026rsquo;s underlying architecture, which is based on a Kubernetes cluster.\nOperational Limitations in VM Management\u003e Operational Limitations in VM Management # Operations such as changing network configurations, taking backups, or creating snapshots in VMs require stopping the VM first, which can be a significant limitation in certain scenarios.\nLarge File Sizes for Exported Templates\u003e Large File Sizes for Exported Templates # The size of exported templates in Harvester is directly proportional to the VM size. This can result in unexpectedly large file sizes for templates.\nFor example, when starting with an original cloud image of 1GB and creating a VM with a 10GB disk, the resulting template size was also 10GB, significantly larger than the original image size.\nSpace Issue in default paritition setup\u003e Space Issue in default paritition setup # One notable storage issue in Harvester is the often insufficient default size of the /usr/local partition, particularly when it comes to system upgrades.\nTo address this, a workaround has been provided through a script available on GitHub.\nThis script is designed to purge unused images, thereby freeing up valuable space in the /usr/local partition. This solution is detailed in Harvester Issue #2668.\nRancher is lagging behind the k8s Releases\u003e Rancher is lagging behind the k8s Releases # Rancher serves as a management platform for various clusters and operates independently of Harvester.\nFor installation, Rancher offers options to use either Helm or Docker.\nWhile the Docker installation method is effective, there have been challenges with installations on the latest k8s versions.\nRancher does not support the most recent k8s versions. For instance, as detailed in the Rancher v2.7.9, the latest supported Kubernetes version is v1.26.9, whereas the Kubernetes has already advanced to version v1.28.\nCapabilities\u003e Capabilities # In terms of technology, Harvester incorporates several advanced features:\nRKE2 Integration: Harvester employs RKE2, which is similar to k3s but includes additional CIS hardening features for improved security. Multus for Networking: The use of Multus in Harvester enhances its networking capabilities, allowing for the configuration of multiple networks within the system. KubeVirt for VM Management: Harvester integrates KubeVirt, enabling the creation of VMs using KVM within containers. Network Management\u003e Network Management # The default network in Harvester is the Management Network, also known as the pod network. Harvester supports the creation of custom networks, enabling users to set up cluster networks (binding to the NIC) and VM networks (configuring subnets and VLANs) for tailored network configurations. The platform includes features for assigning custom MAC addresses and managing multiple networks. |--machine (ens19) |--harvester (ens19) |----vm (management network; vm network `private` that bond to ens19) My network setup involves configuring a virtual machine (VM) with two network interface cards (NICs). The first NIC is connected to the Management Network, and the second NIC is linked to a secondary network card on Harvester (eth19). This configuration allows an external machine to connect to the VM on the same network.\nI have also tested a setup with secondary network only. In this arrangement, there is connectivity between the VM and a machine on the same network, along with internet access.\nThis represents the cluster network setup. Multiple NICs can be chosen. This illustrates the VM network setup with an untagged network. show the VM network setup with type VLAN, but the VLAN ID is set to 1 For 2Nic Test,\nnetwork-config\nnetwork: version: 1 config: - type: physical name: eth0 subnets: - type: dhcp - type: physical name: eth1 subnets: - type: static address: 203.0.113.3/24 For 1Nic Test\nnetwork-config\nnetwork: version: 1 config: - type: physical name: eth0 subnets: - type: static address: 203.0.113.3/24 Virtual Machine Management\u003e Virtual Machine Management # Creating VMs using cloud image and cloud-init. Accessing VMs is possible both through the Harvester host and from external. Adding extra disk to a VM, can be done while the VM is running. Adding or changing network connections, the VM needs to be stopped before making these changes. Taking snapshots of VMs. Managing multiple network interfaces, including setting up second NICs and assigning mac address. Supports backup options to external storage locations like S3 and NFS. The restore functionality provides flexibility to create new VMs or replace existing ones from backups and snapshots. ","date":"2023-12-04","permalink":"/posts/review-of-harvester/","section":"Posts","summary":"Drawbacks and Challenges\u003e Drawbacks and Challenges # High Resource Requirements in Harvester\u003e High Resource Requirements in Harvester # When initiating Harvester, the system immediately demands significant resources, utilizing at least 10GB of RAM and 4 CPUs.","title":"Review of Harvester"},{"content":"","date":"2023-12-04","permalink":"/tags/","section":"Tags","summary":"","title":"Tags"},{"content":"","date":"2023-09-11","permalink":"/tags/aws/","section":"Tags","summary":"","title":"aws"},{"content":"","date":"2023-09-11","permalink":"/tags/efs/","section":"Tags","summary":"","title":"efs"},{"content":"I recently had trouble connecting an ECS container to an EFS access point. Here\u0026rsquo;s an breakdown of what happened and how I fixed it:\nThe Problem:\nWhen I tried to link an ECS container with an EFS access point, I got this error: \u0026quot;'b'mount.nfs4: access denied by server while mounting 127.0.0.1:/'\u0026quot;\nFirst Thoughts:\nI thought the issues could be:\nEFS\u0026rsquo;s security group blocking our connection. Not having the right permissions in the task\u0026rsquo;s IAM role. What I Tried:\nTo figure out what was wrong, I did two things:\nI tried connecting the EFS without the access point directly from EC2. I changed the volume mount settings to leave out the access point. Both worked perfectly, so my first thoughts weren\u0026rsquo;t the problem. After some digging, I found a helpful article. It explained that if you don\u0026rsquo;t set up the right ownership and permission settings for the root directory of an access point, EFS won\u0026rsquo;t create it. This was causing the \u0026ldquo;access denied\u0026rdquo; error.\nThe Solution:\nWhen setting up the access point, I made sure to fill in the owner id, owner group id, and default file permissions in the Root directory creation permission section. This fixed the error.\n","date":"2023-09-11","permalink":"/posts/aws-efs-access-point-error/","section":"Posts","summary":"I recently had trouble connecting an ECS container to an EFS access point. Here\u0026rsquo;s an breakdown of what happened and how I fixed it:\nThe Problem:\nWhen I tried to link an ECS container with an EFS access point, I got this error: \u0026quot;'b'mount.","title":"Fixing Access Issues Between ECS Containers and EFS Access Points"},{"content":"Service-wise Summary\u003e Service-wise Summary # Athena\u003e Athena # Compression\nAthena uses different compression methods based on the need: gzip: Used when focusing on reducing the size of data. lzo or snappy: Used when focusing on speed. Workgroup\nAccess to the workgroup is controlled by IAM policies. Workgroups can be temporarily enabled or disabled. You can define policies for things like query result location or encryption. By turning on \u0026lsquo;Override client-side settings\u0026rsquo;, you can enforce the workgroup\u0026rsquo;s settings on users. Data usage control can be applied in two ways: Per query: The query stops when the amount of scanned data exceeds a limit. Workgroup-wide: Triggers an alert when the amount of scanned data exceeds a limit, but doesn\u0026rsquo;t enforce the limit. Access rights are based on a user\u0026rsquo;s permissions, not on service or EC2 roles. Query Timeout\nDML/DDL queries have a timeout, which can be increased in the service quota. Resolving HIVE_METASTORE_ERROR\nError: \u0026ldquo;Expected \u0026lsquo;xxxx\u0026rsquo; but \u0026lsquo;/\u0026rsquo; found.\u0026rdquo; Reason: This occurs when there\u0026rsquo;s an unsupported character in the partition or column. Fix: Remove the unsupported character. Error: \u0026ldquo;Storage descriptor is not populated.\u0026rdquo; Reason: This occurs when there\u0026rsquo;s a broken partition. Fix: Use \u0026lsquo;msck repair table\u0026rsquo; to repair the table, or copy the data into a separate folder and run the crawler on that folder. Error: \u0026ldquo;Payload size exceed.\u0026rdquo; Reason: This occurs when running a Lambda function in a query, as Lambda has a size limit on returned data. Fix: Upload the result to S3, then attach the presigned URL in the response. EMR\u003e EMR # Block Public Access\nEMR has a setting similar to S3\u0026rsquo;s \u0026lsquo;Block public access\u0026rsquo;. This setting functions before EMR creation and removes public access in the security group, except for allowed port ranges. Consistent View (Legacy)\nPreviously, AWS used DynamoDB to track read-after-write consistency in S3 objects (using EMRFS). EBS Encryption and Custom Software\nEBS encryption and custom software installation in EMR can be accomplished using a custom Amazon Linux AMI. Custom software can also be installed using bootstrap actions, similar to the way user-data is used in EC2. Integration with DynamoDB\nEMR has integration with DynamoDB in Hive. Handling HTTP 503 Slow Down AWSZonS3Exception\nThis error is caused by an excessive number of reads to S3. It can be fixed by increasing the EMRFS retry limit or adding more prefixes, as the rate limit applies per prefix. HBase\nHBase is a column-based NoSQL database, ideal for OLTP operations. Features include: Data storage in S3. Support for read-replicas (although two masters cannot point to the same HBase path, read-replicas can be recreated). The ability to take snapshots to S3. High Availability\nHigh availability is currently supported only in a single AZ (support for multiple AZs is coming soon). For high availability, it\u0026rsquo;s recommended to create multiple masters and use EMRFS. Presto\nPresto can work with data from multiple sources. S3-Dist-CP\nS3-Dist-CP is a tool that helps in distributing data copying between EMRFS and HDFS. Sqoop\nSqoop is used to export and import data between relational databases (like RDS) and HDFS. EMR Managed Scaling\nEMR Managed Scaling reacts faster than traditional auto scaling. It can work with either instance groups or fleets. It is automatically configured, so no manual setup is needed. Glue\u003e Glue # Crawler\nCan use S3 locations or data catalog tables as sources. When S3 location is chosen, it automatically creates or updates the schema and table. When a data catalog table is chosen, it does not create a new table. Choices are available for handling schema updates or deletions. ETL Job\nIn ETL job, you have the option to update the schema. Debugging OOM Exceptions:\nDriver\nProblem: The driver creates a task for each file in an S3 prefix. When there are too many files in one location, it can lead to an OOM error due to the large number of tasks. Solution: Use \u0026lsquo; groupFiles: inPartition\u0026rsquo; to load multiple files in one task, which reduces the number of tasks and prevents OOM. Executor\nProblem: When selecting data from MySQL, the default action is to load all rows. This can cause an OOM error if there are too many rows. Solution: Use a dynamic frame to limit the number of rows selected at once, which prevents OOM. Excluding S3 Storage Class in ETL Job\nThe S3 storage class can be excluded by setting it in the data catalog property or in the dynamic frame. FindMatches\nUses machine learning to find records from different tables that refer to the same object (such as a customer or product). PostAction\nCan be used to deduplicate records before ingesting data into Redshift. Kinesis Stream\u003e Kinesis Stream # Resharding\nThis involves splitting and merging shards. For instance, during a split, shard1 can be divided into shard10 and shard11. Here, shard1 becomes the parent, while shard10 and shard11 become the children. After resharding, the parent shard (shard1 in this case) becomes read-only but remains available. For ordered data reading, the parent should be consumed first, followed by the children. This process needs to be managed in the SDK. Enhanced Fanout\nKinesis limits throughput by shard (1MB for write, 2MB for read). Enhanced fanout can increase read throughput. This process uses a Lambda function to read from the stream and Lambda replicas to distribute the data to more destinations. Kinesis Stream as a Database\nThe data is stored until it expires. The expiry period can be set from 1 day to 1 year. The data is not removed when read, meaning it can be consumed multiple times. Stateless Nature\nConsumers need to remember their read position to prevent re-reading data. This is automatically handled in the KCL, which creates a DDB table to manage consumer memberships and track their states. If using the SDK, developers must implement the checkpoint by tracking the number of records read or by time to achieve the same effect. Using KCL\nA DDB table is created to maintain the consumer state. If the iterator expires unexpectedly, it could be due to the DDB not having enough write capacity. Handling Duplicate Records\nSequence numbers can be used to identify duplicates. Retries are common in streams and can occur due to network issues or unexpected consumer exits, which can lead to the checkpoint not being saved. Kinesis Firehose\u003e Kinesis Firehose # Data can be delivered to storage (S3) or databases (DDB, Redshift, RDS, OpenSearch). The minimum batch time limit is 1 minute, which may not be ideal for real-time processing. A Lambda function can be used to transform data before it\u0026rsquo;s delivered or to send alerts. Kinesis Analytics\u003e Kinesis Analytics # A CSV file in S3 can be attached as reference data to join with the stream. The data can be output to Lambda. RANDOM_CUT_FOREST can be used for anomaly detection. Data can be pre-processed with a Lambda function. OpenSearch\u003e OpenSearch # Issues leading to High JVM Pressure\nHaving too many divisions of an index, also known as shards. Uneven distribution of shards. Running summaries on text fields or wildcards. Combining large data tables. An overload of requests. UltraWarm Feature\nA cost-effective, read-only feature. The index state management tool can automate moving data to UltraWarm. To fine-tune performance, adjust the \u0026lsquo;max_num_segments\u0026rsquo; value in \u0026lsquo;force_merge\u0026rsquo;. A larger \u0026lsquo;max_num_segments\u0026rsquo; makes the process faster, but can cause delays after indexing. Quicksight\u003e Quicksight # Setting Up Row-Level Security on a Dataset\nAttach a CSV file or a user-based query rule to the dataset. The file or query includes \u0026ldquo;user\u0026rdquo; or \u0026ldquo;group\u0026rdquo; and filtering conditions (similar to SQL\u0026rsquo;s WHERE clause: WHERE variable=value). The default setting is to restrict access; if there\u0026rsquo;s no entry, there\u0026rsquo;s no visibility. Tag-based rules can be used in API calls to create embedded URLs for guest users. Implementing Column Level Security\nChoose the columns that specific users or groups can view. To retrieve data from Athena, establish a connection to Athena and register the S3 bucket in Quicksight. Features of the Paid Version\nIsolated namespaces that readers can access. Authentication via Active Directory. Redshift\u003e Redshift # DBlink\nAn add-on to PostgreSQL that enables data queries directly from Redshift. It operates in read-only mode. Audit Log\nCan be activated as needed. Column Level Access Control\nAccess can be provided through the GRANT SELECT(COL1, COL2,\u0026hellip;) command. Query Monitoring Rule A rule includes:\nA rule name. One or more conditions. An action to be taken (stop, log, hop, change priority). Short Query Acceleration\nIn the workload management settings, this feature can prioritize faster queries. Spectrum\nThis feature is not compatible with Glacier data. S3\u003e S3 # S3 Select\nFor Glacier, it functions only with instant retrieval. Intelligent Tiering and Reduced Redundancy Storage are not compatible. Data output can be formatted as CSV or JSON. Data input can be formatted as CSV or JSON. Compression is supported, but only gzip or bzip formats are allowed. For columnar formats like Parquet, only gzip or snappy formats are allowed. Lake Formation\u003e Lake Formation # To maintain compatibility, \u0026ldquo;super\u0026rdquo; permission is given to IAMAllowedPrincipals. This means that if the IAM policy permits the data catalog, it will bypass the Lake Formation\u0026rsquo;s permission control. Here are the steps to use Lake Formation:\nTake away the IAMAllowedPrincipals\u0026rsquo; rights in Data Lake permissions, under Administrative roles and tasks for Database Creators. In the data catalog settings, deselect \u0026lsquo;Use only IAM access control for new databases\u0026rsquo; and \u0026lsquo;Use only IAM access control for new tables in new databases\u0026rsquo;. Register the S3 in the Data Lake location. Assign permissions in Data Lake permissions. If \u0026lsquo;select\u0026rsquo; permission is given, it will also allow the S3 permission. Topic-wise Summary\u003e Topic-wise Summary # Encryption\u003e Encryption # Quicksight: Employs AWS managed keys or KMS-CMK for data encryption.\nRedshift: Utilizes CloudHSM, on-premise HSM, or KMS for data encryption.\nEMR:\nFor EMRFS: Server side: Uses either AWS managed keys or customer-managed KMS keys. Client side: Employs customer-managed KMS keys or custom KMS keys. For instance storage/EBS: Prefers EBS when possible. If not, it falls back to LUKS encryption. Access Control\u003e Access Control # Concepts\nRow-Level Security:\nConceals rows by filtering data based on a certain value (e.g., using the SQL command: SELECT * FROM table WHERE foo=bar). Column-Level Security:\nHides specific columns (these columns will not be included in a SELECT statement). Athena:\nAccess is managed through user/group permissions. EMR:\nAccess is managed via an EC2 role. For EMRFS, an IAM role is defined in the security configuration (this role is assumed by the EC2 role). Redshift:\nAccess is managed via a service role. Usage permissions are granted to IAM roles (similar to MySQL). Quicksight:\nImplements both row-level and column-level security on datasets. Authentication methods: IAM/SAML2/MFA are available for free. Active Directory is available with the Enterprise edition. Lake Formation:\nImplements both row-level and column-level security on the data catalog. Best Practices:\nRedshift:\nTo use data in Lake Formation, grant SELECT permission to the Redshift role. To allow a user to use a table in Redshift, use GRANT USAGE in Redshift. EMRFS:\nS3 access should be granted through an IAM role, not an EC2 role. The EC2 role assumes the IAM role to retrieve data from S3. ","date":"2023-07-31","permalink":"/posts/exam-aws-data/","section":"Posts","summary":"Service-wise Summary\u003e Service-wise Summary # Athena\u003e Athena # Compression\nAthena uses different compression methods based on the need: gzip: Used when focusing on reducing the size of data. lzo or snappy: Used when focusing on speed.","title":"AWS Certified Data Analytics - Specialty"},{"content":"","date":"2023-07-31","permalink":"/tags/data/","section":"Tags","summary":"","title":"data"},{"content":"Scan and Query:\nAmazon DynamoDB has two operations for retrieving data: Scan and Query. Scan retrieves all items in a table, making it inefficient for larger tables. Query is designed to retrieve specific items based on their partition key values, allowing filtering of results. Indexing:\nThe primary key consists of the partition key and the sort key (optional), which uniquely identify each item in a table. While the primary key is essential, it may not always be sufficient for accessing data in the way needed. Global Secondary Index (GSI) is a separate index with its partition key and sort key that can be used for querying on non-primary key attributes. GSI can significantly reduce the number of items that need to be scanned and improve query speed. Primary key has to be unique, but GSI can be duplicated. Select and Filter:\nSelect and Filter expressions can also be used to specify attributes and conditions for retrieving data. Query is generally recommended for specific and efficient data retrieval, especially when used with a GSI. The choice of which operation to use depends on the specific use case and how data needs to be accessed and queried. ","date":"2023-05-08","permalink":"/posts/brief-overview-of-scan-query-select-and-filter-of-dyanmodb/","section":"Posts","summary":"Scan and Query:\nAmazon DynamoDB has two operations for retrieving data: Scan and Query. Scan retrieves all items in a table, making it inefficient for larger tables. Query is designed to retrieve specific items based on their partition key values, allowing filtering of results.","title":"Brief Overview of Scan, Query, Select, and Filter of DyanmoDB"},{"content":"","date":"2023-05-08","permalink":"/tags/cloud/","section":"Tags","summary":"","title":"cloud"},{"content":"","date":"2023-05-08","permalink":"/tags/db/","section":"Tags","summary":"","title":"db"},{"content":"","date":"2023-05-08","permalink":"/tags/ddb/","section":"Tags","summary":"","title":"ddb"},{"content":"","date":"2023-05-08","permalink":"/tags/grpc/","section":"Tags","summary":"","title":"grpc"},{"content":"Workflow of RPC server\u003e Workflow of RPC server # Receive a RPC request Parse request into message, context Service interceptor RPCMethodHandler Give RPC response [metadata (optional), message protobuf object / raise exception] Metadata has to be sent explicitly.\nSteps to work on RPC client-server\u003e Steps to work on RPC client-server # Server\u003e Server # Create RPC server object Create servicer object Add middleware to server Add servicer to RPC Start the server Client\u003e Client # Create channel object Add middleware to channel Send RPC request with stub object Ping-pong between client-server:\u003e Ping-pong between client-server: # In bidirectional streaming RPC, the call is initiated by the client invoking the method, and the server receives the client metadata, method name, and deadline The server can choose to send back its initial metadata or wait for the client to start streaming messages Client and server-side stream processing is application-specific Since the two streams are independent, the client and server can read and write messages in any order For example, a server can wait until it has received all of a client’s messages before writing its messages, or the server and client can play “ping-pong” This is similar to a WebSocket, providing a live chat between client and server from the stub (channel) Example flow:\nClient yields request_stream1 Server receives request_stream1 Server yields result_stream1 Client yields stream2 Server receives stream2 Server yields result_stream2 The client keeps yielding stream, and the server iterates through the stream The server retrieves and stores each stream until iterator.next() == null The server responds with a stream message and ends the stream When sending two stream objects:\nIt is still considered a single request Multiple messages are sent through the channel (stub) Each stream message is sent to the same target server (behind ALB) The channel can last a long time unless the client or server closes it, passes the deadline, or reaches the idle_timeout When streaming, it follows a consumer and producer pattern In Client streaming RPC, the client acts as a producer and server as a consumer In Bidirectional streaming RPC, both client and server act as producers and consumers simultaneously Reference: https://www.oreilly.com/library/view/grpc-up-and/9781492058328/ch04.html\nMy Lab to Experience gRPC\u003e My Lab to Experience gRPC # For experiencing gRPC, I created a simple client and server to manage the records of a message \u0026ldquo;video.\u0026rdquo;\nInstall the basic packages needed: pip3 install grpcio grpcio-tools grpcio_reflection\nTo start the server:\npython3 server.py To compile the proto:\npython3 -m grpc_tools.protoc -I. --python_out=. --pyi_out=. --grpc_python_out=. video_service.proto To send requests to the server:\nexport SERVER=localhost:50051 Unary - stream:\ngrpcurl -proto video_service.proto -plaintext $SERVER video_service.VideoService/ListVideos Unary - (with param):\ngrpcurl -proto video_service.proto -plaintext -d \u0026#39;{\u0026#34;id\u0026#34;:\u0026#34;2\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;Sample Video\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;https://example.com/sample.mp4\u0026#34;, \u0026#34;duration\u0026#34;: 120}\u0026#39; $SERVER video_service.VideoService/AddVideo grpcurl -proto video_service.proto -plaintext -d \u0026#39;{\u0026#34;id\u0026#34;:\u0026#34;1\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;Sample Video\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;https://example.com/sample.mp4\u0026#34;, \u0026#34;duration\u0026#34;: 120}\u0026#39; $SERVER video_service.VideoService/AddVideo Stream:\ngrpcurl -proto video_service.proto -plaintext -d @ $SERVER video_service.VideoService/GetVideos \u0026lt;\u0026lt; EOF {\u0026#34;id\u0026#34;:\u0026#34;2\u0026#34;} {\u0026#34;id\u0026#34;:\u0026#34;1\u0026#34;} EOF grpcurl --plaintext $SERVER list Interactive way to send a stream:\ngrpcurl --proto video_service.proto -insecure -vv -d @ $SERVER video_service.VideoService/GetVideo {\u0026#34;id\u0026#34;:\u0026#34;2\u0026#34;} wait for the response for video 2 {\u0026#34;id\u0026#34;:\u0026#34;1\u0026#34;} wait for the response for video 1 Examples\u003e Examples # There are some examples worth reading from the gRPC library repository.\nConcepts related\u003e Concepts related # Health checking:\nSpecial servicers that require an extra package Can service health checks with its own thread pools, not affecting the thread pool of other servicers Interceptors:\nServer Interceptors: - Modify the request from the client before processing. - Interrupt each received RPC request. - Call `continuation` to proceed. - Return a handler to override the default handler. Client Interceptors: - Invoked before the RPC request is sent and after the RPC response is received. - Add a middleware to the channel, which can extend multiple per-RPC-type client interceptors. Implement a function per RPC type. - Inside the functions, modify the RPC request from `ClientCallDetails`, then call `continuation`. This will send the RPC call, wait for the response from the server, and then return the RPC response. The response can be modified if desired. Metadata:\nSimilar to headers in HTTP Multiplex:\nOne channel with multiple stubs One server with multiple services Route guide:\nBest example to understand unary-unary, unary-stream, stream-unary, and stream-stream rpc requests Options / Settings related\u003e Options / Settings related # Keepalive Timeout Load balancing policies Retry Wait for ready - send the request when the service is not busy Compression - gzip ALB\u003e ALB # Sticky sessions can be tricky:\nSticky sessions are used when there are many clients that keep requesting the service and want to reuse connections. Sticky sessions in gRPC are not ideal when working with load balancing:\nLoad balancing will not work effectively, as it causes traffic to be sent to the same instance. If there are limited clients but each sends a lot of requests, we want the requests to distribute evenly to the backends. However, sticky sessions cause the same client\u0026rsquo;s requests to be sent to the same instance, overloading the backend. For more information on gRPC load balancing, visit: https://majidfn.com/blog/grpc-load-balancing/\nHealth Check\u003e Health Check # gRPC is an HTTP request with a binary payload:\nIt uses POST requests and the URI is the method (e.g., video_service.VideoService/ListVideo). Status codes: 0 is normal; 12 is not implemented. ","date":"2023-05-08","permalink":"/posts/understanding-grpc-key-concepts-and-examples/","section":"Posts","summary":"Workflow of RPC server\u003e Workflow of RPC server # Receive a RPC request Parse request into message, context Service interceptor RPCMethodHandler Give RPC response [metadata (optional), message protobuf object / raise exception] Metadata has to be sent explicitly.","title":"Understanding gRPC: Key Concepts and Examples"},{"content":"Real Hardware Requirments\u003e Real Hardware Requirments # Suggestion from doc:\n4 physical CPU cores 9 GB of free memory 35 GB of storage space Recommended (to make it work for development):\nat least 15GB memory. 18GB is better. 60 GB storage Increase Memory:\u003e Increase Memory: # crc stop crc config set memory \u0026lt;memory_in_mb\u0026gt; crc start Prune images in the cluster:\u003e Prune images in the cluster: # OpenShift Local uses Podman to run containers for the infrastructure of the cluster. Image cache accumulates and grows quickly. Need to SSH into the KVM and prune the images. Access the host:\nUse the SSH command and the key (~/.crc/machines/crc/id_ecdsa) found in the .crc folder: ( See) ssh -i \u0026lt;key\u0026gt; core@\u0026lt;master-hostname\u0026gt; To find the master ip:\nCreate a pod with hostNetwork install iproute2 use ip addr to find the host IP. apiVersion: v1 kind: Pod metadata: name: test spec: hostNetwork: true containers: - name: alpine image: alpine tty: true Prune dangling images:\npodman image prune Enlarge disk size: (Thanks to this discussion)\u003e Enlarge disk size: (Thanks to this discussion) # Enlarge the KVM image: CRC_MACHINE_IMAGE=\u0026#34;$HOME/.crc/machine/crc/crc\u0026#34; crc stop qemu-img resize ${CRC_MACHINE_IMAGE} +24G Find the root partition (the largest one) using virt-filesystems: virt-filesystems --long -h --all -a $HOME/.crc/machines/crc/crc Name Type VFS Label MBR Size Parent /dev/sda1 filesystem unknown - - 1.0M - /dev/sda2 filesystem ext4 boot - 1.0G - /dev/sda3 filesystem xfs root - 39G - /dev/sda1 partition - - - 1.0M /dev/sda /dev/sda2 partition - - - 1.0G /dev/sda /dev/sda3 partition - - - 39G /dev/sda /dev/sda device - - - 40G - Enlarge the partition in KVM: cp ${CRC_MACHINE_IMAGE} ${CRC_MACHINE_IMAGE}.ORIGINAL virt-resize --expand /dev/sda3 ${CRC_MACHINE_IMAGE}.ORIGINAL ${CRC_MACHINE_IMAGE} ","date":"2023-05-07","permalink":"/posts/admin-tips-for-managing-openshift-local-clusters-pruning-images-enlarging-disk-size-and-increasing-memory/","section":"Posts","summary":"Real Hardware Requirments\u003e Real Hardware Requirments # Suggestion from doc:\n4 physical CPU cores 9 GB of free memory 35 GB of storage space Recommended (to make it work for development):","title":"Admin Tips for Managing OpenShift Local Cluster: Pruning Images, Enlarging Disk Size, and Increasing Memory"},{"content":"","date":"2023-05-07","permalink":"/tags/case-study/","section":"Tags","summary":"","title":"case-study"},{"content":"A recent article on Prime Video Tech describes their approach to scaling up their audio-video monitoring service and reducing costs by 90%.\nThis blog post aims to critically analyze the article and address some issues and misconceptions.\nThe Service and the Problem\nThe monitoring service converts video streams into video/audio formats and stores them on Amazon S3. Multiple detectors identify problems in the video, and the results are saved back to S3. Escalating costs were due to data transfer between S3, orchestration with AWS Step Functions, and not being able to use EC2 Savings Plans. Detectors had to download the same data multiple times, causing high costs and inefficient resource use. Scaling bottleneck: orchestration management using AWS Step Functions led to reaching account limits and charges per state transition. My View:\nThe post\u0026rsquo;s motivation is valid as it shares a story about addressing cost and scaling issues in a Prime Video service.\nHowever, bloggers and YouTubers used the article to create headlines pitting microservices against monolithic architectures.\nThe real issue is not about microservices vs. monoliths, but the pricing model on AWS and understanding the context of the solution.\nThe post is missing information: size of video streams, detector runtime, and service profile (CPU, memory, concurrent users, usage patterns, etc.).\nThese factors are essential to understanding scalability and cost implications.\nThe solution involves vertical scaling, which can be more expensive and challenging to adjust based on usage patterns, possibly leading to underutilization during off-peak times.\nMy Advice/Suggestion:\nConsider trade-offs, limitations, and applicability of the solution. Understand that AWS is not synonymous with microservices; the real issue is its pricing model. Know your service\u0026rsquo;s requirements and usage patterns before making architectural decisions. Take a balanced approach and make informed decisions based on the specific needs of your application, rather than being swayed by sensational headlines. ","date":"2023-05-07","permalink":"/posts/critical-review-of-hit-discussions-video-monitoring-service-pitfalls-of-blaming-microservices/","section":"Posts","summary":"A recent article on Prime Video Tech describes their approach to scaling up their audio-video monitoring service and reducing costs by 90%.\nThis blog post aims to critically analyze the article and address some issues and misconceptions.","title":"Critical Review of Hit Discussion's Video Monitoring Service: Pitfalls of Blaming Microservices"},{"content":"","date":"2023-05-07","permalink":"/tags/k8s/","section":"Tags","summary":"","title":"k8s"},{"content":"","date":"2023-05-07","permalink":"/tags/openshift-local/","section":"Tags","summary":"","title":"openshift-local"},{"content":"This post explores various networking concepts and configurations for Linux systems, including:\nbridging VLAN filtering IPvlan Vxlan These concepts are useful for creating and managing virtual networks.\nI\u0026rsquo;m sharing this information based on my own interest and learning experiences. Please note that this may not cover all aspects of networking, and it\u0026rsquo;s recommended to consult additional resources or an expert for more in-depth information.\nBridge\u003e Bridge # A bridge connects two or more network segments, allowing them to act as a single network. In Linux, you can create network namespaces and connect them using a bridge. This configuration allows the namespaces to communicate with each other, but they won\u0026rsquo;t have internet access unless you set up NAT on the host.\nBy default, Docker is using the Bridge network.\nLinux Networking: Bridge, iptables, and Docker\nA simple lab on it:\nCreate two network namespaces, each with its own individual IP address. Bridge the namespaces together using the same bridge. At this point, the two network namespaces can ping each other, but they won\u0026rsquo;t have internet access. To provide internet access, configure NAT on the host machine, which will allow the network namespaces to access the internet. NS1=\u0026#34;ns1\u0026#34; NS2=\u0026#34;ns2\u0026#34; NS1=\u0026#34;ns1\u0026#34; VETH1=\u0026#34;veth1\u0026#34; VPEER1=\u0026#34;vpeer1\u0026#34; NS2=\u0026#34;ns2\u0026#34; VETH2=\u0026#34;veth2\u0026#34; VPEER2=\u0026#34;vpeer2\u0026#34; BR_ADDR=\u0026#34;10.10.0.1\u0026#34; VPEER_ADDR1=\u0026#34;10.10.0.10\u0026#34; VPEER_ADDR2=\u0026#34;10.10.0.20\u0026#34; BR_DEV=\u0026#34;br0\u0026#34; ip netns add $NS1 ip netns add $NS2 ip link add ${VETH1} type veth peer name ${VPEER1} ip link add ${VETH2} type veth peer name ${VPEER2} ip link add ${BR_DEV} type bridge ip link set ${VETH1} master ${BR_DEV} ip link set ${VETH2} master ${BR_DEV} ip addr add ${BR_ADDR}/16 dev ${BR_DEV} ip link set ${VPEER1} netns ${NS1} ip link set ${VPEER2} netns ${NS2} ip link set ${BR_DEV} up ip link set ${VETH1} up ip link set ${VETH2} up ip netns exec ${NS1} ip link set lo up ip netns exec ${NS2} ip link set lo up ip netns exec ${NS1} ip link set ${VPEER1} up ip netns exec ${NS2} ip link set ${VPEER2} up ip netns exec ${NS1} ip addr add ${VPEER_ADDR1}/16 dev ${VPEER1} ip netns exec ${NS2} ip addr add ${VPEER_ADDR2}/16 dev ${VPEER2} ip netns exec ${NS1} ip route add default via ${BR_ADDR} ip netns exec ${NS2} ip route add default via ${BR_ADDR} bash -c \u0026#39;echo 1 \u0026gt; /proc/sys/net/ipv4/ip_forward\u0026#39; iptables -t nat -A POSTROUTING -s ${BR_ADDR}/16 ! -o ${BR_DEV} -j MASQUERADE VLAN filtering\u003e VLAN filtering # VLAN filtering allows a single bridge to manage traffic for multiple VLANs, similar to a switch. This enables you to create a single bridge and separate network namespaces into different VLANs, preventing them from communicating with each other.\nWithout VLAN filtering: VLAN Filter Support on Bridge (Without VLAN Filtering) With VLAN filtering: Introduction to Linux Bridging Commands and Features (VLAN Filter) IPvlan\u003e IPvlan # IPvlan allows a single MAC address to have multiple IPs. This can be useful in Docker, where you might want a container to have an IP address in the same subnet as the host network.\nIPvlan Linux Networking\nip netns add ns0 ip link add link enp0s8 ipvl0 type ipvlan mode l2 ip link set dev ipvl0 netns ns0 ip netns exec ns0 bash ip link set dev ipvl0 up ip link set dev lo up ip addr add 192.168.0.16/24 dev ipvl0 ip addr add 127.0.0.1 dev lo ip route add default via 192.168.0.1 dev ipvl0 Vxlan\u003e Vxlan # Vxlan is an overlay network that can connect multiple Linux machines and build a network on top of it. This is used in systems like Kubernetes, where the node network is an underlay, and the pod network is an overlay. From the pod\u0026rsquo;s perspective, the node does not exist, and the overlay network appears as a single entity.\nVirtual Networking Labs: Overlay Networks\n","date":"2023-05-06","permalink":"/posts/linux-virtual-network/","section":"Posts","summary":"This post explores various networking concepts and configurations for Linux systems, including:\nbridging VLAN filtering IPvlan Vxlan These concepts are useful for creating and managing virtual networks.\nI\u0026rsquo;m sharing this information based on my own interest and learning experiences.","title":"[Note] Linux Virual Networking"},{"content":"","date":"2023-05-06","permalink":"/tags/bridge/","section":"Tags","summary":"","title":"bridge"},{"content":"","date":"2023-05-06","permalink":"/tags/docker/","section":"Tags","summary":"","title":"docker"},{"content":"","date":"2023-05-06","permalink":"/tags/ipvlan/","section":"Tags","summary":"","title":"ipvlan"},{"content":"","date":"2023-05-06","permalink":"/tags/linux/","section":"Tags","summary":"","title":"linux"},{"content":"","date":"2023-05-06","permalink":"/tags/networking/","section":"Tags","summary":"","title":"networking"},{"content":"","date":"2023-05-06","permalink":"/tags/vlan/","section":"Tags","summary":"","title":"vlan"},{"content":"","date":"2023-05-06","permalink":"/tags/vxlan/","section":"Tags","summary":"","title":"vxlan"},{"content":"","date":"2023-05-05","permalink":"/tags/acl/","section":"Tags","summary":"","title":"acl"},{"content":"","date":"2023-05-05","permalink":"/tags/alb/","section":"Tags","summary":"","title":"alb"},{"content":"Pacemaker is a powerful cluster resource manager designed to provide high availability (HA) and auto-failover capabilities for services and applications. In this blog post, we will cover some essential aspects of Pacemaker, from its basic functionality to different modes and installation.\nKey Points to Know About Pacemaker:\nRole in a Cluster: Pacemaker works like a \u0026lsquo;systemctl\u0026rsquo; for clusters, ensuring that services remain available and automatically failing over to another node if a primary node goes down.\nDefault Behavior: By default, Pacemaker maintains a single running service in the cluster and adds the service to other nodes when the primary node is down.\nOperating Modes: Pacemaker offers various modes, including clone mode and master-slave mode, to suit different cluster configurations and requirements.\nAuto-Failover: Auto-failover can be set up using a virtual IP (VIP) and configuring constraints for the \u0026lsquo;IPaddr2\u0026rsquo; resource to follow the master node.\nFencing: Pacemaker enables fencing by default to isolate and protect resources. However, it can be disabled by running the command pcs property set stonith-enabled=false.\nTwo-Node Clusters: For two-node clusters, it is necessary to disable quorum with the command pcs property set no-quorum-policy=ignore.\nPacemaker Web GUI: Pacemaker\u0026rsquo;s Web GUI is a fully functional command line interface that allows you to manage all aspects of the cluster.\nGalera and Master-Slave Mode: Galera is a special case of master-slave mode, where it operates as a multi-master configuration. To set up Galera, you need to:\nSet master-max = \u0026lt;no_of_node\u0026gt; Manually boot the cluster to generate grastate.dat Installing Pacemaker: To install Pacemaker, refer to the installation guide provided by ClusterLabs here. Pacemaker is a robust and flexible cluster resource manager that provides essential high availability and auto-failover capabilities for various services and applications. Understanding its core features and configuration options can help ensure that your cluster remains operational and resilient to failures.\n","date":"2023-05-05","permalink":"/posts/understanding-pacemaker-for-high-availability/","section":"Posts","summary":"Pacemaker is a powerful cluster resource manager designed to provide high availability (HA) and auto-failover capabilities for services and applications. In this blog post, we will cover some essential aspects of Pacemaker, from its basic functionality to different modes and installation.","title":"An Introduction to Pacemaker: Ensuring High Availability and Auto-Failover for Your Cluster"},{"content":"","date":"2023-05-05","permalink":"/tags/athena/","section":"Tags","summary":"","title":"athena"},{"content":"","date":"2023-05-05","permalink":"/tags/autofs/","section":"Tags","summary":"","title":"autofs"},{"content":"","date":"2023-05-05","permalink":"/tags/bind9/","section":"Tags","summary":"","title":"bind9"},{"content":"","date":"2023-05-05","permalink":"/tags/cloudtrail/","section":"Tags","summary":"","title":"cloudtrail"},{"content":"","date":"2023-05-05","permalink":"/tags/cloudwatch/","section":"Tags","summary":"","title":"cloudwatch"},{"content":"","date":"2023-05-05","permalink":"/tags/disk/","section":"Tags","summary":"","title":"disk"},{"content":"","date":"2023-05-05","permalink":"/tags/dns/","section":"Tags","summary":"","title":"dns"},{"content":"Enable ALB access logging for debugging:\nFollow this guide: https://docs.aws.amazon.com/elasticloadbalancing/latest/application/enable-access-logging.html Access logs storage:\nLogs are saved in S3 Difficult to view directly Log format:\nNot written to a single file New file with timestamp after a set interval Solution: Set up Athena to query logs\nFollow this guide: https://docs.aws.amazon.com/athena/latest/ug/application-load-balancer-logs.html ","date":"2023-05-05","permalink":"/posts/enabling-alb-access-logs-and-analyzing-with-athena/","section":"Posts","summary":"Enable ALB access logging for debugging:\nFollow this guide: https://docs.aws.amazon.com/elasticloadbalancing/latest/application/enable-access-logging.html Access logs storage:\nLogs are saved in S3 Difficult to view directly Log format:\nNot written to a single file New file with timestamp after a set interval Solution: Set up Athena to query logs","title":"Enabling ALB Access Logs and Analyzing with Athena"},{"content":"","date":"2023-05-05","permalink":"/tags/firewall/","section":"Tags","summary":"","title":"firewall"},{"content":"","date":"2023-05-05","permalink":"/tags/firewalld/","section":"Tags","summary":"","title":"firewalld"},{"content":"","date":"2023-05-05","permalink":"/tags/ha/","section":"Tags","summary":"","title":"ha"},{"content":"","date":"2023-05-05","permalink":"/tags/iptables/","section":"Tags","summary":"","title":"iptables"},{"content":"","date":"2023-05-05","permalink":"/tags/lvm/","section":"Tags","summary":"","title":"lvm"},{"content":"Working with LVM (Logical Volume Management) can simplify the process of managing disk space. This post will guide you through enlarging a disk in LVM, migrating a non-LVM file system to LVM, and migrating the root file system to LVM.\nEnlarging a Disk in LVM\u003e Enlarging a Disk in LVM # Refer to this article for more information: Manage and Create LVM Partition Using vgcreate, lvcreate, and lvextend.\nTo enlarge an existing LVM:\nCheck if the new disk is detected using lsblk. If not, scan it with: echo \u0026#34;- - -\u0026#34; \u0026gt; /sys/class/scsi_host/host\u0026lt;N=1,2,3,4...\u0026gt;/scan Create a physical volume, extend the volume group, and extend the logical volume: pvcreate \u0026lt;new_disk\u0026gt; vgextend \u0026lt;vg\u0026gt; \u0026lt;new_disk\u0026gt; lvextend -l +100%FREE -r \u0026lt;lvm\u0026gt; Resize the file system:\nFor ext4, use resize2fs. For xfs, use xfsgrow. You should able to see the resize disk on\ndf -h Migrating Non-LVM File System to LVM\u003e Migrating Non-LVM File System to LVM # For example, if you want to move /var to LVM:\nCreate the LVM. Mount it to a temporary location (e.g., /tmp/lvm). Copy data from /var to the LVM: rsync -avz /var/ /tmp/lvm/ Unmount it. Update /etc/fstab with the new LVM. Use UUID to reference the file system. Find the UUID with blkid. Refer to fstab documentation for more details.\nMigrating Root File System to LVM\u003e Migrating Root File System to LVM # Refer to Converting an Existing Root Filesystem to LVM Partition.\nThe steps are similar to migrating a non-LVM file system:\nRepeat the steps of migrating Non-LVM File System Perform additional steps: chroot to update initrd. Edit grub.conf with the new root. Restart the system. Following these steps will help you manage disk space using LVM, whether you need to enlarge a disk, migrate a non-LVM file system to LVM, or migrate the root file system to LVM.\n","date":"2023-05-05","permalink":"/posts/managing-disk-space-with-lvm-a-comprehensive-guide/","section":"Posts","summary":"Working with LVM (Logical Volume Management) can simplify the process of managing disk space. This post will guide you through enlarging a disk in LVM, migrating a non-LVM file system to LVM, and migrating the root file system to LVM.","title":"Managing Disk Space with LVM: A Comprehensive Guide"},{"content":"Reference: https://upcloud.com/resources/tutorials/configure-iptables-centos\nProblem: Non-persistent iptables rule after system restarts.\nThis is how to add a rule to iptables:\nsudo iptables -A INPUT -p tcp --dport ssh -j ACCEPT Solution:\nThis command applies the rule immediately, but it won\u0026rsquo;t persist after a system restart.\nTo make iptables rules persistent, follow these steps:\nInstall iptables-services: sudo yum install iptables-services Start and enable iptables: sudo systemctl start iptables sudo systemctl enable iptables Save the current rules to a file: sudo iptables-save \u0026gt; /etc/sysconfig/iptables To restore the rules, you can either restart iptables or use iptables-restore:\nsudo iptables-restore \u0026lt; /etc/sysconfig/iptables If you need to manage iptables rules alongside Docker, remember:\nAlways restart iptables before Docker: sudo systemctl restart iptables sudo systemctl restart docker If you need to add custom rules to iptables for Docker containers, use the DOCKER-USER chain. This chain is designed to allow user-defined rules.\nTo add a custom rule to the DOCKER-USER chain, for example, allowing HTTP access to Docker containers:\nsudo iptables -A DOCKER-USER -p tcp --dport 80 -j ACCEPT Save and restore the rules to ensure persistence across system restarts: sudo iptables-save \u0026gt; /etc/sysconfig/iptables sudo systemctl restart iptables sudo systemctl restart docker By following these practices, we can manage iptables rules effectively while preventing conflicts with Docker\u0026rsquo;s automatic iptables management. This approach ensures a seamless integration of iptables with Docker and maintains the security of the system.\n","date":"2023-05-05","permalink":"/posts/managing-iptables-as-a-service-and-integrating-with-docker/","section":"Posts","summary":"Reference: https://upcloud.com/resources/tutorials/configure-iptables-centos\nProblem: Non-persistent iptables rule after system restarts.\nThis is how to add a rule to iptables:\nsudo iptables -A INPUT -p tcp --dport ssh -j ACCEPT Solution:\nThis command applies the rule immediately, but it won\u0026rsquo;t persist after a system restart.","title":"Managing iptables as a Service and Integrating with Docker"},{"content":"After updating the command or env parameters in docker-compose.yml file, I restarted the container with docker-compose restart to make the change effect.\nHowever, I realize that the changes were not applied, and the container is still using the old config.\nTo apply the changes, I have to re-create the container:\ndocker-compose down docker-compose up -d This will stop and remove the existing container and create a new one with the updated config.\n","date":"2023-05-05","permalink":"/posts/mistake-when-updating-docker-compose/","section":"Posts","summary":"After updating the command or env parameters in docker-compose.yml file, I restarted the container with docker-compose restart to make the change effect.\nHowever, I realize that the changes were not applied, and the container is still using the old config.","title":"Mistake when updating docker-compose.yml"},{"content":"","date":"2023-05-05","permalink":"/tags/nat/","section":"Tags","summary":"","title":"nat"},{"content":"To set up a virtual networking lab on docker, I need to have the right to modify the network interface.\nOne way to achieve this is by running a Docker container with the --cap-add=NET_ADMIN flag, which grants the container the ability to configure network interfaces.\nAdditionally, adding --net=host flag to allow the container allow me to access the host network.\ndocker run -it --rm --cap-add=NET_ADMIN --net=host alpine Granting the NET_ADMIN capability and using the --net=host flag can be potentially dangerous because it allows a container to modify the network configuration of the host system.\nHowever, in a playground or testing environment, these capabilities can be useful for experimenting with different network configurations and scenarios.\nIt\u0026rsquo;s important to exercise caution and not use these capabilities in a production environment where security is a top priority.\n","date":"2023-05-05","permalink":"/posts/network-acl-in-containers-for-virtual-networking-labs/","section":"Posts","summary":"To set up a virtual networking lab on docker, I need to have the right to modify the network interface.\nOne way to achieve this is by running a Docker container with the --cap-add=NET_ADMIN flag, which grants the container the ability to configure network interfaces.","title":"Network ACL in Containers for Virtual Networking Labs"},{"content":"","date":"2023-05-05","permalink":"/tags/nfs/","section":"Tags","summary":"","title":"nfs"},{"content":"","date":"2023-05-05","permalink":"/tags/pacemaker/","section":"Tags","summary":"","title":"pacemaker"},{"content":"Setup guide\u003e Setup guide # With Ubuntu: https://www.digitalocean.com/community/tutorials/how-to-configure-bind-as-a-private-network-dns-server-on-ubuntu-20-04.\nWith Docker: https://github.com/hugotkk/lab-docker-bind.\nZone Transfer\u003e Zone Transfer # With zone transfer enable, when a DNS record is updated in the primary DNS server, the update will be transferred automatically to the secondary DNS server.\nIn the primary server, we should set\ntype private allow-transfer { \u0026lt;secondary-ip\u0026gt;; } In the secondary server, we should set\ntype slave; masters { \u0026lt;primary-ip\u0026gt; }; Recursive Queries\u003e Recursive Queries # Also, if we want to enable recursive queries (forwarding queries to a forwarder if the answer isn\u0026rsquo;t in the local DNS), we should only allow trusted servers to do so, especially if we\u0026rsquo;re a private DNS server.\nacl \u0026#34;trusted\u0026#34; { \u0026lt;trusted_servers\u0026gt;; }; options { .... recursion yes; allow-recursion { trusted; }; forwarders { 8.8.8.8; 8.8.4.4; }; To accomplish this, create an ACL that includes the IP addresses of trusted servers.\nIn the options section,\nenable recursion specify the trusted ACL in the allow-recursion. add one or more forwarders (e.g., public DNS servers such as 8.8.8.8 and 8.8.4.4) in the forwarders. ","date":"2023-05-05","permalink":"/posts/setting-up-a-master-slave-dns-server-with-bind9-on-ubuntu-and-docker/","section":"Posts","summary":"Setup guide\u003e Setup guide # With Ubuntu: https://www.digitalocean.com/community/tutorials/how-to-configure-bind-as-a-private-network-dns-server-on-ubuntu-20-04.\nWith Docker: https://github.com/hugotkk/lab-docker-bind.\nZone Transfer\u003e Zone Transfer # With zone transfer enable, when a DNS record is updated in the primary DNS server, the update will be transferred automatically to the secondary DNS server.","title":"Setting Up a Master-Slave DNS Server with Bind9 on Ubuntu \u0026 Docker"},{"content":"Set up NFS:\nFollow this guide (for ubuntu): https://www.digitalocean.com/community/tutorials/how-to-set-up-an-nfs-mount-on-ubuntu-20-04 Autofs is a file system management tool that offers several key features:\nDynamic Mounting: File systems are mounted on-demand when they are accessed, reducing the number of mounts that need to be managed manually. Improved Performance: By reducing the number of mounted file systems, Autofs can improve system performance by reducing the amount of system resources required to manage mounts. Increased Security: Autofs can increase system security by reducing the number of mounted file systems available at any given time. This reduces the attack surface of the system. Autofs mount methods:\nDirect Indirect Let\u0026rsquo;s say nfs server is 192.168.0.101.\nDirect mapping example:\nMap 192.168.0.101:/data to /data vi /etc/master.auto /- /etc/direct.auto /home /etc/home.auto vi /etc/direct.auto /data 192.168.0.101:/data Indirect mapping example:\nMap 192.168.0.101:/data/hugotse to /home/hugotse Map 192.168.0.101:/data/ to /home/ vi /etc/home.auto hugotse 192.168.0.101:/data/hugotse * 192.168.0.101:/data/\u0026amp; Autofs man page:\nhttps://manpages.ubuntu.com/manpages/focal/en/man5/autofs.5.html ","date":"2023-05-05","permalink":"/posts/setting-up-autofs-for-nfs-mounts/","section":"Posts","summary":"Set up NFS:\nFollow this guide (for ubuntu): https://www.digitalocean.com/community/tutorials/how-to-set-up-an-nfs-mount-on-ubuntu-20-04 Autofs is a file system management tool that offers several key features:\nDynamic Mounting: File systems are mounted on-demand when they are accessed, reducing the number of mounts that need to be managed manually.","title":"Setting Up Autofs for NFS Mounts"},{"content":"In this article, I will cover the basics of setting up firewalls on different distros, including firewalld, ufw, and iptables.\nWe will also discuss the daily tasks of firewall management, such as allowing or blocking specific source addresses/destination ports, port forwarding, and NAT.\nWeb Application Example:\u003e Web Application Example: # A web server at 192.168.0.100 A node.js app at 192.168.0.200:8000 that is not publicly accessible Tasks:\nAllow port 80 from public:\u003e Allow port 80 from public: # iptables -A INPUT -p tcp -m tcp --dport 80 -j ACCEPT firewall-cmd --zone=public --add-port=80/tcp --permanent ufw allow 80/tcp Allow SSH only from a trusted IP, e.g., 43.164.66.12:\u003e Allow SSH only from a trusted IP, e.g., 43.164.66.12: # iptables -A INPUT -p tcp -s 43.164.66.12 --dport 22 -j ACCEPT firewall-cmd --zone=public --add-rich-rule \u0026#39;rule family=\u0026#34;ipv4\u0026#34; source address=43.164.66.12 port port=22 accept\u0026#39; ufw allow from 43.164.66.12 to any port 22/tcp Forward port 8080 requests to 192.168.0.200:8000:\u003e Forward port 8080 requests to 192.168.0.200:8000: # iptables -t nat -A PREROUTING -p tcp --dport 8080 \\ -j DNAT --to-destination 192.168.0.200:8000 iptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE firewall-cmd --zone=public --add-masquerade firewall-cmd --zone=\u0026#34;public\u0026#34; --add-forward-port=port=8080:proto=tcp:toaddr=198.51.100.0:toport=8000 Drop traffic from bad IP, e.g., 233.228.5.86:\u003e Drop traffic from bad IP, e.g., 233.228.5.86: # firewall-cmd --zone=public --add-rich-rule \u0026#39;rule family=\u0026#34;ipv4\u0026#34; source address=233.228.5.86 drop\u0026#39; ufw deny from 233.228.5.86 When working with the NAT table, we need to enable ip_forward as well in /etc/ufw/sysctl.conf:\nnet.ipv4.ip_forward = 1 NAT Example:\u003e NAT Example: # A Linux router with IP 192.160.0.100 with internet access Internal servers 192.160.0.200, 192.160.0.201, 192.160.0.203 using 192.160.0.100 as router. Tasks:\nConfig NAT for internal server to have internet access:\u003e Config NAT for internal server to have internet access: # iptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE firewall-cmd --zone=public --add-masquerade By following these guidelines, you can set up your firewall effectively and ensure the security of your system.\nReference:\u003e Reference: # iptables allow/deny with filter NAT \u0026amp; port forwarding firewalld ufw ","date":"2023-05-05","permalink":"/posts/summary-of-setting-up-firewall-on-different-distros/","section":"Posts","summary":"In this article, I will cover the basics of setting up firewalls on different distros, including firewalld, ufw, and iptables.\nWe will also discuss the daily tasks of firewall management, such as allowing or blocking specific source addresses/destination ports, port forwarding, and NAT.","title":"Summary of Setting up Firewall on Different Distros"},{"content":"","date":"2023-05-05","permalink":"/tags/ufw/","section":"Tags","summary":"","title":"ufw"},{"content":"AWS logs\u0026rsquo; default interface can be challenging to navigate for in-depth analysis. Amazon Athena can help address two common issues.\nFirst, CloudTrail logs\u0026rsquo; default filters can be limiting. However, with Athena, you can use SQL to apply filters to each log field, allowing for more detailed analysis and improved insights.\nSecond, while CloudWatch Logs Insights is useful for log analysis, it lacks user-friendly options for exporting reports or searching historical data. Athena can help with this as well.\nTo get started with Athena, follow these steps:\nTo query CloudTrail logs using Athena, refer to Setting up CloudTrail Logs with Athena. To query CloudWatch logs using Athena and connectors, check out Setting up CloudWatch Logs Connector for Athena. ","date":"2023-05-05","permalink":"/posts/using-amazon-athena-for-easier-aws-log-analysis/","section":"Posts","summary":"AWS logs\u0026rsquo; default interface can be challenging to navigate for in-depth analysis. Amazon Athena can help address two common issues.\nFirst, CloudTrail logs\u0026rsquo; default filters can be limiting. However, with Athena, you can use SQL to apply filters to each log field, allowing for more detailed analysis and improved insights.","title":"Using Amazon Athena for Easier AWS Log Analysis"},{"content":"","date":"2023-05-05","permalink":"/tags/virtual-networking/","section":"Tags","summary":"","title":"virtual-networking"},{"content":"","date":"2023-05-05","permalink":"/tags/vpn/","section":"Tags","summary":"","title":"vpn"},{"content":"","date":"2023-05-05","permalink":"/tags/wireguard/","section":"Tags","summary":"","title":"wireguard"},{"content":"In this blog post, we will explore setting up WireGuard, a modern and secure VPN solution, as well as understanding the NAT approach and various network topologies used in VPN configurations.\nSetting Up WireGuard\u003e Setting Up WireGuard # Although the official WireGuard website doesn\u0026rsquo;t provide a configuration reference or examples, Stavros\u0026rsquo; blog post offers a clear guide on configuring WireGuard: How to Configure WireGuard. The WireGuard website does have a quickstart guide, but it focuses on command line setup: WireGuard Quickstart.\nwith NAT\u003e with NAT # For a lab setup using Docker, refer to this repository: docker-compose.yml.\nThe lab setup contains two networks:\ninternet (public subnet) hugo_home (private subnet) The \u0026ldquo;server\u0026rdquo; and \u0026ldquo;client\u0026rdquo; containers face the public network, with the VPN set up between them.\nThe goal is for the client to access services in the private subnet (e.g., \u0026ldquo;nginx\u0026rdquo;, \u0026ldquo;httpd\u0026rdquo;, \u0026ldquo;httpbin\u0026rdquo;) using NAT.\nMore use cases\u003e More use cases # Three topologies are discussed in this article: WireGuard Point-to-Site Routing.\nThese topologies resemble virtual networking architectures rather than VPN.\nSite-to-Site\u003e Site-to-Site # To set up a site-to-site connection with WireGuard:\nConfigure services behind SiteA and SiteB endpoints to use their respective endpoints as routers. On the router, add a rule to route traffic going to the other site\u0026rsquo;s prefix to the router at the other site\u0026rsquo;s endpoint. Port Forwarding\u003e Port Forwarding # Configure SiteA\u0026rsquo;s endpoint to access services behind SiteB\u0026rsquo;s endpoint, as explained in Summary of Setting Up Firewall on Different Distros. Point-to-Site (Site Gateway)\u003e Point-to-Site (Site Gateway) # Configure SiteA\u0026rsquo;s endpoint to use SiteB\u0026rsquo;s endpoint as the router. SiteB\u0026rsquo;s endpoint will configure a route to direct traffic from SiteA\u0026rsquo;s endpoint prefix to SiteA\u0026rsquo;s endpoint directly. By following these guidelines, we can set up WireGuard VPN, understand the NAT approach, and apply various network topologies for needs.\n","date":"2023-05-05","permalink":"/posts/wireguard-vpn-setup-and-network-topologies/","section":"Posts","summary":"In this blog post, we will explore setting up WireGuard, a modern and secure VPN solution, as well as understanding the NAT approach and various network topologies used in VPN configurations.","title":"WireGuard VPN Setup and Network Topologies"},{"content":"Comparing varchar DateTime:\nUse \u0026lsquo;\u0026gt;\u0026rsquo; and \u0026lsquo;\u0026lt;\u0026rsquo; operators Example: time \u0026gt;= \u0026#39;2022-07-20T00:00:00Z\u0026#39; Comparing int64 DateTime:\nConvert int64 and varchar to timestamp: FROM_UNIXTIME(time/1000) \u0026lt; TIMESTAMP \u0026#39;2022-07-20 00:00:00\u0026#39; Convert int64 to varchar: DATE_FORMAT(FROM_UNIXTIME(time/1000), \u0026#39;%Y-%m-%dT%H:%i:%sZ\u0026#39;) \u0026lt;= \u0026#39;2022-02-13T00:00:00Z\u0026#39; ","date":"2023-05-05","permalink":"/posts/working-with-datetime-in-aws-athena/","section":"Posts","summary":"Comparing varchar DateTime:\nUse \u0026lsquo;\u0026gt;\u0026rsquo; and \u0026lsquo;\u0026lt;\u0026rsquo; operators Example: time \u0026gt;= \u0026#39;2022-07-20T00:00:00Z\u0026#39; Comparing int64 DateTime:\nConvert int64 and varchar to timestamp: FROM_UNIXTIME(time/1000) \u0026lt; TIMESTAMP \u0026#39;2022-07-20 00:00:00\u0026#39; Convert int64 to varchar: DATE_FORMAT(FROM_UNIXTIME(time/1000), \u0026#39;%Y-%m-%dT%H:%i:%sZ\u0026#39;) \u0026lt;= \u0026#39;2022-02-13T00:00:00Z\u0026#39; ","title":"Working with DateTime in AWS Athena"},{"content":"","date":"2023-05-04","permalink":"/tags/2fa/","section":"Tags","summary":"","title":"2fa"},{"content":"There are two ways to mount volumes in Docker:\nHostpath: docker run -it --rm -v $PWD/myvol:/app ubuntu Volume: docker volume create myvol docker run -it --rm -v myvol:/app ubuntu When dealing with volume ownership, incorrect folder ownership can cause the container to crash, requiring time to debug logs.\nCommon issues:\nPermission denied errors in logs Bitnami images built with non-root user OpenShift starts containers with random UID Examples of volume permissions in Docker:\nPrometheus:\nData store: /prometheus Config path: /etc/prometheus Rules for volume ownership and permissions:\nIf the host volume doesn\u0026rsquo;t exist, Docker daemon will create it with root ownership. Example:\ndocker run -it --rm -v $PWD/data:/data prometheus $PWD/data will be owned by root.\nIf the host volume doesn\u0026rsquo;t exist but the path exists in the container, a new volume will be created, and the files inside that path will be copied to it. Ownership will follow the one inside the container. Example:\ndocker run -it --rm -v $PWD/data:/etc/prometheus prometheus $PWD/data ownership will be the same as /etc/prometheus. Files in /etc/promethues will be copied as well.\nIf the volume exists, it will follow its own ownership. Example: (@ubuntu)\nmkdir data docker run -it --rm -v $PWD/data:/etc/prometheus prometheus $PWD/myvol will be owned by ubuntu.\nIssues arise when the container is running as a non-root user. In these cases, folders may be writable by the container or the host, but not both.\nSolutions for non-root user containers (not ideal):\nStart the container with root user (may not work for applications that require a specific user, e.g., MariaDB).\nOn the host, create the folder first, then use sudo to change the volume owner to the container user.\nChange the folder permission to 777, allowing both host and container to write to it.\n","date":"2023-05-04","permalink":"/posts/docker-volume-mounting-permissions-and-ownership-explained/","section":"Posts","summary":"There are two ways to mount volumes in Docker:\nHostpath: docker run -it --rm -v $PWD/myvol:/app ubuntu Volume: docker volume create myvol docker run -it --rm -v myvol:/app ubuntu When dealing with volume ownership, incorrect folder ownership can cause the container to crash, requiring time to debug logs.","title":"Docker Volume Mounting: Permissions and Ownership Explained"},{"content":"","date":"2023-05-04","permalink":"/tags/google-authentication/","section":"Tags","summary":"","title":"google-authentication"},{"content":" HostPath: Shares storage between nodes Can be used for data persistence across container restarts EmptyDir: Independent for each pod Shared between containers within a pod Data is not persisted across container restarts ","date":"2023-05-04","permalink":"/posts/k8s-volumes-hostpath-and-emptydir-explained/","section":"Posts","summary":" HostPath: Shares storage between nodes Can be used for data persistence across container restarts EmptyDir: Independent for each pod Shared between containers within a pod Data is not persisted across container restarts ","title":"k8s Volumes: HostPath and EmptyDir Explained"},{"content":"","date":"2023-05-04","permalink":"/tags/mount/","section":"Tags","summary":"","title":"mount"},{"content":"Docker\u003e Docker # ubuntu@ip-172-31-6-200:~$ docker run --rm -it -P httpd ubuntu@ip-172-31-6-200:~$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 6f6f53ee7dde httpd \u0026#34;httpd-foreground\u0026#34; 12 seconds ago Up 10 seconds 0.0.0.0:32768-\u0026gt;80/tcp, :::32768-\u0026gt;80/tcp dreamy_goodall Docker automatically knows which ports a container needs due to the EXPOSE directive in the container\u0026rsquo;s Dockerfile. (httpd\u0026rsquo;s Dockerfile) Removing EXPOSE from the Dockerfile will not publish the port automatically anymore when using the -P flag. Howerver, containers can still reach ports within the container via its IP/alias. Docker does not enforce any security policies between containers. We can test it by:\nCreate a custom network Start two containers, connected to the same network Install net-tools on both containers Listen to port 3000 on container u1 Connect to u1:3000 on container u2 Connection successful! On host:\ndocker network create mynetwork docker run -it --rm --name u1 --network mynetwork ubuntu docker run -it --rm --name u2 --network mynetwork ubuntu On both container:\napt update apt install -y iproute2 netcat On u1:\nroot@b7bfb1fa159e:/# while true; do nc -l 3000; done On u2:\nroot@1bd1cee0ce53:/# nc b7bfb1fa159e 3000 -v Connection to b7bfb1fa159e (172.18.0.2) 3000 port [tcp/*] succeeded! Kubernetes\u003e Kubernetes # Pods can reach each other, even in different namespaces. Containers only need to listen on a port inside, and there is not nessessary to specify the exposed port in the Dockerfile or mention the pod config. The ports field in the container object is used to list the ports to expose from the container. Not specifying a port here does not prevent that port from being exposed. Any port that is listening on the default 0.0.0.0 address inside a container will be accessible from the network. Modifying the ports array with a strategic merge patch may corrupt the data. The ports field is more like a self-document purpose. Reference: https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/pod-v1/#ports\nWe can re-test it in k8s, starting the pods in different namespace.\nOn host:\nkubectl create ns ns1 kubectl create ns ns2 kubectl run -n ns1 -it u1 --image=ubuntu kubectl run -n ns2 -it u2 --image=ubuntu repeat what we did in Docker - u2 is able to connect to u1:3000 with ip.\n","date":"2023-05-04","permalink":"/posts/networking-in-docker-and-kubernetes-port-expose-and-pod-communication/","section":"Posts","summary":"Docker\u003e Docker # ubuntu@ip-172-31-6-200:~$ docker run --rm -it -P httpd ubuntu@ip-172-31-6-200:~$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 6f6f53ee7dde httpd \u0026#34;httpd-foreground\u0026#34; 12 seconds ago Up 10 seconds 0.","title":"Networking in Docker and Kubernetes: Port Expose and Pod Communication"},{"content":"","date":"2023-05-04","permalink":"/tags/security/","section":"Tags","summary":"","title":"security"},{"content":"","date":"2023-05-04","permalink":"/tags/service-account/","section":"Tags","summary":"","title":"service-account"},{"content":"Follow Digital Ocean tutorial for the setup: https://www.digitalocean.com/community/tutorials/how-to-set-up-multi-factor-authentication-for-ssh-on-centos-7\nNotes on /etc/ssh/sshd_config:\nWith UsePAM yes, both password and keyboard-interactive follow /etc/pam.d/sshd PasswordAuthentication yes is the same as AuthenticationMethods password Differences between AuthenticationMethods password and keyboard-interactive:\npassword accepts only 1 response from the user keyboard-interactive accepts multiple responses from user Enforcing both password and Google Authenticator to login:\nUse keyboard-interactive to ensure successful authentication with 2FA and password Prompt differences:\npassword: (hugo@192.168.0.14) Password: keyboard-interactive: Password: Time synchronization:\nCrucial for accurate authentication For Docker, ensure the host\u0026rsquo;s clock is synchronized ","date":"2023-05-04","permalink":"/posts/setting-up-google-auth-on-ssh-on-centos-7/","section":"Posts","summary":"Follow Digital Ocean tutorial for the setup: https://www.digitalocean.com/community/tutorials/how-to-set-up-multi-factor-authentication-for-ssh-on-centos-7\nNotes on /etc/ssh/sshd_config:\nWith UsePAM yes, both password and keyboard-interactive follow /etc/pam.d/sshd PasswordAuthentication yes is the same as AuthenticationMethods password Differences between AuthenticationMethods password and keyboard-interactive:","title":"Setting Up Google Auth on SSH on CentOS-7"},{"content":"","date":"2023-05-04","permalink":"/tags/ssh/","section":"Tags","summary":"","title":"ssh"},{"content":"The token will be stored at /var/run/secrets/kubernetes.io/serviceaccount/token in container. ( See here)\nThree ways to use:\nBy code: Kubernetes Python library With kubectl binary: it will use the token automatically as your credentials. So we can do kubectl get po within the pod curl: eg: TOKEN=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token) API_ENDPOINT=$KUBERNETES_SERVICE_HOST:$KUBERNETES_SERVICE_PORT_HTTPS curl --cacert /var/run/secrets/kubernetes.io/serviceaccount/ca.crt \\ -H \u0026#39;Authentification: Bearer $TOKEN\u0026#39; \\ https://$API_ENDPOINT/v1/api/pods ","date":"2023-05-04","permalink":"/posts/three-ways-to-use-kubernetes-service-account-token/","section":"Posts","summary":"The token will be stored at /var/run/secrets/kubernetes.io/serviceaccount/token in container. ( See here)\nThree ways to use:\nBy code: Kubernetes Python library With kubectl binary: it will use the token automatically as your credentials.","title":"Three Ways to Use Kubernetes Service Account Token"},{"content":"","date":"2023-05-04","permalink":"/tags/ubuntu/","section":"Tags","summary":"","title":"ubuntu"},{"content":"Working with Ubuntu 22.04 autoinstall can be challenging due to incomplete documentation and time-consuming debugging. This post shares my experience and offers tips to help others facing similar issues.\nHere is the documentation of autoinstall and curtain.\nAnd my work: https://github.com/hugotkk/labs/blob/main/ubuntu-quickstart/user-data\nRemember the #cloud-config Header\u003e Remember the #cloud-config Header # The #cloud-config header is crucial for a successful autoinstall. Without it, the process won\u0026rsquo;t work.\nCustomizing Storage: Fixing the \u0026lsquo;fstype\u0026rsquo; Error\u003e Customizing Storage: Fixing the \u0026lsquo;fstype\u0026rsquo; Error # While customizing storage, I encountered an error: \u0026ldquo;LVM_LogicalVolume object has no attribute \u0026lsquo;fstype\u0026rsquo;.\u0026rdquo; To resolve this, modify the \u0026lsquo;mount\u0026rsquo; reference to \u0026lsquo;format\u0026rsquo; instead of LVM, as shown below:\n- id: lv-root type: lvm_partition name: root volgroup: vg size: 10G - id: root-fs type: format fstype: ext4 volume: lv-root - id: mount-root type: mount path: / device: lv-root # should reference to root-fs not lv-root Installing Software in late-commands\u003e Installing Software in late-commands # When trying to install software in the late-commands section, keep in mind that during autoinstall, Ubuntu is mounted on /target, not /. To address this, use curtin:\nlate-commands: - curtin in-target --target=/target -- \u0026lt;INSTALL_CMD_LIKE_APT_INSTALL\u0026gt; Two Ways to Use autoinstall\u003e Two Ways to Use autoinstall # There are two methods for using autoinstall: over HTTP and with an additional volume. I found the latter more practical in my case as I\u0026rsquo;m using Virtualbox.\n","date":"2023-05-04","permalink":"/posts/ubuntu-2204-autoinstall/","section":"Posts","summary":"Working with Ubuntu 22.04 autoinstall can be challenging due to incomplete documentation and time-consuming debugging. This post shares my experience and offers tips to help others facing similar issues.\nHere is the documentation of autoinstall and curtain.","title":"Ubuntu 22.04 autoinstall"},{"content":"","date":"2023-05-04","permalink":"/tags/volume/","section":"Tags","summary":"","title":"volume"},{"content":"","date":"2023-04-26","permalink":"/tags/monitor/","section":"Tags","summary":"","title":"monitor"},{"content":"","date":"2023-04-26","permalink":"/tags/prometheus/","section":"Tags","summary":"","title":"prometheus"},{"content":"An example of Prometheus data:\nhttp_request_total{method=\u0026#34;GET\u0026#34;, endpoint=\u0026#34;/contact-us\u0026#34;, status=\u0026#34;200\u0026#34;} 1 2 3 4 5 http_request_total{method=\u0026#34;POST\u0026#34;, endpoint=\u0026#34;/auth\u0026#34;, status=\u0026#34;400\u0026#34;} 1 6 8 10 15 Key Concepts\u003e Key Concepts # Metric: Quantity measurement (e.g.: http_request_total) Metric label: Metadata for the measurement (e.g.: method=\u0026quot;GET\u0026quot;) Sample: Data point at a certain time (e.g.: 5) - float64 Series: Unique combination of metric labels (e.g.: http_request_total{method=\u0026quot;GET\u0026quot;, endpoint=\u0026quot;/contact-us\u0026quot;, status=\u0026quot;200\u0026quot;} and http_request_total{method=\u0026quot;POST\u0026quot;, endpoint=\u0026quot;/auth\u0026quot;, status=\u0026quot;400\u0026quot;}) Time series: Samples over time (e.g.: 1 2 3 4 5) Data Types\u003e Data Types # Instant vector: http_request_total{method=\u0026quot;GET\u0026quot;} Range vector: http_request_total{method=\u0026quot;GET\u0026quot;}[5m] Scalar: numbers Metric Types\u003e Metric Types # Prometheus supports four metric types:\nGauge: Values can go up and down (e.g.: logged_users) Counter: Values can only increase (e.g.: http_request_total) Histogram: Provides \u0026lt;metric_name\u0026gt;_bucket, \u0026lt;metric_name\u0026gt;_sum, \u0026lt;metric_name\u0026gt;_count. Use histogram_quantile() for server-side quantile calculation (e.g.: http_request_duration_seconds) Summary: Similar to Histogram, but quantiles are calculated client-side (application). Thus, it cannot be further aggregated. promql\u003e promql # Operator Precedence\u003e Operator Precedence # Prometheus supports a range of binary operators with different precedence levels. From highest to lowest precedence:\n^ *, /, %, atan2 +, - ==, !=, \u0026lt;=, \u0026lt;, \u0026gt;=, \u0026gt; and, unless or Reference\nModifiers\u003e Modifiers # @ 1609746000 - pretend the query time is 1609746000 offset 5m - pretend the query time is 5 minutes ago Have to use right after the select (before any function call)\nVector Matching\u003e Vector Matching # Vector scalar:\nExample: http_request_total / 2 Vector Vector:\nTypes of matching: One-to-One One-to-Many Many-to-One Matches vectors using labels by default Customize matching key with ignore() or in() Use group_right() or group_left() for many side Use group_left(labels) to bring labels from one to many side Example:\nmethod_code:http_errors:rate5m{method=\u0026#34;get\u0026#34;, code=\u0026#34;500\u0026#34;} 24 method_code:http_errors:rate5m{method=\u0026#34;get\u0026#34;, code=\u0026#34;404\u0026#34;} 30 method_code:http_errors:rate5m{method=\u0026#34;put\u0026#34;, code=\u0026#34;501\u0026#34;} 3 method_code:http_errors:rate5m{method=\u0026#34;post\u0026#34;, code=\u0026#34;500\u0026#34;} 6 method_code:http_errors:rate5m{method=\u0026#34;post\u0026#34;, code=\u0026#34;404\u0026#34;} 21 method:http_requests:rate5m{method=\u0026#34;get\u0026#34;, foo=\u0026#34;bar\u0026#34;} 600 method:http_requests:rate5m{method=\u0026#34;del\u0026#34;, foo=\u0026#34;bar1\u0026#34;} 34 method:http_requests:rate5m{method=\u0026#34;post\u0026#34;, foo=\u0026#34;bar2\u0026#34;} 12 method_code:http_errors:rate5m{code=\u0026#34;500\u0026#34;} / ignoring(code) group_left(foo) method:http_requests:rate5m {method=\u0026#34;get\u0026#34;, code=\u0026#34;500\u0026#34;, foo=\u0026#34;bar\u0026#34;} 0.04 {method=\u0026#34;get\u0026#34;, code=\u0026#34;404\u0026#34;, foo=\u0026#34;bar\u0026#34;} 0.05 {method=\u0026#34;post\u0026#34;, code=\u0026#34;500\u0026#34;, foo=\u0026#34;bar2\u0026#34;} 0.05 {method=\u0026#34;post\u0026#34;, code=\u0026#34;404\u0026#34;, foo=\u0026#34;bar2\u0026#34;} 0.175 If no group_left(foo), foo=”bar” will gone\nReference\nCommon Prometheus Functions\u003e Common Prometheus Functions # changes(): Number of changes over time time(): Current timestamp timestamp(): Timestamp of the sample Derivative and Rate: deriv(): gauge; rate(), irate(): counter Delta and Increase: delta(), idelta(): gauge increase(): counter irate() vs rate(): irate(): (last - first datapoint)/time range rate(): (projected end - start time datapoint)/time range Aggregration: \u0026lt;aggregation\u0026gt;: sum, count, max, min, avg, etc: Aggregates across dimensions (group by labels) \u0026lt;aggregation\u0026gt;_over_time(): Aggregates across time (group by time) Examples:\nsum(http_request_total) Result:\n{} 9 sum_over_time(http_request_total{method=\u0026#34;GET\u0026#34;}[5m]) Result:\n{method=\u0026#34;GET\u0026#34;, endpoint=\u0026#34;/contact-us\u0026#34;, status=\u0026#34;200\u0026#34;} 10 # 1+2+3+4+5 {method=\u0026#34;POST\u0026#34;, endpoint=\u0026#34;/auth\u0026#34;, status=\u0026#34;400\u0026#34;} 25 #1+6+8+10+15 Reference\nPrometheus Client Library Usage\u003e Prometheus Client Library Usage # Instrumentation Writing exporters Pushing metrics to Pushgateway Gist reference\nStorage\u003e Storage # Not recommended to use NFS for storage: reference for storage Agent Mode\u003e Agent Mode # Disables query, alert, and recording rule functions Scrapes metrics from target and remotely writes to other instances Reference Service Discovery\u003e Service Discovery # Static: Define target servers in the config file *_sd_config: Use built-in configurations (e.g.: EC2, Kubernetes, file) Custom: Use file_sd_config. Update the file periodically. Each scrape config can have:\ninterval timeout proxy metrics_path Relabeling\u003e Relabeling # relabel_configs: Modify scrape parameters before scraping (e.g.: Blackbox exporter) metrics_relabel_configs: Modify data collected after scraping (e.g.: remove unwanted metrics) Alerting in Prometheus\u003e Alerting in Prometheus # Evaluates rules, fires alerts, routes to destination Does not handle notifications Routes by matching rules with labels Labels: alert identity Annotations: longer-form description Annotations support templating with go lang syntax Reference labels in annotations can be done by {{ $labels.foo }} Alertmanager\u003e Alertmanager # Silencing alerts use cases:\nProvisioning new servers Decommissioning servers Maintenance Inhibiting:\nStop a group of alerts when another alert is triggered Example: Cluster down alert inhibits memory or disk check alerts ","date":"2023-04-26","permalink":"/posts/note-about-prometheus-2/","section":"Posts","summary":"An example of Prometheus data:\nhttp_request_total{method=\u0026#34;GET\u0026#34;, endpoint=\u0026#34;/contact-us\u0026#34;, status=\u0026#34;200\u0026#34;} 1 2 3 4 5 http_request_total{method=\u0026#34;POST\u0026#34;, endpoint=\u0026#34;/auth\u0026#34;, status=\u0026#34;400\u0026#34;} 1 6 8 10 15 Key Concepts\u003e Key Concepts # Metric: Quantity measurement (e.g.: http_request_total) Metric label: Metadata for the measurement (e.","title":"Understanding Prometheus - Metrics, Data Types, and Querying"},{"content":"","date":"2023-04-19","permalink":"/tags/ad/","section":"Tags","summary":"","title":"ad"},{"content":"","date":"2023-04-19","permalink":"/tags/adfs/","section":"Tags","summary":"","title":"adfs"},{"content":"","date":"2023-04-19","permalink":"/tags/api-gateway/","section":"Tags","summary":"","title":"api-gateway"},{"content":"Two connections maintained\u003e Two connections maintained # ALB to target servers. Client to ALB. Keepalive/h2 setting\u003e Keepalive/h2 setting # Applies to ALB-to-target connection, not client-to-target servers. Why Keepalive help to improve the performance: Limited number of connections between ALB to target servers, keepalive helps. Sticky session\u003e Sticky session # Cookie-based. Idle_timeout\u003e Idle_timeout # Applies to both ALB-to-target and client-to-ALB connections. Closes connections without data sent/received within timeout period. Timeout configuration\u003e Timeout configuration # ALB idle_timeout should be longer than the application timeout. Prevents traffic being sent to dead targets and causing 502 errors. ","date":"2023-04-19","permalink":"/posts/aws-alb/","section":"Posts","summary":"Two connections maintained\u003e Two connections maintained # ALB to target servers. Client to ALB. Keepalive/h2 setting\u003e Keepalive/h2 setting # Applies to ALB-to-target connection, not client-to-target servers. Why Keepalive help to improve the performance: Limited number of connections between ALB to target servers, keepalive helps.","title":"AWS ALB"},{"content":"WebSocket Overview\u003e WebSocket Overview # WebSocket Characteristics Bi-directional, persistent TCP connection between client and server Scaling Limitations WebSocket is stateful, cannot horizontally scale without a backend to store state (eg: Redis) API Gateway with Websocket\u003e API Gateway with Websocket # Benefits\nAids scaling by maintaining WebSocket connections Serverless - Client and Lambda interact with API Gateway Lambda uses APIGatewayConnection API to send data to open connections Client sends data to gateway directly Limitations\nWebSocket connection lasts for 2 hours Idle timeout is 10 minutes Lab: Building a Chat Room\u003e Lab: Building a Chat Room # Setup\u003e Setup # I watched this tutorial on how to build a chat using Lambda, WebSocket, and API Gateway with Node.js\nAccess the code in this GitHub Gist\nServer Setup\nserver.py is a simple HTTP server with Python to serve the client-side code Lambda Function\nindex.mjs is the Lambda code for the chat room Implements $connect, $disconnect, join, leave, and sendAll functions Uses DynamoDB to store connectionIds and roomId with the connectionIds that joined Common Errors and Solutions\u003e Common Errors and Solutions # Global Variable Persistence\u003e Global Variable Persistence # The video stores the connections and rooms data in global variables in lambda. Initially, it was thought that it would not work because each invocation is stateless. However, variables in the global scope can be shared between Lambda invocations. This is mentioned in the Comparing the effect of global scope. However, it is more reliable to use a database for storing data. The use of global variables should be limited to demo purposes only and should never be used in the real world. Connection Endpoint\u003e Connection Endpoint # The endpoints can be found in the AWS Console under \u0026ldquo;API Gateway\u0026rdquo; \u0026gt; \u0026ldquo;Stages\u0026rdquo;. One is WebSocket, starting with wss://, and another is HTTPS, starting with https://. When posting the data back to the client from Lambda, use the HTTPS endpoint but remove @connections at the end. For example, if the endpoint on the console is https://d6sq1c52q5.execute-api.us-east-1.amazonaws.com/production/@connections, we should omit the @connection when using PostToConnectionCommand. Lambda Permissions\u003e Lambda Permissions # Lambda needs permission to call invoke and manage connection Use AmazonAPIGatewayInvokeFullAccess or create a custom policy Custom policy example:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;execute-api:Invoke\u0026#34;, \u0026#34;execute-api:ManageConnections\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:execute-api:us-east-1:059035646743:d8ck9i69wa/production/*\u0026#34;\u0026#34; } ] } Refer to the AWS documentation for more information.\n\u0026ldquo;Going Away\u0026rdquo; Exception\u003e \u0026ldquo;Going Away\u0026rdquo; Exception # You might encounter a \u0026ldquo;Going away\u0026rdquo; exception if you try to post data to a connection in $connect This is not supported, as mentioned in this Stack Overflow post Using Node.js 18 in Lambda Function\u003e Using Node.js 18 in Lambda Function # If you use Node.js 18, require('aws-sdk') will not work due to the use of ES module style When connecting to the WebSocket, it may return a 502 error, which is also logged in the API Gateway access log but not useful for debugging purposes. To diagnose the issue, check the Lambda error log on CloudWatch To tail the CloudWatch log, use the following command:\naws logs tail \u0026lt;log_group\u0026gt; --follow Refer to the AWS CLI documentation for more information.\n","date":"2023-04-19","permalink":"/posts/aws-api-gateway-wss/","section":"Posts","summary":"WebSocket Overview\u003e WebSocket Overview # WebSocket Characteristics Bi-directional, persistent TCP connection between client and server Scaling Limitations WebSocket is stateful, cannot horizontally scale without a backend to store state (eg: Redis) API Gateway with Websocket\u003e API Gateway with Websocket # Benefits","title":"Building a Chat Room with Lambda and Websocket in AWS API Gateway"},{"content":"To enable authorization the api with Amazon Cognito User Pools:\nCreate a user pool. Check out Secure your API Gateway with Amazon Cognito User Pools for a video tutorial. In the \u0026ldquo;Method Request\u0026rdquo; \u0026gt; \u0026ldquo;Auth\u0026rdquo; section of the API Gateway console, select the user pool. Access the API with an ID token: curl --location \u0026#39;https://\u0026lt;my_api_gateway_domain\u0026gt;/\u0026lt;my_api\u0026gt;\u0026#39; \\ --header \u0026#39;Authorization: Bearer \u0026lt;id_token\u0026gt;\u0026#39; ``` To generate the ID token from Cognito:\nGet an authorization token with the authorize API: https://\u0026lt;my_cognito_domain\u0026gt;/oauth2/authorize?response_type=code\u0026amp;client_id=\u0026lt;client_id\u0026gt;\u0026amp;redirect_uri=\u0026lt;redirect_url\u0026gt;\u0026amp;state=STATE\u0026amp;scope=openid%20email Use the authorization token to exchange it for an ID token from the token API: curl --location \u0026#39;https://\u0026lt;my_cognito_domain\u0026gt;/oauth2/token\u0026#39; \\ --header \u0026#39;Content-Type: application/x-www-form-urlencoded\u0026#39; \\ --data-urlencode \u0026#39;grant_type=authorization_code\u0026#39; \\ --data-urlencode \u0026#39;client_id=\u0026lt;client_id\u0026gt;\u0026#39; \\ --data-urlencode \u0026#39;client_secret=\u0026lt;client_secret\u0026gt;\u0026#39; \\ --data-urlencode \u0026#39;code=\u0026lt;auth_token\u0026gt;\u0026#39; \\ --data-urlencode \u0026#39;redirect_uri=\u0026lt;redirect_uri\u0026gt;\u0026#39; ``` For more details about the authorize and token endpoints, check out the following links:\nAuthorize endpoint Token endpoint A more straightforward way to obtain the tokens is by using Postman, which supports OAuth 2.0 authentication. Check out OAuth2 0 Authorization with Postman for a video tutorial.\n","date":"2023-04-19","permalink":"/posts/aws-api-gateway-with-cognito/","section":"Posts","summary":"To enable authorization the api with Amazon Cognito User Pools:\nCreate a user pool. Check out Secure your API Gateway with Amazon Cognito User Pools for a video tutorial. In the \u0026ldquo;Method Request\u0026rdquo; \u0026gt; \u0026ldquo;Auth\u0026rdquo; section of the API Gateway console, select the user pool.","title":"Building a Secure API Gateway with Cognito"},{"content":"","date":"2023-04-19","permalink":"/tags/cognito/","section":"Tags","summary":"","title":"cognito"},{"content":"Main difference\u003e Main difference # REST HTTP Quota Management Per group Not supported API key Management Supported Not supported Authorization Lambda/Cognito Lambda/IAM/JWT Lambda Input Payload only Request details (Event) VTL model Supported Not supported SDK and Documentation Generation Supported Not supported Lambda integration\u003e Lambda integration # Both REST and HTTP Lambda integrations offer a powerful and flexible way to integrate Lambda functions with API Gateway, with some differences in input/output format and response handling.\nInput / Output Format\u003e Input / Output Format # Lambda Proxy Integration in REST and Lambda Integration in HTTP both pass the request information to the event object in the Lambda function. However, the input and output format are different. REST Lambda Proxy Integration Input Format HTTP Lambda Integration Proxy Format In HTTP APIs, if the Lambda function returns valid JSON and doesn\u0026rsquo;t return a statusCode or a string, API Gateway assumes that the result is the body of the response. In REST APIs, the Lambda function has to construct the response object with the appropriate status code, headers, and body. This is not possible in HTTP APIs. VTL\u003e VTL # In REST APIs, the Lambda function can use VTL templates to transform the response data. VTL templates allow for specifying how the response data should be transformed from one format to another. In HTTP APIs, only JSON data is supported and response transformation using VTL templates is not possible. * The VTL is used in REST APIs for the following:\nRequest \u0026amp; Response Validation Generate an SDK Initialize a mapping template Data Transformations\u003e Data Transformations # Integration Type Path/ Query/ Header Mapping Template lambda N/A Y lambda proxy N N http Y Y http proxy Y Y Data that Backend Receives\u003e Data that Backend Receives # Integration Type Path Query Headers Payload lambda N N N Y lambda proxy Y Y Y Y http Y Y Y Y http proxy Y Y Y Y ","date":"2023-04-19","permalink":"/posts/aws-api-gateway-comparson/","section":"Posts","summary":"Main difference\u003e Main difference # REST HTTP Quota Management Per group Not supported API key Management Supported Not supported Authorization Lambda/Cognito Lambda/IAM/JWT Lambda Input Payload only Request details (Event) VTL model Supported Not supported SDK and Documentation Generation Supported Not supported Lambda integration\u003e Lambda integration # Both REST and HTTP Lambda integrations offer a powerful and flexible way to integrate Lambda functions with API Gateway, with some differences in input/output format and response handling.","title":"Comparing HTTP and REST API Gateways"},{"content":"","date":"2023-04-19","permalink":"/tags/devops/","section":"Tags","summary":"","title":"devops"},{"content":"","date":"2023-04-19","permalink":"/tags/display-filter/","section":"Tags","summary":"","title":"display-filter"},{"content":"","date":"2023-04-19","permalink":"/tags/dynamodb/","section":"Tags","summary":"","title":"dynamodb"},{"content":"","date":"2023-04-19","permalink":"/tags/ecs/","section":"Tags","summary":"","title":"ecs"},{"content":"","date":"2023-04-19","permalink":"/tags/iam/","section":"Tags","summary":"","title":"iam"},{"content":"","date":"2023-04-19","permalink":"/tags/identity-provider/","section":"Tags","summary":"","title":"identity-provider"},{"content":"","date":"2023-04-19","permalink":"/tags/lambda/","section":"Tags","summary":"","title":"lambda"},{"content":"","date":"2023-04-19","permalink":"/tags/nlb/","section":"Tags","summary":"","title":"nlb"},{"content":"","date":"2023-04-19","permalink":"/tags/secret-manager/","section":"Tags","summary":"","title":"secret-manager"},{"content":" Create a new Windows Server 2012 instance and install the following Roles and Features:\nDNS ADFS AD Promote the server to a Domain Controller and create a new forest. I named mine hhuge9.com.\nFollow a tutorial (such as this one: https://www.youtube.com/watch?v=9eq3IeDAkvA) to configure ADFS.\nYou can skip the process of generating the certificate in those tutorial, as it can be self-signed and needs to be in PFX format for ADFS to use it.\nIf you only have an OpenSSH key and certificate, you can convert it to PFX format using the following command:\nopenssl pkcs12 -export -out certificate.pfx -inkey hhuge9.com.key -in hhuge9.com.crt -certpbe PBE-SHA1-3DES -keypbe PBE-SHA1-3DES -nomac\nThen, copy the resulting PFX file to the Windows server, double-click it, and start the import process. Once it\u0026rsquo;s imported, the certificate should be shown in the ADFS Wizard.\nIn Active Directory, create a new user (I named mine tsek) and include their email address (which is a required field for the RoleSessionName).\nCreate a group called AWS-437735673474-ADFS-Admin (replace 437735673474 with your actual AWS account ID and ADFS-Admin with the name of the IAM role you want to assume in AWS).\nAdd the tsek user to the AWS-437735673474-ADFS-Admin group.\nFollow the tutorial at https://aws.amazon.com/blogs/security/aws-federated-authentication-with-active-directory-federation-services-ad-fs/ to set up a relay party trust on ADFS and add claim rules.\nIn AWS IAM, add an identity provider and set the IAM role to \u0026ldquo;ADFS-Admin\u0026rdquo;.\nUse the ADFS login page ( https://hhuge9.com/adfs/ls/idpinitiatedsignon) to log in to AWS using your AD credentials.\n","date":"2023-04-19","permalink":"/posts/aws-adfs/","section":"Posts","summary":"Create a new Windows Server 2012 instance and install the following Roles and Features:\nDNS ADFS AD Promote the server to a Domain Controller and create a new forest. I named mine hhuge9.","title":"Setting up ADFS Login in AWS with Windows Server 2012"},{"content":"gRPC\u003e gRPC # Concept: Similar to REST API and JSON. Payload format: Binary Performance: Faster as smaller size than JSON. Usage: Primarily for service-to-service communication. Browser-to-service not widely supported. Protocol: Built on HTTP/2 client -\u0026gt; ALB: HTTPS only; ALB -\u0026gt; target: HTTP/HTTPS ","date":"2023-04-19","permalink":"/posts/aws-alb-grpc/","section":"Posts","summary":"gRPC\u003e gRPC # Concept: Similar to REST API and JSON. Payload format: Binary Performance: Faster as smaller size than JSON. Usage: Primarily for service-to-service communication. Browser-to-service not widely supported. Protocol: Built on HTTP/2 client -\u0026gt; ALB: HTTPS only; ALB -\u0026gt; target: HTTP/HTTPS ","title":"Setup gPRC with AWS ALB"},{"content":"","date":"2023-04-19","permalink":"/tags/tls/","section":"Tags","summary":"","title":"tls"},{"content":"TLS Passthrough with AWS NLB\u003e TLS Passthrough with AWS NLB # To setup TLS passthrough with NLB, follow these steps:\nListen: TCP - can be 80 or 443 Target: TCP - 443 Backend: HTTPS on 443 Here are some interesting features of NLB:\nProxy Protocol v2 TCP Adds a binary header before the HTTP payload Provides the information about the client like X-Forward-For in ALB To enable Proxy Protocol v2, the target server has to understand the protocol Health check has to be TCP as well Nginx can listen to Proxy Protocol v2 - config available The backend is a simple HTTP server -\u0026gt; it can see the X-Forward-To headers that were passed in the request No security group policy on NLB Health check HTTPS: 443 with invalid cert -\u0026gt; OK HTTP: if HTTP redirect to HTTPS was set up, the health check status will be 301 Can check the health check request in the server access log If 400 - wrong protocol. H2 or sending HTTPS to HTTP port If no request comes in - security group blocked - in ALB or EC2 Can shorten the health threshold and interval to speed up the health check initially Preserve client IP addresses Can preserve client IP. In PHP, the $_SERVER['REMOTE_ADDR'] will change to the client\u0026rsquo;s IP Zone level load balancing By default, NLB load balances within the zone Imagine that the NLB is a group of EC2 in different zones. The CNAME has to return multiple IPs to each EC2 The route algorithm is affected on the load balancer EC2 zone itself. Not cross-zone, but ALB is cross-zone by default ","date":"2023-04-19","permalink":"/posts/aws-nlb/","section":"Posts","summary":"TLS Passthrough with AWS NLB\u003e TLS Passthrough with AWS NLB # To setup TLS passthrough with NLB, follow these steps:\nListen: TCP - can be 80 or 443 Target: TCP - 443 Backend: HTTPS on 443 Here are some interesting features of NLB:","title":"Understanding TLS Passthrough and Other Features of AWS NLB"},{"content":"I was surprised to find that AWS Lambda cannot directly reference records from Secrets Manager in environment, especially considering that ECS can reference records from both Parameter Store and Secrets Manager. There are two ways to overcome this limitation in Lambda:\nUse the AWS API. Use a Lambda Extension to retrieve secrets from Secrets Manager. ","date":"2023-04-19","permalink":"/posts/unexpected-limitations-aws-lambdas-inability-to-directly-reference-secrets-manager-and-parameter-store/","section":"Posts","summary":"I was surprised to find that AWS Lambda cannot directly reference records from Secrets Manager in environment, especially considering that ECS can reference records from both Parameter Store and Secrets Manager.","title":"Unexpected Limitations: AWS Lambda's Inability to Directly Reference Secrets Manager and Parameter Store"},{"content":"","date":"2023-04-19","permalink":"/tags/websocket/","section":"Tags","summary":"","title":"websocket"},{"content":"","date":"2023-04-19","permalink":"/tags/windows/","section":"Tags","summary":"","title":"windows"},{"content":"","date":"2023-04-19","permalink":"/tags/wireshark/","section":"Tags","summary":"","title":"wireshark"},{"content":" Changing Column Display\nDecrypting HTTPS Traffic:\nClose all Chrome instances Start Chrome with custom param to log certificates to ssh-key.log Reference ssh-key.log in Preferences \u0026gt; Protocols \u0026gt; TLS \u0026gt; (Pre)-Master-Secret log Common Display Filter Examples\n","date":"2023-04-19","permalink":"/posts/wireshark-customization-https-traffic-decryption-and-common-display-filters/","section":"Posts","summary":"Changing Column Display\nDecrypting HTTPS Traffic:\nClose all Chrome instances Start Chrome with custom param to log certificates to ssh-key.log Reference ssh-key.log in Preferences \u0026gt; Protocols \u0026gt; TLS \u0026gt; (Pre)-Master-Secret log Common Display Filter Examples","title":"Wireshark: Customization, HTTPS Traffic Decryption and Common Display Filters"},{"content":"","date":"2023-04-13","permalink":"/tags/api-request/","section":"Tags","summary":"","title":"api-request"},{"content":"","date":"2023-04-13","permalink":"/tags/authorization/","section":"Tags","summary":"","title":"authorization"},{"content":" To create a signed AWS API request, follow the detailed guide provided in Create a signed AWS API request.\nTo generate a signed request in Python, you can use the following script created by ChatGPT:\n","date":"2023-04-13","permalink":"/posts/aws-sign-v4/","section":"Posts","summary":"To create a signed AWS API request, follow the detailed guide provided in Create a signed AWS API request.\nTo generate a signed request in Python, you can use the following script created by ChatGPT:","title":"Creating a Signed AWS API Request: A Handy Guide"},{"content":"","date":"2023-04-13","permalink":"/tags/python/","section":"Tags","summary":"","title":"python"},{"content":"","date":"2023-04-06","permalink":"/tags/oauth2/","section":"Tags","summary":"","title":"oauth2"},{"content":"","date":"2023-04-06","permalink":"/tags/oidc/","section":"Tags","summary":"","title":"oidc"},{"content":" Create a Certificate Authority (CA)\nIssue certificates for authorized users\nDefine CA public key and principals name in ~/.ssh/authorized_keys instead of users\u0026rsquo; public keys\nUse key (~/.ssh/id_rsa) with CA-issued certificate (~/.ssh/id_rsa-cert.pub) to SSH into server\nPer-account SSH certificate setup under ~/; can also be configured at system level (/etc/ssh/sshd_config)\nAdvantages:\nKey rotation simplified; admin issues new certificate for new key, no need to update public keys on server You can follow the full procedure for using a CA with SSH at https://www.lorier.net/docs/ssh-ca.html or Creating SSH CA Certificate Signing Keys.\n","date":"2023-04-06","permalink":"/posts/ssh-certificate/","section":"Posts","summary":"Create a Certificate Authority (CA)\nIssue certificates for authorized users\nDefine CA public key and principals name in ~/.ssh/authorized_keys instead of users\u0026rsquo; public keys\nUse key (~/.ssh/id_rsa) with CA-issued certificate (~/.","title":"Secure and Simplified SSH Key Management with Certificate Authority (CA)"},{"content":"Difference between OAuth2.0, OIDC and SAML2\u003e Difference between OAuth2.0, OIDC and SAML2 # OAuth 2.0 provides authorization using ID token. OIDC provides authentication using access token. SAML2 provides both authentication and authorization. OAuth 2.0\u003e OAuth 2.0 # allows users to log in, agree to the OAuth permission grant, and generate an access token (like an API key). Access tokens have permission to access the API (resource server), but they are not related to the user\u0026rsquo;s identity. Access tokens can be renewed using a refresh token. ID tokens can show who the user is. For example, when logging into a website with Google, Google will generate an access token for the user, which the website can use to call APIs to get the user\u0026rsquo;s email and name. The browser cookie will link the user with the access and refresh token. Amazon Cognito\u003e Amazon Cognito # User pools can integrate with ALB and API Gateway. Identity pools can manage access control on AWS resources such as S3 and DynamoDB. User mapping can associate IAM roles with specific users or groups. By default, user mapping maps into authenticated and unauthenticated IAM roles. Rules can be configured to assign specific IAM roles based on conditions (e.g., specific attributes or group membership). With an ID token obtained through Amazon Cognito, temporary AWS access keys and secrets can be obtained using aws get-credentials-for-identity. These AWS credentials can be used to access AWS resources. Authentication Process\u003e Authentication Process # Clicking the login button and redirecting to the authorize URL. Entering login credentials on the Cognito login page. Redirecting to a specified redirect URL upon successful authentication. Making additional requests to obtain access tokens and refresh tokens depending on the grant type. Using the authorized code obtained from the initial request to obtain access tokens and refresh tokens if the grant type is authorization_code. Including the response_type=token parameter in the authorization request if the grant type is implicit, which returns the access token and refresh token directly in the response without additional requests to the token URL. Authenticated Role Selection\u003e Authenticated Role Selection # Amazon Cognito user pools come with two default roles: Unauthenticated and Authenticated. Role selection allows for different roles to be assigned to authenticated users. \u0026ldquo;Choose role with rules\u0026rdquo; allows for rules to be set based on the JWT. The following attributes can be referenced in the JWT: phone, email, username, default attributes, and custom attributes. To reference the default and custom attributes in the IAM policy, the authorize scope needs to include \u0026ldquo;profile\u0026rdquo;. Users are allowed to be assigned to groups, and each group can be assigned multiple IAM roles. cognito:roles and cognito:preferred_role will be added to the JWT token when \u0026ldquo;Choose role from token\u0026rdquo; is used to grant permission to user. cognito:groups can also be found in the JWT if the user is in a group. Lab1: Cognito Identity Pool\u003e Lab1: Cognito Identity Pool # What I Need\u003e What I Need # Cognito User and Identity Pool IAM roles for authenticated and unauthenticated user Both roles grant PutObject right to the S3 bucket S3 bucket Upload Object to S3 Bucket with Cognito\u003e Upload Object to S3 Bucket with Cognito # With AWS CLI\u003e With AWS CLI # https://gist.github.com/hugotkk/bf3daf2148d9bc82303f62cb360e6401\nGet ID token from Cognito Get AWS credentials from Cognito API Use the credentials to upload, download, or presign an S3 object With Cognito and AWS JavaScript SDK on Browser\u003e With Cognito and AWS JavaScript SDK on Browser # Similar to AWS CLI, but on a web page Need additional CORS policy to allow object update from the webpage The Python script will start a simple HTTP server and serve the file https://gist.github.com/hugotkk/21385eb08a366b587f1f434eff43f381\nLab2: Authenticated role selection\u003e Lab2: Authenticated role selection # Create a new user in the Cognito user pool In the IAM roles for authenticated and unauthenticated users, remove S3 permissions Create a new IAM role with S3 permissions For \u0026ldquo;Choose role with rules\u0026rdquo; testing: Assign the attribute profile: admin to the first user Create a new rule that checks if the claim has profile and is equal to admin, and assigns the new IAM role For \u0026ldquo;Choose role from token\u0026rdquo; testing: Create a new group and add the first user to the group Attach the new IAM role to the group Test by uploading an object using login.html (code provided in previous example) Expected result: The first user should be able to update objects in S3, while the second user should not be able to. ","date":"2023-04-06","permalink":"/posts/aws-cognito/","section":"Posts","summary":"Difference between OAuth2.0, OIDC and SAML2\u003e Difference between OAuth2.0, OIDC and SAML2 # OAuth 2.0 provides authorization using ID token. OIDC provides authentication using access token. SAML2 provides both authentication and authorization.","title":"Understanding OAuth 2.0: Explore with Amazon Cognito"},{"content":"DC Cannot Replicate\u003e DC Cannot Replicate # Problem: Target principal name incorrect during AD replication Solution ( Demote and Promote AD): Demote the DC at \u0026ldquo;Remove Roles and Features\u0026rdquo; Manually clean update the metadata: Remove the DC record at AD Users and Computers Remove the DC-related record at AD Sites and Services \u0026amp; in DNS Promote the DC back Result: Replication resumes, syncing with other DCs For \u0026ldquo;PRC server not operating\u0026rdquo; error, check DNS issues with dcdiag and try dns scavenging PDC Gone, Operation Master Role not Transferred\u003e PDC Gone, Operation Master Role not Transferred # Solution: Forceful Takeover Use Move-ADDirectoryServerOperationMasterRole to move the Operation Master. Removing an Orphaned Domain\u003e Removing an Orphaned Domain # Problem: Trusted domain removed without demoting DC, resulting in ghost domain/DC Solution: Use ntdsutil metadata cleanup for both domain controller and domain Delete domain controller with How to remove a domain controller that no longer exists? guide Delete domain with remove orphan domain guide ","date":"2023-04-05","permalink":"/posts/windows-ad-issues/","section":"Posts","summary":"DC Cannot Replicate\u003e DC Cannot Replicate # Problem: Target principal name incorrect during AD replication Solution ( Demote and Promote AD): Demote the DC at \u0026ldquo;Remove Roles and Features\u0026rdquo; Manually clean update the metadata: Remove the DC record at AD Users and Computers Remove the DC-related record at AD Sites and Services \u0026amp; in DNS Promote the DC back Result: Replication resumes, syncing with other DCs For \u0026ldquo;PRC server not operating\u0026rdquo; error, check DNS issues with dcdiag and try dns scavenging PDC Gone, Operation Master Role not Transferred\u003e PDC Gone, Operation Master Role not Transferred # Solution: Forceful Takeover Use Move-ADDirectoryServerOperationMasterRole to move the Operation Master.","title":"Active Directory Issues and Solutions"},{"content":"","date":"2023-03-31","permalink":"/tags/ai/","section":"Tags","summary":"","title":"ai"},{"content":"","date":"2023-03-31","permalink":"/tags/chatgpt/","section":"Tags","summary":"","title":"chatgpt"},{"content":"My Use Cases\u003e My Use Cases # Summarizing Video or Article Content Use https://downsub.com for video subtitles and https://chatgptsplitter.com/ for text Ask GPT-4 to summarize content in bullet points and elaborate on each point in detail For GPT-3.5, the chunk size is 1500. For GPT-4.0, it is 15000 Preparing Open-Ended Questions and Lab Exercises for Revision Provide notes to GPT-4 Request generation of open-ended questions and lab exercises for revision and deeper understanding Memorization Techniques Using Visual Mnemonics Ask GPT-4 for visual memory techniques suggestions to remember specific phrases or names Rewriting Emails or Paragraphs Use GPT-4 to rewrite and improve structure, clarity, and tone of emails or paragraphs SEO Suggestions for Meta Description, Title, and Tags Request GPT-4\u0026rsquo;s assistance in generating optimized meta descriptions, titles, and tags for improved search engine visibility Learning Code Provide code to GPT-4 Request an explanation Requesting Code Templates Tips for Using GPT-4 Effectively\u003e Tips for Using GPT-4 Effectively # If the answer is not relevant:\nRegenerate the result Edit the prompt for specificity or clarity Start a new chat to reset context Best practices\u003e Best practices # Best practices for prompt engineering with OpenAI API Useful links\u003e Useful links # Sam Altman: OpenAI CEO on GPT-4, ChatGPT, and the Future of AI - Known Issues\u003e Known Issues # Unreliable answers when out of token window or lacking data Verification of results needed Inability to reply in markdown format We can ask GPT to output in code block format for copying. ","date":"2023-03-31","permalink":"/posts/chatgpt/","section":"Posts","summary":"My Use Cases\u003e My Use Cases # Summarizing Video or Article Content Use https://downsub.com for video subtitles and https://chatgptsplitter.com/ for text Ask GPT-4 to summarize content in bullet points and elaborate on each point in detail For GPT-3.","title":"Maximizing Productivity and Learning with GPT-4: Use Cases and Best Practices"},{"content":"","date":"2023-03-31","permalink":"/tags/openai/","section":"Tags","summary":"","title":"openai"},{"content":"","date":"2023-03-31","permalink":"/tags/prompt/","section":"Tags","summary":"","title":"prompt"},{"content":"Concepts\u003e Concepts # Block Public Access\u003e Block Public Access # Prevent any public access in ACL or bucket policy. ACL: If you block public access in ACL, any public access granted to everyone will be ignored. Bucket Policy: If you block public access in bucket policy, any policy that allows S3 operations by * will be ignored. eg: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;PublicReadGetObject\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:GetObject\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::Bucket-Name/*\u0026#34; ] } ] } Object Ownership\u003e Object Ownership # Ensure that all objects are owned by the bucket owner and disable ACLs. Choose between disabling ACL enabling ACL while keeping ownership the same as the bucket owner by default but it can be overrided by the object writer enabling ACL while keeping ownership the same as the object writer Static Website Hosting\u003e Static Website Hosting # Set a default index page and error page. Provide a URL for access. (not same as the s3 URL) Access Control\u003e Access Control # Use ACL to control access rights on an object level. Note that enabling ACL may cause varied object ownership in shared buckets. Use bucket policy to control access at the bucket level. Keep in mind that bucket policies only apply to objects owned by the bucket owner. Access Points\u003e Access Points # Simplify policy management by attaching a policy to an access point. Use Access point policy to override existing bucket policies for granular control. Use S3 Access Points to reduce the risk of policy errors or inconsistencies. Steps ( How to setup a public accessible S3 bucket)\u003e Steps ( How to setup a public accessible S3 bucket) # Configure Static Website Hosting in Properties Disable Object Ownership in Permissions Enable public access With ACL Disable block public access of ACL in the Permissions. Go to Objects, select objects, then click \u0026ldquo;make public using ACL\u0026rdquo; (This can be done by AWS CLI as well). Or via Bucket Policy Disable block public access of bucket policy in the Permissions. Create a bucket policy that allows everyone to get objects. Integrate with CloudFront\u003e Integrate with CloudFront # Instead of using static website hosting or making S3 public, we can use CloudFront to host a static web host with S3.\nHere\u0026rsquo;s how:\nCreate a distribution and S3 origin. To define the index page, set the root object in the distribution level for requesting /. We can use a private S3 bucket and use Origin Access Control to grant permission for CloudFront to access the bucket. OAI (Origin Access Identity) is now considered legacy, and it\u0026rsquo;s recommended to use Origin Access Control instead. We can control policies for caching and manage headers, cookies, and query strings: Viewer Viewer + CloudFront Viewer + Exclude Specific (filter from Viewer + CloudFront) Add or remove headers, CORS, and security headers. Add a whitelist or blacklist Geo Restriction to restrict access based on geography. ","date":"2023-03-29","permalink":"/posts/aws-s3-hosting/","section":"Posts","summary":"Concepts\u003e Concepts # Block Public Access\u003e Block Public Access # Prevent any public access in ACL or bucket policy. ACL: If you block public access in ACL, any public access granted to everyone will be ignored.","title":"How to host a static website on S3"},{"content":"Identity Management\u003e Identity Management # CyberArk AWS SSM SSH Certificate SSO\u003e SSO # Kerberos ADFS 2FA\u003e 2FA # Google Authentificator Vault\u003e Vault # Hashicorp Vault AWS Secret Manager Vulnerability Scanner\u003e Vulnerability Scanner # OpenVAS Web Security Scanner\u003e Web Security Scanner # Acunetix OWASP ZAP ","date":"2023-03-29","permalink":"/posts/security-tools/","section":"Posts","summary":"Identity Management\u003e Identity Management # CyberArk AWS SSM SSH Certificate SSO\u003e SSO # Kerberos ADFS 2FA\u003e 2FA # Google Authentificator Vault\u003e Vault # Hashicorp Vault AWS Secret Manager Vulnerability Scanner\u003e Vulnerability Scanner # OpenVAS Web Security Scanner\u003e Web Security Scanner # Acunetix OWASP ZAP ","title":"Identity Management, Vault, and Security Scanner Tools for Cybersecurity"},{"content":"","date":"2023-03-29","permalink":"/tags/s3/","section":"Tags","summary":"","title":"s3"},{"content":"","date":"2022-12-15","permalink":"/tags/calculator/","section":"Tags","summary":"","title":"calculator"},{"content":"","date":"2022-12-15","permalink":"/tags/converter/","section":"Tags","summary":"","title":"converter"},{"content":"","date":"2022-12-15","permalink":"/tags/grafana/","section":"Tags","summary":"","title":"grafana"},{"content":"","date":"2022-12-15","permalink":"/tags/ip/","section":"Tags","summary":"","title":"ip"},{"content":"","date":"2022-12-15","permalink":"/tags/mx/","section":"Tags","summary":"","title":"mx"},{"content":"Installation\u003e Installation # Prometheus \u0026amp; the related components (eg: pushgateway, blackbox_exporter, alertmanager) - scrape engine - it\u0026rsquo;s better to look at the README.md and Dockerfile on their github repo to get the information about the docker setup\nGrafana - for visualization; drawing graphs\nNode Exporter (via package manager) - Centos7 (need epel repo); Ubuntu20.04 - monitor VM\u0026rsquo;s states\ncAdvisor - monitor docker host\u0026rsquo;s states\nOthers exporters\nkube-prometheus-stack (eks) - included grafana and prometheus\nReferences\u003e References # docker-compose.yml for prometheus and node exporter grafana.ini prometheus.yml alertmanager.yml web-config.yml - tls and basic auth settings - pass --web.config.file when starting the binary. This works on any prometheus related products eg: prometheus, node_exporter, alertmanager prometheus query functions Grafana Dashboards\u003e Grafana Dashboards # cadvisor node-exporter Prometheus\u003e Prometheus # Customise the storage config Setup an alert Setup federation - a primary prometheus will work as an aggregator to collect metrics from other prometheus instances. This can reduce the loading on a single prometheus server. Setup recording rules Setup node-exporter with push gatewaty - but this isn\u0026rsquo;t recommended. The reasons are in here Snapshot (backup) EC2 host discovery - use for dynamically adding the ec2 instances as the scrape targets Thanos\u003e Thanos # To archive the HA, third-party solutions like thanos will be needed.\nThese two articles are very good introduction of the concept of thanos\nDeep Dive into Thanos-Part I Deep Dive into Thanos-Part II Also, there is a hand-on lab provided by killercoda which is recommended by thanos\nThis shows the architecture of thanos.\nThe thanos gateway works as a proxy on each prometheus instance (sidecar) to communicate to other thanos components. Also, data can be optionally saved to object stores (like aws s3 / mino) for long-term storage as well.\nQueries will add an additional layer to the system. Instead of obtaining the data directly from prometheus, the client (grafana / end user) will now talk to the thanos querier instead. The queries will\ndeduplicate the data they get from the replicas combine the data from the long term object store and shards Thus, with the queries, prometheus instances can logically group into different shards / replicas.\nGrafana\u003e Grafana # Provision dashboards and Data sources But we can\u0026rsquo;t provision the users, orgs, alerts Add prometheus to grafana Import or export a dashboard Backup grafana Create repeated row / panel Instead of creating a field overriding, we can choose the series color quickly by just clicking the color bar next to the series HA in Grafana - setup database as a backend to share the persistent data ","date":"2022-12-15","permalink":"/posts/notes-about-prometheus-grafana/","section":"Posts","summary":"Installation\u003e Installation # Prometheus \u0026amp; the related components (eg: pushgateway, blackbox_exporter, alertmanager) - scrape engine - it\u0026rsquo;s better to look at the README.md and Dockerfile on their github repo to get the information about the docker setup","title":"Notes about Prometheus \u0026 Grafana"},{"content":"","date":"2022-12-15","permalink":"/tags/nslookup/","section":"Tags","summary":"","title":"nslookup"},{"content":"","date":"2022-12-15","permalink":"/tags/web/","section":"Tags","summary":"","title":"web"},{"content":"Checker\u003e Checker # nslookup MX Toolbox - mx, spx, dkim, dmac record, blacklist What Is My Ip Address - ip lookup who.is - domain lookup SSL Labs - check ssl setting Calculator\u003e Calculator # Kubernetes Instance Calculator EC2 Price Converter\u003e Converter # Convert 500 Megabytes to Bytes Epoch \u0026amp; Unix Timestamp Conversion Tools IOPS, MB/s, GB/day Converter JSON to Go - Convert JSON to golang struct HKT to GMT watt to kwh jwk generator jwk encoder / decoder ","date":"2022-12-15","permalink":"/posts/web-tools/","section":"Posts","summary":"Checker\u003e Checker # nslookup MX Toolbox - mx, spx, dkim, dmac record, blacklist What Is My Ip Address - ip lookup who.is - domain lookup SSL Labs - check ssl setting Calculator\u003e Calculator # Kubernetes Instance Calculator EC2 Price Converter\u003e Converter # Convert 500 Megabytes to Bytes Epoch \u0026amp; Unix Timestamp Conversion Tools IOPS, MB/s, GB/day Converter JSON to Go - Convert JSON to golang struct HKT to GMT watt to kwh jwk generator jwk encoder / decoder ","title":"Web Tools"},{"content":"","date":"2022-12-14","permalink":"/tags/blockchain/","section":"Tags","summary":"","title":"blockchain"},{"content":"","date":"2022-12-14","permalink":"/tags/nft/","section":"Tags","summary":"","title":"nft"},{"content":"Useful links\u003e Useful links # Explorer\u003e Explorer # solana explorer NFT marketplace\u003e NFT marketplace # solanart solasea Good articles to understand the concept of solana\u003e Good articles to understand the concept of solana # Solana NFT 101 Solana’s Token Program, Explained Understanding Solana’s Mint Accounts and Token Accounts How NFTs are represented in Solana In the Solana world, data will be stored in different accounts. Accounts will be associated with each other by cross-referencing. The Solana program will be used to manipulate those accounts (create, delete, set link). Business logic will be introduced from a separated program.\nCommon tasks eg: creating token / nft can be done with some universal solana programs ( spl Token program, metaplex) on solana network. Developers don\u0026rsquo;t need to write their own solana program. This makes things become more standard. People write their own business logic by extending the functions of those universal programs but won\u0026rsquo;t start from zero.\nNotes\u003e Notes # solfaucet can claim free SOL (devnet) for development Most of the people use metaplex to create nft Metaplex\u0026rsquo;s Token Metadata Program defined the standard of a solana NFT Metaplex\u0026rsquo;s candy machine provided abilities to sell the nft eg: loot box, start/end date, whitelist sugar is a cli tool to create candy machines. It simplified the task but I got a \u0026ldquo;Request header too large\u0026rdquo; error when uploading a large file (100MB) Metaplex javascript sdk can solve the issue of sugar but their documentation at this moment is a bit old (even on github README). The examples are still in V1 but sugar cli has already moved V2. The syntax of V2 is quite different from V1. Therefore, I have to refer to this sdk doc for development. If \u0026ldquo;animation_url\u0026rdquo; is set to a mp4 / gif, it will playable inside the solana wallet (like phantom). I thought it was related to the \u0026ldquo;properties.files\u0026rdquo; field. \u0026ldquo;properties.files\u0026rdquo; should include all related resources. For examples, if the NFT have a mp4 \u0026ldquo;animation_url\u0026rdquo; and jpg \u0026ldquo;image\u0026rdquo;, \u0026ldquo;properties.files\u0026rdquo; should include both When uploading files to arweave with metaplex sdk, it will not append ?ext=\u0026lt;extension\u0026gt; to the URL. This caused the image and videos to fail to be shown correctly on the phantom wallet. The file extension should always be included like https://www.arweave.net/efgh1234?ext=mp4. this calculator is useful to calculator how much do we need to pay for the persistent storage on arweave spl token program cli is used to create token / nft but it provides the basic functions only. For instance, there is no metadata for the nft token created from the token program. Metaplex is kind of extension which added the metadata and venting machine functions on top of the token program solana cli - will be used to create accounts / transfer / check balances. More use cases are in here. When create a solana account, it will print out 12-words seed phases and save the private key in ~/.config/solana/id.json. Please save the seed phases. The private key can be derived from seed phases but we can\u0026rsquo;t recover the seed phases from private keys. Solana-cli-created accounts can only be imported to the phantom wallet with a private key. The default derivation path of Phantom is different from solana-cli. Changing the derivation path is only supported when importing a hardware wallet to phantom. Therefore, there is no way to import the account from seed phases. To do that, we have to convert uint8array (id.json) to base58. This can be done by library like base58-js web3.js - web3 library of solana but people usually use the anchor\u0026rsquo;s version because most of the solana programs are developed from anchor this shows the public clusters in the solana network. I use quicknode to create a dedicated solana endpoint instead. More examples can be found on Solana Cookbook, Solana Web3 Demo, Solana Development Guide ","date":"2022-12-14","permalink":"/posts/notes-about-solana/","section":"Posts","summary":"Useful links\u003e Useful links # Explorer\u003e Explorer # solana explorer NFT marketplace\u003e NFT marketplace # solanart solasea Good articles to understand the concept of solana\u003e Good articles to understand the concept of solana # Solana NFT 101 Solana’s Token Program, Explained Understanding Solana’s Mint Accounts and Token Accounts How NFTs are represented in Solana In the Solana world, data will be stored in different accounts.","title":"Notes about Solana"},{"content":"","date":"2022-12-14","permalink":"/tags/solana/","section":"Tags","summary":"","title":"solana"},{"content":"Dockerfile\u003e Dockerfile # Most containers are not well-documented, making it hard to find essential information about them on Docker Hub or GitHub\u0026rsquo;s README. Some of the critical information to know includes:\nWhich user does the container run as? Which ports are used by the application? Where are the config files/directory? Where will the persistent data be stored? The best way to find these answers is by examining the Dockerfile. For example:\nPrometheus\nUser: nobody Port: 9090 Config path: /etc/prometheus/prometheus.yml Data path: /prometheus php:8-apache-buster\nPort: 80 PHP config: /usr/local/etc/php/php.ini Apache config: /etc/apache2/conf-available/docker-php.conf Document root: /var/www/html/ Disk Usage\u003e Disk Usage # Analyse the disk usage of docker\nRun the command: docker system df -v Reference: https://docs.docker.com/engine/reference/commandline/system_df/ Live Restore\u003e Live Restore # This will keep containers running during the docker upgrade and systemctl stop dockerd\nEdit the file /etc/docker/daemon.json Add the following content to the file: { \u0026#34;live-restore\u0026#34;: true, } Address pool\u003e Address pool # To override the default address pool, add the following content to the daemon.json file: { \u0026#34;default-address-pools\u0026#34;: [ { \u0026#34;base\u0026#34;: \u0026#34;10.1.0.0/16\u0026#34;, \u0026#34;size\u0026#34;: 24 } ] } Reference: https://forums.docker.com/t/docker-default-address-pool-customization-question/112969 Logging\u003e Logging # Docker logging does not rotate by default. To change the logging driver to JSON, you need to modify the Docker daemon configuration file (/etc/docker/daemon.json) and add the following line: { \u0026#34;log-driver\u0026#34;: \u0026#34;json-file\u0026#34; } To set the retention policy for the JSON logging driver, you can add the following configuration options to the same file: { \u0026#34;log-opts\u0026#34;: { \u0026#34;max-size\u0026#34;: \u0026#34;10m\u0026#34;, \u0026#34;max-file\u0026#34;: \u0026#34;3\u0026#34; } } This will retain a maximum of 3 log files, each with a maximum size of 10 MB. After making changes to the configuration file, you need to restart the Docker daemon for the changes to take effect: sudo systemctl restart docker Docker Swarm\u003e Docker Swarm # Service Management\u003e Service Management # To reference a service, use tasks.\u0026lt;service-name\u0026gt; instead of a specific replica. Publishing a port will expose it on all Swarm nodes, requiring a load balancer to balance the service. A load balancer distributes incoming requests to nodes running the service, which will handle the request using one of its replicas. Horizontal scaling of the service is achieved by adding additional replicas to handle increased traffic. Volume Management\u003e Volume Management # Volumes/local mount points are independent on each node. When creating a volume/local mount point for a service, it is created on the node where the task is assigned. Changes made to a volume/local mount point on one node will not be reflected on other nodes. To ensure data consistency across nodes, use a distributed file system like GlusterFS or NFS, or a cloud-based storage service like Amazon EFS or Google Cloud Storage. When using a distributed file system or cloud-based storage service, the volume/local mount point is shared across all nodes in the Swarm, ensuring that all nodes have access to the same data. iptables\u003e iptables # Docker uses iptables for traffic routing\nBe cautious when restarting iptables service to avoid overriding Docker\u0026rsquo;s rules\nServices like firewalld or ufw may interfere with Docker\u0026rsquo;s iptables rules\nTurning off iptables may have side effects\nRouting in iptables forwards published ports/container subnets to NAT table\nNAT table routes traffic between containers\nDocker host defaults to IPv4 forwarding enabled, making containers public on subnets\nDocker host can be used as router to access container directly\nExample: Limiting access to specific IPs for Node.js container with port 80 and published port 8080 on Docker host\nDropping port 8080 in firewalld won\u0026rsquo;t work; traffic is forwarded to NAT table Dropping container port 80 in Docker\u0026rsquo;s custom chain (DOCKER_USER) will work but it will also block all traffic into the container, not just from outside Docker host Useful links\u003e Useful links # install docker Dockerfile reference docker-comopse.yml reference /etc/docker/daemon.json reference how to trust the tls certificate of a registry with TLS enabled ","date":"2022-12-13","permalink":"/posts/docker/","section":"Posts","summary":"Dockerfile\u003e Dockerfile # Most containers are not well-documented, making it hard to find essential information about them on Docker Hub or GitHub\u0026rsquo;s README. Some of the critical information to know includes:","title":"Docker"},{"content":"","date":"2022-09-06","permalink":"/tags/istio/","section":"Tags","summary":"","title":"istio"},{"content":"Main concept\u003e Main concept # Injecting a proxy in front of each pods - so we can do traffic management\nTraffic Shifting Fault Injection Circuit Breaking Mirror and apply security features\nmtls between pods security policy or some logging / auditing works\nImagine there is a proxy server in front of each pod.\nVirtualService and DestinationRule are the configure of the proxy server\nActually the offical documentation is good enough. here is a good course to learn about istio.\nIstio Settings\u003e Istio Settings # min tls protocol memory and cpu limit on istiod - I use this to reduce the memory limit on istiod. The default profile is 2G memory by default. block un-registry egress by default - See Service Entry dns proxing - See Service Entry enable egress gateway Install Multicluster\u003e Install Multicluster # Setup Guide\nThe Before you Begin section is important. For Multi-Primary Multicluster, we need to generate a root certificate to issue the certificates for cluster1 and cluster2. See Configure Trust\nMulti-primary: istiod on each cluster Primary-remote: only primary will have istiod - remote will share the istiod with primary On different networks: the pods are in different networks at 2 k8s clusters. EKS support vpc networking, the pods can be in the vpc subnet so the pods in different cluster can be configured on the same network. I have set up 2 k3s clusters on aws ec2. The pods are on different networks. Install VM Workload\u003e Install VM Workload # istio can bring VM into the service mesh. treat vm as a k8s pod\nWorkload Entry: Pod Workload Group: Deployment Service Entry: Service I follow this guide to set this up\nThere are few options for the installation. I chose\nMulti Network - as the vm and k8s pods are in different networks (For example: vm 192.168.0.1/24 k8s pods 10.0.1.0/24. In AWS, we pods can use the aws vpc network which mean if we have a subnet 192.168.0.1/24 both vm and eks pods can be in same subnet) Automated WorkloadEntry Creation - workload entry will automatically create after joining the mesh dns\u003e dns # Before the installation, we need to setup a dns server first. VM need to know about the k8s cluster (*.cluster.local eg: httpbin.default.svc.cluster.local).\nHere are some notes on setting up dnsmasq on ec2.\n/etc/dnsmasq.conf\nbind-interfaces listen-address=127.0.0.1 server=169.254.169.254 address=/cluster.local/\u0026lt;k8s_cluster_ip\u0026gt;/ bind-interfaces + listen-address make the dns server work. bind-interfaces will bind port 53 on all interfaces but listen-address will restrict it on specific interface need to forward queries to aws\u0026rsquo;s dns (169.254.169.254) /etc/resolv.conf\nnameserver 127.0.0.1 update resolv.conf to tell the vm to use the dnsmasq\nFor ubuntu, the resolv.conf will be managed by systemd-resolved so it is better to disable it\nsystemctl disable systemd-resolved systemctl stop systemd-resolved system-resolved will create a symbolic link to /etc/resolv.conf. When systemd-resolved is disable, the resolv.conf may disappear after the server restart. So be careful of that.\nInstall with revision\u003e Install with revision # If istio is installed with the canary method, the istiod service name will be different.\nwithout revision: istiod.istio-system.svc.cluster.local with revision: istiod-\u0026lt;revision\u0026gt;.istio-system.svc.cluster.local The templates at Expose services inside the cluster via the east-west gateway are using istiod.istio-system.svc so we have to change it manually\nAlso we have to add --revision when deploy the east west gateway\nService vs ServiceEntry\u003e Service vs ServiceEntry # the vm can be selected with Service - ServiceEntry is not necessary If we want to use ServiceEntry, DNS proxy need to be enabled Upgrade\u003e Upgrade # In-placed\u003e In-placed # run istioctl upgrade with the updated istioctl binary\nCanary\u003e Canary # For example, we want to upgrade istio from 1.13.0 to 1.14.3.\nWe can\ninstall new version (let 2 versions coexist) switch to new version uninstall the old version Firstly, istio has to be installed with --revision\nistioctl operator init --revision 1-13-0 and apply IstioOperator config with specific revision\nspec: ... revision: 1-13-0 To enable auto injection in namespace (example is default), instead of\nkubectl label ns default istio-injection=enabled we have to specify the revision\nkubectl label ns default istio.io/rev=1-13-0 To upgrade istio:\nDownload 1.14.3 istioctl binary init 1.14.3 operator istioctl operator init --revision 1-14-3 apply IstiOperator config with revision: 1-14-3 Update injection label - kubectl label ns default istio.io/rev=1-14-3 --overwrite Uninstall the old version - istioctl x uninstall --revision 1-13-0 Sidecar injection\u003e Sidecar injection # cannot enable globally - not like mTLS - need to be apply per namespace / per pod hostNetwork: true in pod will stop the injection injection can be override in pod level Service Entry\u003e Service Entry # This is confusing me so much.\nWhy the hostname resolution does not work?\u003e Why the hostname resolution does not work? # My expectation: like k8s\u0026rsquo; service but it\u0026rsquo;s for external services - so we are adding an A record to the k8s DNS\nFor example:\napiVersion: networking.istio.io/v1alpha3 kind: ServiceEntry metadata: name: mysql spec: hosts: - db.hhuge9.com location: MESH_EXTERNAL endpoints: - XXXXX Expected that I can connect to my own mysql service with mysql -h db.hhuge9.com at any pod in the mesh after apply this config\nBut the result is - db.hhuge9.com cannot be resolved\nThe reason is - ServiceEntry will not do anything on the dns unless we enable the DNS proxying feature\nWhen we apply this ServiceEntry\napiVersion: networking.istio.io/v1alpha3 kind: ServiceEntry metadata: name: google spec: hosts: - google.hhuge9.com location: MESH_EXTERNAL resolution: DNS endpoints: - 142.250.200.14 with enabling dns proxying:\nISTIO_META_DNS_CAPTURE: true - will add entry to k8s dns. google.hhuge9.com will be resolved with the endpoints ips (142.250.200.14) ISTIO_META_DNS_AUTO_ALLOCATE: true - will allocate an internal ip (like service in k8s). google.hhuge9.com will be resolve with that ips We can leave ISTIO_META_DNS_AUTO_ALLOCATE: false and assign an internal ip at the addresses field\n... hosts: - google.hhuge9.com addresses: - 192.168.0.1 ... endpoints: - 142.250.200.14 Therefore nslookup google.hhuge9.com will return 192.168.0.1\nBut for the host name resolution, we need ISTIO_META_DNS_CAPTURE: true\nIs it just a canonical name / A record?\u003e Is it just a canonical name / A record? # I doubted why ServiceEntry is needed. We can access the service directly without applying a config, right? It looks so useless\nAcutualy, It becomes useful when we want to control the egress traffic When the REGISTRY_ONLY policy is enabled, only registered services can be accessed within the mesh. Therefore, a service entry of facebook.com will be needed if we want to access facebook inside the pod\nAuthorizationPolicy\u003e AuthorizationPolicy # Useful examples can be found on Concept \u0026gt; Security \u0026gt; Authorization policies and Reference \u0026gt; Security \u0026gt; AuthorizationPolicy Best practise is applying allow-nothing first then apply our policies allow-all and deny-all are useful in debug apiVersion: security.istio.io/v1beta1 kind: AuthorizationPolicy metadata: name: httpbin namespace: foo spec: selector: matchLabels: app: httpbin version: v1 action: ALLOW rules: - from: - source: principals: [\u0026#34;cluster.local/ns/default/sa/sleep\u0026#34;] - source: namespaces: [\u0026#34;dev\u0026#34;] to: - operation: methods: [\u0026#34;GET\u0026#34;] when: - key: request.header[foo] values: [\u0026#34;bar\u0026#34;] This policy means - allow access to the pod in namespace foo with label httpbin and v1 with the existence of\nGET request and foo: bar header and the source is from namespace dev or with principals cluster.local/ns/default/sa/sleep principal: cluster.local/ns/default/sa/sleep means sleep.default.svc.cluster.local. It is the service account name when the mtls is enabled\nThe account name can be confirmed by printing out the environment variables with env in the pod\nIngress Gateway\u003e Ingress Gateway # http https - tls termination on proxy https - tls termination on pod (tls passthrough) Egress Gateway\u003e Egress Gateway # http https initial the request with http but connect to the target with https Traffic Management\u003e Traffic Management # Most of the examples can be found on here\nFault Injection - like: return 500 / add delay to the requests Traffic Shifting - eg: 50% to v1; 50% to v2 - good for canary / blue green deployment / AB testing Circuit Breaking - exclude the pods if face consecutive 5XX error / set max connections Mirror - mirror the traffic. good for logging / testing / monitoring ","date":"2022-09-06","permalink":"/posts/exam-istio/","section":"Posts","summary":"Main concept\u003e Main concept # Injecting a proxy in front of each pods - so we can do traffic management\nTraffic Shifting Fault Injection Circuit Breaking Mirror and apply security features","title":"Istio Note"},{"content":"","date":"2022-09-06","permalink":"/tags/service-mesh/","section":"Tags","summary":"","title":"service-mesh"},{"content":"Notes\u003e Notes # installation - curl -sSL https://bit.ly/2ysbOFE | bash -s \u0026ndash; 2.2.7 1.5.3 to start a basic test network (two org, 1 application channel) network.sh up createChannel -c mychannel -s couchdb without fabric-ca network.sh up createChannel -ca -c mychannel -s couchdb with fabric-ca organizations/cryptogen/crypto-config-org3.yaml + cryptogen will be used to bootstrap all required crypto stuff like certificates / tls / user identity for you - this is what I call \u0026ldquo;without fabric-ca\u0026rdquo; in the with fabric-ca way, you have to cretae a fabric-ca-server and register the peer, generate the user identity and certificates manually with fabric-ca-client configtx/configtx.yaml + configtxgen will be used to generate the genesis block, channel transaction and org definition for you peer environment variables setup - useful when performing channel update and chaincode installation / execution peer0.org1.example.com peer0.org2.example.com peer1.org3.example.com tls setup for orderer, peer and fabric-ca configtx.yaml reference Leader, Anchor (Gossip)\u003e Leader, Anchor (Gossip) # leader peer it will always receive the block from orderer it will then send the block to others peers it can be set to keep the peer always up-to-date it can be auto-elected or config statically anchor peer The network uses the GOSSIP mechanism to know peers from other org with anchor peer. For example: org1 peer1 has to communicate with org2 peer1. it can know org2 peer1 from org1 anchor peer. an anchor peer can talk with another anchor peer from different org to enable this in peer, define CORE_PEER_GOSSIP_EXTERNALENDPOINT to expose peer to the GOSSIP channel create channels\u003e create channels # orderer system channel use configtxgen -outputBlock to generate genesis block config ORDERER_GENERAL_GENESISFILE with genesis.block in orderer application channel use configtxgen -outputCreateChannelTx to create transaction create block with peer channel create add peer to the channel with peer channel join -b add org3\u003e add org3 # update organizations/cryptogen/crypto-config-org3.yaml\ngenerate certificates for the org3 peer\nupdate configtx/config.yaml with org3 definition\nprint out the definition\nupdate channel config\nfetch channel config make a change request for adding new org definition to the channel config sign the change request by org commit the change bring up new org\u0026rsquo;s peer\nadd peer\u003e add peer # same as add org but no need to update channel config add orderer\u003e add orderer # same as add org but need to update both orderer and application channel config (update the Addresses and Consensers list) bring up the new orderer with the latest orderer config block (don\u0026rsquo;t use the original genesis.block) service discovery\u003e service discovery # configure external endpoints add anchor peers to channel config setup persistent config query peers query channel config query endorsers when the chaincode wasn\u0026rsquo;t installed on the peer. it will return an error: chaincode definition wasn\u0026rsquo;t found. chaincode\u003e chaincode # use peer lifecycle chaincode package to package the chaincode source code. no need to call npm install / go mod vendor before packaging because the package command will exclude the dependencies folders (vendors / node_modules) for you use peer lifecycle chaincode install to install the chaincode. a chaincode container will be started in each installed peers To commit a chaincode, suggests to follow the Using Private Data in Fabric tutorial\npeer lifecycle chaincode approveformyorg peer lifecycle chaincode commit It is a good example as it show how to add the extra params to the chaincode definition\nFor example,\n--signature-policy: define which peers need to endorse during the invoke --collections-config: config the private data definition --init-required: need to add -I when first time invoke the chaincode To check which org has approved the chaincode, use\npeer lifecycle chaincode checkcommitreadiness Notice that those extra params have to be kept in the whole commit process.\nFor example if you set --init-required in approveformyorg. you have to keep this in approveformyorg for other org, commit and checkcommitreadiness.\nSome information need to know to commit a new version of chaincode:\ncc_package_id - peer lifecycle chaincode queryinstalled channel_name - peer channel list chaincode version and sequence - peer lifecycle chaincode querycommitted or peer lifecycle chaincode queryapproved However, there is no way to know what extra parameters are used in approveformyorg by other orgs.\nIt is hard to approve a chaincode without knowing the extra params being used in other org.\nTo execute an approved chaincode,\nuse peer chaincode invoke --waitForEvent and peerAddresses are needed in peer chaincode invoke --transient can be used to pass an object to the chaincode --tls means use tls to connect to the orderer but not peers if tls is enabled in peers, you must connect peer with tls by export CORE_PEER_TLS_ENABLED=true To define which / how many org are required in peer lifecycle chaincode commit\nchange \u0026ldquo;LifecycleEndorsement\u0026rdquo; policy in channel config To define which / how many org are required to endorse in peer chaincode invoke\nchange \u0026ldquo;Endorsements\u0026rdquo; policy in channel config use --signature-policy to approve and commit the chaincode. this will override the channel config fabric-ca-client\u003e fabric-ca-client # fabric-ca-client will look at the fabric-ca-client-config.yml and msp (user identity) at the $FABRIC_CA_CLIENT_HOME folder.\nby setting up this environment variable, you could run commands with that user identity directly without enrollment\nfabric-ca-client enroll will generate the fabric-ca-client-config.yml and msp (user identity) at the $FABRIC_CA_CLIENT_HOME folder.\nyou have to specify -M when enrolling a user so you won\u0026rsquo;t override the user identity config.\neverytime when you call fabric-ca-client enroll. a new pair of cert and key will be generated in the $FABRIC_CA_CLIENT_HOME/msp folder\nUsing Private Data in Fabric has a good example to show how to create a user in fabric-ca\nlogin as org1.example.com admin ($FABRIC_CA_CLIENT_HOME/msp) export FABRIC_CA_CLIENT_HOME=${PWD}/organizations/peerOrganizations/org1.example.com/ create new user owner fabric-ca-client register --caname ca-org1 --id.name owner --id.secret ownerpw --id.type client --tls.certfiles \u0026#34;${PWD}/organizations/fabric-ca/org1/tls-cert.pem\u0026#34; generate owner\u0026rsquo;s certificate in ${PWD}/organizations/peerOrganizations/org1.example.com/users/owner@org1.example.com/msp fabric-ca-client enroll -u https://owner:ownerpw@localhost:7054 --caname ca-org1 -M \u0026#34;${PWD}/organizations/peerOrganizations/org1.example.com/users/owner@org1.example.com/msp\u0026#34; --tls.certfiles \u0026#34;${PWD}/organizations/fabric-ca/org1/tls-cert.pem\u0026#34; if remove -M, $FABRIC_CA_CLIENT_HOME/msp will be overridden so the user identity will change to owner. If you don\u0026rsquo;t have any backup for the msp folder, you have to enroll in an admin account to re-generate the config again. fabric-ca-client enroll -u https://admin:adminpw@localhost:7054 --caname ca-org1 --tls.certfiles \u0026#34;${PWD}/organizations/fabric-ca/org1/tls-cert.pem\u0026#34; Fabric CA User\u0026rsquo;s Guide recorded many useful examples for daily operations\ncreate a user with type: client - fabric-ca-client register login \u0026amp; generate the certificate - fabric-ca-client enroll list out the certs in ca - fabric-ca-client certificate list set user\u0026rsquo;s affiliation - \u0026ndash;affiliation org2 set user as revoker of peer and client - \u0026ndash;id.attrs \u0026lsquo;\u0026ldquo;hf.Registrar.Roles=peer,client\u0026rdquo;,hf.Revoker=true\u0026rsquo; revoke a user identity - fabric-ca-client revoke -e update the crl - fabric-ca-client gencrl renew the cert - fabric-ca-client reenroll fabric-ca-server\u003e fabric-ca-server # use fabric-ca-server init -b to generate the dummy server config. They will be stored in $FABRIC_CA_HOME\nupdate csr.cn, csr.names, csr.hosts, ca.name, tls.enabled in $FABRIC_CA_HOME/fabric-ca-server-config.yaml\nbefore starting the server, make sure to unset all conflict fabric-ca-server environment variables. They will override the setting in config.yaml. I spent lots of time debugging because of this.\nfabric-ca-server start -b to start the ca server\nenable hsm\nthis page tells how to enable hsm in fabric-ca-server. basically, we need the libsofthsm2.so pin and label to config the bccsp session in fabric-ca-server-config.yaml. then the private keys will be stored in hsm instead of the msp folder however they don\u0026rsquo;t provide any fabric-ca binary with pscs11 enabled to play with fabric-ca-server with hsm. we have to compile softhsm2 and fabric-ca by ourselves enable mysql\nby default, the user identities will be stored in a sqlite file if you plan to create a ca in cluster - build multiple ca servers and use haproxy to load balancer them - you need a mysql server to store the user identities globally (among the cluster) the config can be found in fabric-ca-server-config.yaml db:type you may need to set sql_mode='' in the mysql server to fix the incapability intermediate ca\nstart ca with fabric-ca-server start -b admin:adminpw -u http://\u0026lt;enrollmentID\u0026gt;:\u0026lt;secret\u0026gt;@\u0026lt;parentserver\u0026gt;:\u0026lt;parentport\u0026gt; to enroll a peer with intermediate ca. you need to concat root ca + intermediate certs for the org definition and msp (user identity) Note for exam\u003e Note for exam # remote desktop environment XFCE 4.14 Guacamole 1.4.0 XFCE Terminal Emulator (black background, white font) Ubuntu 20.04 Firefox Browser PSI secure browser interface retake policy - one free retake per Exam purchase Update on Certification Exam Proctoring Migration mentioned a few important things about the exam: no personal bookmarks anymore - it\u0026rsquo;s very stupid\u0026hellip; copying and pasting the yaml in vim will cause incorrect indentation - fix it by :set paste! copy and paste from the terminal will be Copy = CTRL+SHIFT+C, Paste = CTRL+SHIFT+V for Paste - you may need to get used to it External monitor: only 1 active monitor is allowed - If you are using macbook, you will see 2 monitors in \u0026ldquo;About This Macs \u0026gt; Displays\u0026rdquo;. There is no way for me to disable the built-in monitor unless I close the lid. So I can\u0026rsquo;t use the monitor during the exam. ","date":"2022-07-18","permalink":"/posts/exam-hyperledger-fabric/","section":"Posts","summary":"Notes\u003e Notes # installation - curl -sSL https://bit.ly/2ysbOFE | bash -s \u0026ndash; 2.2.7 1.5.3 to start a basic test network (two org, 1 application channel) network.sh up createChannel -c mychannel -s couchdb without fabric-ca network.","title":"Hyperledger fabric"},{"content":"api\u003e api # api gateway throttling limits aws throttling limit (region level) per account per-api per-stage (methods) per-client (usage plan) three type of endpoint edge-optimized (default) - route to nearest cloudfront regional private application discovery service\u003e application discovery service # for migration planning connection type application discovery agent -\u0026gt; install on server. support vm / physical server discovery connector -\u0026gt; install on vCenter (is a ova) Migration Hub import =\u0026gt; import the details directly asg\u003e asg # will automatically tag the instances by default cooldown will start after last instances launched if there are multiple instance scale at the moment athena\u003e athena # performance tunning partition data compression (glue) optimise the file size (aws glue) use columnar (apache orc, parquet with spark or hive on EMR) prevent select * use limit by ( guide for columnar) billing\u003e billing # cost allocation tags - tags will show in the cost \u0026amp; usage report budget - create alert if cost exceed the budget setup for cost analysis enable cost allocation tags in billing allow user access the billing cost allocation tags =\u0026gt; tags user-define =\u0026gt; user:XXXX aws generated =\u0026gt; aws:XXXX cost explorer =\u0026gt; ui for search and filtering cost category =\u0026gt; filter in cost explorer (saved filter) cost budget =\u0026gt; billing alarm with foretasted charged + filtering + linked account; billing alert =\u0026gt; amount already be charged billing alert include recurring fee like premium support ec2 instance-hours but exclude one off fee refund forecast cf\u003e cf # access control cf + waf + elb, it should be cf (set custom header) \u0026gt; waf (validate the rule) \u0026gt; alb cf + s3, =\u0026gt; cf with oai \u0026gt; s3 bucket policy cf + alb =\u0026gt; cf with custom header \u0026gt; alb rule reason of cfn with s3 access denied errors s3 block public access must turn off if no oas policy is set - because it will override the permissions that allow public read access if request pays is turn on, the request must include the payer header object cannot be kms encrypted cfn\u003e cfn # can use automatic deployment to auto deploy existing stackset to new accounts in organisation cloudhsm\u003e cloudhsm # need tcp/3389 for windows and tcp/22 for linux to connect to ec2 to install cloudhsm client; tcp/2223-2225 to communicate with the cluster cloudtrail\u003e cloudtrail # best practice to migrate to org trail create org trail in central account create bucket for org (need to set bucket policy to allow member account to write to it) enable cloudtrail feature in org create org trail through cli move old trail data from member accounts to org trail bucket stop cloudtrail in member accounts and remove the old trail buckets codecommit\u003e codecommit # data protection use macie =\u0026gt; can help protecting data in s3 codedeploy\u003e codedeploy # need to connect to s3 and codedeploy endpoints cw\u003e cw # cw embedded metric format =\u0026gt; can automatrically create metric from log cw endpoints =\u0026gt; monitoring.us-east-2.amazonaws.com monitoring.XXXX no az treats each unique combination of dimensions as a separate metric, even if the metrics have the same metric name data pipeline\u003e data pipeline # components pipeline definition pipeline schedule task runner swf which is specific for data engineering task runner can run on on-premise hosts task runners can be run on any compute resources (ec2 and on-premise servers) use resources in multiple regions only supported in limited region data sync\u003e data sync # use for transfer data between on-premise and aws or between aws service support cross-region (s3 \u0026lt;=\u0026gt; s3, efx \u0026lt;=\u0026gt; efx, efs \u0026lt;=\u0026gt; efs) sync source location destination location ddb\u003e ddb # support atomic counter local secondary index only can create at table creation eb\u003e eb # can stop start eb environment with lambda at scheduled time doesn\u0026rsquo;t support HTTPS_PROXY ebs\u003e ebs # aws only recommend raid0 summary table for different volume types gp2 range: 100-16k iops baseline: base on volume (limited by burst credits) provision: no gp3 range: 3k-16k iops baseline: consistent 3k iops provision: 500iops/gb io2, io1 range: 100-32k iops / 32k-64 iops (only available for nitro system ) provision: io1: 50iops/gb; io2: 500iops/gb io2 block express only support with specific instance ( R5b, X2idn, and X2iedn) range: 256k iops provision: 1000iops/gb instance store - temporary block-level storage (physically attach to the host so not an network drive) ( support in specific instance type. it is free) the i/o performance are limited by ec2 instance type. although you can use raid0 to increase iops but still have a max for that queue length on ssd: 1/1000iops default block size is 4kb use case of each volume type gp2, gp3 =\u0026gt; boot, dev, test io1, io2 =\u0026gt; db st1 (hdd) =\u0026gt; large sequential workloads like data / log processing (EMR, ETL, data warehouse) sc1 =\u0026gt; save costs ec2\u003e ec2 # use cases of dual home separate the traffic by role (frontend, backend) ha (move the eni to other instance) security appliance reason eni is binding to subnet eni - when creating the eni, it inherits the public ipv4 address attribute from subnet efs\u003e efs # HA - regional replication =\u0026gt; notice that it means multi az not multi regions cross region backup create 1st lambda to backup data from efs to s3 in region a; turn on cross-region replication in s3; create 2nd lambda to restore data from s3 \u0026gt; efs in region b data sync backup solution which does not work in cross region data pipeline =\u0026gt; the backup instance cannot mount 2 efs in different regions efs-to-efs =\u0026gt; same as data pipeline solution but implemented by lambda function only aws backup =\u0026gt; does not support cross region backup dns name - file-system-id.efs.aws-region.amazonaws.com (like cw. us-east-2) efs can deliver sub or low single digit millisecond latencies with \u0026gt; 10gbps through and 500k iops launching instance is limited by the number of vcpu running per account per region running elasticache\u003e elasticache # caching strategies lazy load - set cache when select from db write through - set cache when write to db ttl - write through + lazy load but set an expire date connection endpoints node ep - read and write primary ep for write; reader ep for read (cluster mode disabled) configuration ep for read and write like node ep (cluster mode enable) automatically cache query to elasticache for rds, aurora and redshift (use proxy) support up to 500 nodes and shards elb\u003e elb # classic load balancer only support at most one subnet per az iam\u003e iam # set SAML session tags for access control (add attribute to idp metadata) policy to deny access on specific region - deny all except the global service in console, the instance profile are automatically created along with the iam role with the same name ArnLike is case-sensitive but support wildwards like * and ? group name limit is 128 characters temporary security credentials are valid until they expire mTurk\u003e mTurk # submit a request to mTurk. outsource manual tasks like taking survey, text recognition, data migration to public opsworks\u003e opsworks # setup custom recipe to config the application with other aws services information =\u0026gt; solid example for adding redis cluster connection information to rail application org\u003e org # org features to enable all consolidated billing scp is one of the aws organisation feature default is allow all =\u0026gt; can only use deny list only use allow list =\u0026gt; have to remove FullAWSAccess (the default allow all policy) other\u003e other # Public Data Sets - data set for public access. more details fileb:// is supported in kms (key) ec2 user data (gzip) s3 (encryption key) govcloud comparson billing and using can be viewed in standard account only us citizen employees can administer the govcloud authentication is isolated from amazon.com network is isolated from other region migrate IBM MQ to Amazon MQ migrate ibm db2 luw to rds (mysql postgresql) iot monitor can check whether the rule has been executed rds\u003e rds # RMAN restore isn\u0026rsquo;t supported for Amazon RDS for Oracle DB instances (RMAN is an backup and store tool for oracle db) route53\u003e route53 # health check must respond with 2xx or 3xx. support tcp and http support DNSSEC s3\u003e s3 # access control to object c (account c) from request user a (account a) and s3 bucket (account b) check the iam role in account a check the bucket policy in account b check the object acl in object owner event notification support object and bucket level but it will resend the notification and sometimes will delay so people use cloudwatch event instead there is a check in trusted advisor for the check open access in s3 bucket but no remediation for that. to fix the bucket permission automatically and use lambda + cloudwatch event cf cannot cache if the request is larger than 30gb. can use range request to chunk the large file into smaller object requester pays don\u0026rsquo;t support 1. anonymous request 2. SOAP request default 100, max 1000 in each account genomics data processing use case sync data to s3 with data sync use s3 for data storage use storage gateway (on-premise access) / fsx (ec2 access) s3 encryption - only support symmetric keys when downloading encrypted s3 object, have to download the encrypted object along with a cipher blob version of the data key. client send the cipher blob to kms to get the plaintext version of data key to decrypt the object reduced redundancy is one of the storage class in s3 but not recommend bylaws - may have a chance to lose the object secret manager\u003e secret manager # set RotationSchedule to schedule an auto rotation (to run a custom lambda) the rds password snowball\u003e snowball # tips to increase performance batch small file multiple copy operations at one time (2 terminals 2 cp command) connect to multiple workstation (1 snowball can connect to multiple workstation) step for using snowball start the snowball setup workstation by download ova image and import to the vmware use cp command (something like aws s3api cp) to copy file to snowball can upload through gui / command line send the device back to aws. they will import your data to s3 take at least 1 weeks sso\u003e sso # permission sets - 1 permission set has multiple iam policies =\u0026gt; associate to user / group sso ad (identity provider) -\u0026gt; aws sso -\u0026gt; application (github, dropbox) / aws accounts sources of identity provider aws sso ad connector aws managed ad external ad (two way trust) server -\u0026gt; client server = adfs create an app config app sign-in and sign-out url client = integrated website trusted idp config idP\u0026rsquo;s sign-in and sign-out url + cert user login with ad\u0026rsquo;s app endpoint =\u0026gt; ad post data to app\u0026rsquo;s sign-in url =\u0026gt; app receive and decrypt the data from ad and give permission to user aws iam federation =\u0026gt; single account only storage gateway\u003e storage gateway # need to download ova and import the vm to create a endpoint to bridge on-premise and aws storage gateway type volume =\u0026gt; mount as a disk (iSCSI) =\u0026gt; s3 =\u0026gt; ebs cached =\u0026gt; save some frequently used data to vm stored =\u0026gt; completely on s3 file =\u0026gt; smb / nfs tape =\u0026gt; tape backup software support\u003e support # paid support plans allow unlimited number of users to open technical support cases swf\u003e swf # swf vs step function: use step function first. if does not fit =\u0026gt; swf use case: processing large product catalogue using Amazon Mechanical Turk vpc\u003e vpc # 5 sg / eni for dx, need to enabled route propagation resource arn supported in dx tenancy vpc will determine the instance tenancy by default. for vpce, need to ensure the private dns option is enabled worksplace\u003e worksplace # use connection aliases for cross-region workspaces redirection create connection alias share to other account associate with directories in each region setup route53 for failover setup connection string maintenance - support regular maintenance windows (eg 15:00-16:00) or manual maintenance but cannot set something like patching on tue 3:00 workspaces application manager - package manager to help installing software workspaces support Windows 10 desktop but no Windows server ","date":"2022-04-06","permalink":"/posts/exam-aws-sa/","section":"Posts","summary":"api\u003e api # api gateway throttling limits aws throttling limit (region level) per account per-api per-stage (methods) per-client (usage plan) three type of endpoint edge-optimized (default) - route to nearest cloudfront regional private application discovery service\u003e application discovery service # for migration planning connection type application discovery agent -\u0026gt; install on server.","title":"AWS SA Professional"},{"content":"","date":"2022-04-06","permalink":"/tags/cert/","section":"Tags","summary":"","title":"cert"},{"content":"","date":"2022-04-06","permalink":"/tags/exam/","section":"Tags","summary":"","title":"exam"},{"content":"","date":"2022-04-06","permalink":"/tags/sa/","section":"Tags","summary":"","title":"sa"},{"content":"DevOps choices\u003e DevOps choices # Deployment\u003e Deployment # faster boot time - opsworks slower; ami faster using chef - opsworks need to update config when new node online - opsworks Configure lifecycle event less administrative overhead: eb \u0026gt; cloudformation when both solutions works auto healing: opsworks, codedeploy, eb (bcoz of the asg) rolling: eb, opsworks (not ideal, it is manual deploy), cloudformation+asg+ AutoScalingRollingUpdate policy, codedeploy rolling = drop traffic to n instance \u0026gt; deploy \u0026gt; allow traffic in-place = deploy the deploy to all instances (parallel) blue/green deployment: eb ( cname swap), codedeploy, 2x(cfn+asg+elb)+route53 or 2(cfn+asg)+elb(weighted target groups) blue/green deployment and want to delay the old asg termination: codedeploy canary deployment: codedeploy only on lambda / ecs, eb ( traffic splitting), api gateway eb\u0026rsquo;s immutable deployment: create 2nd asg. deploy code to new asg and create new resources in batch \u0026gt; delete old asg after the deployment. (kind of rolling deployment. not a blue/green, the new resource will accept the traffic during the deployment) a/b test for a long time: (cloudformation+asg+alb)x2+route53 weighted round robin multi app, multi dependencies: use docker: cfn, eb rollback by CloudWatch alarm: codedeploy, eb not work (only by health check) some nodes are not updated after a successful codedeploy deployment: asg create new node during deployment codedeploy sucks in the lifecycle event hook (ec2 ~1hr): script error codedeploy sucks at the AllowTraffic lifecycle event: elb health check failed opsworks sucks at booting: agent doesn\u0026rsquo;t start or incorrect iam role in instance profile all lifecycle event skipped in codedeploy: agent doesn\u0026rsquo;t start / security group blocked the communication limit the resources on cfn launching: use catalog (like a marketplace). more iam control to the cloudformation template. With cloudfromation, you cannot limit user upload what cloudformation template. Backup \u0026amp; Restore\u003e Backup \u0026amp; Restore # cross region efs backup: lambda: (at region1) use ec2 with efs mount put data to region2 s3 -\u0026gt; (region2) ec2 with efs mount pull data from s3 Cloudformation\u003e Cloudformation # work as teams / multi tiers (network, security, application): nested stack / importvalue ASG\u003e ASG # troubleshoot asg instance: standby / asg lifecycle hook (1hr only) handle predictable traffic: scheduled scaling prevent scale-in: instance scale-in protection sending log / licence register deregister: asg lifecycle hook Data analysis / Loggings\u003e Data analysis / Loggings # batch jobs / reporting (like spark): EMR visualise data (like BI): Redshift / Quicksight (cost effective) log searching: elastic search (ES) (renamed to opensearch) query S3 data: athena report time slow: offload the job to other application(like lambda) with kinesis stream / scale-up the cluster with asg apache hive ~= aws glue DB\u003e DB # dynamodb stream = kinesis stream (more advance) throttle on dynamodb stream: limited to 2 connections at the moment. use 1 lambda \u0026gt; sns \u0026gt; other lambda(s) dynamodb with many read: dynamodb accelerator (like redis) multi-region read \u0026amp; write: dynamodb global table multi-region read only: aurora short DR time: read replica -\u0026gt; promote (/ aurora global database) long DR time (few hours): lambda to backup and restore conditionalcheckfailedexception at dynamodb: too many write on same record data inconsistency in dynamodb: need to use strong consistent read DB security: auth with iam Config\u003e Config # config aggreation: use stackset to enable config cross accounts \u0026gt; assign a dedicated administrator \u0026gt; auth config aggregator (like peer connection, request at one side and accept at other account) | use org organisation config organisation rule: can use this to put rules to all account in organisation Application Discovery Service\u003e Application Discovery Service # Setting up\u003e Setting up # Discovery Connector - agentless - install on vmware centre Discovery Agent - agent - install on host CodeCommit\u003e CodeCommit # approval rule (pull request)\u003e approval rule (pull request) # targets: branch approval pool members (iam user) protect branches\u003e protect branches # migrate from git\u003e migrate from git # git clone git_repo_url --mirror to create bare repo git push codecommit_repo_url --all git push codecommit_repo_url --tags CodeBuild\u003e CodeBuild # find the branch name in codebuild: CODEBUILD_SOURCE_VERSION one codebuild can have one builspec.yml data encryption: at rest and in transit can use aws managed / custom docker image at build environment custom docker image can be chosen from ecr (same/cross account) or custom registry buildspec.yml\u003e buildspec.yml # many phases but all are inline commmands can use parameter-store and secrets-manager can set \u0026ldquo;finally\u0026rdquo; block in each phase \u0026amp; on-failure behavior CodeDeploy\u003e CodeDeploy # appspec.yml\u003e appspec.yml # resources + hooks (ecs \u0026amp; lambda) files + permission + hooks (ec2) lifecycle\u003e lifecycle # notification\u003e notification # targets: sns / aws chatbot event: any activities (push, merge, delete branch\u0026hellip;) trigger\u003e trigger # targets: sns / lambda event: push branches or tags api gateway\u003e api gateway # can do canary deploy targets\u003e targets # lambda step functions http event sqs kinesis data stream config\u003e config # trigger type: configuration changes / periodic scope: aws resource \u0026gt; ec2:securitygroup notifications\u003e notifications # Settings \u0026gt; Delivery method \u0026gt; sns topic - give you all changes (summary) Settings \u0026gt; Amazon Cloudwatch Events rule - good for watching specific resource config change Events\u003e Events # can send to another account can send to another account in organisation Cloudwatch\u003e Cloudwatch # create alarm from logs\u003e create alarm from logs # logs \u0026gt; metric filter \u0026gt; metric \u0026gt; alarm \u0026gt; sns send logs to other place for analysis\u003e send logs to other place for analysis # logs \u0026gt; subscription filter \u0026gt; lambda (cannot cross account, kinesis can) \u0026gt; s3 \u0026gt; athena logs \u0026gt; subscription filter \u0026gt; kinesis firehose \u0026gt; s3 \u0026gt; athena logs \u0026gt; subscription filter \u0026gt; kinesis stream \u0026gt; kinesis firehose \u0026gt; s3 \u0026gt; athena Kinesis\u003e Kinesis # kinesis stream -\u0026gt; real time data stream (like enhanced version of DYNAMODB Stream) for analysis / aggregation kinesis stream firehose -\u0026gt; for storage (s3) but can do some pre-processing Supported Writer \u0026amp; Reader\u003e Supported Writer \u0026amp; Reader # aws sdk / agent / library (KPL) \u0026gt; stream \u0026gt; library (client) / firehose / lambda aws sdk / agent / stream / CloudWatch event / CloudWatch logs \u0026gt; firehose \u0026gt; S3 / ES / Redshift / MongoDB / Splunk Security\u003e Security # GuardDuty: threat detection Macie: data level eg: s3 Security Hub: give advises / integrated with different aws products like inspector: cvs scanning / hardending (cis) S3\u003e S3 # Cross account replication\u003e Cross account replication # AcctA BuckA AcctB BuckB iam role in acctA trust s3 to assume role give permission to s3 to get buckA object give permission to acctB to replica buckA object give permission to s3 to encrypt \u0026amp; decrypt buckA object bucket policy in acctB allow roleA to replica and put object to buckB Cloudformation\u003e Cloudformation # Custom Resource\u003e Custom Resource # bind to lambda / sns need to handle the create / update / delete event from the stack ECS\u003e ECS # AMI\u003e AMI # use ECS-optimised AMI - have container agent installed\nloggings\u003e loggings # container log - awslogs - need container agent system log - cloudwatch agent ","date":"2022-03-06","permalink":"/posts/exam-aws-devops/","section":"Posts","summary":"DevOps choices\u003e DevOps choices # Deployment\u003e Deployment # faster boot time - opsworks slower; ami faster using chef - opsworks need to update config when new node online - opsworks Configure lifecycle event less administrative overhead: eb \u0026gt; cloudformation when both solutions works auto healing: opsworks, codedeploy, eb (bcoz of the asg) rolling: eb, opsworks (not ideal, it is manual deploy), cloudformation+asg+ AutoScalingRollingUpdate policy, codedeploy rolling = drop traffic to n instance \u0026gt; deploy \u0026gt; allow traffic in-place = deploy the deploy to all instances (parallel) blue/green deployment: eb ( cname swap), codedeploy, 2x(cfn+asg+elb)+route53 or 2(cfn+asg)+elb(weighted target groups) blue/green deployment and want to delay the old asg termination: codedeploy canary deployment: codedeploy only on lambda / ecs, eb ( traffic splitting), api gateway eb\u0026rsquo;s immutable deployment: create 2nd asg.","title":"AWS Certified DevOps Engineer - Professional"},{"content":"","date":"2022-02-21","permalink":"/tags/aurora/","section":"Tags","summary":"","title":"aurora"},{"content":"RDS Read Replica\u003e RDS Read Replica # cross AZ: OK cross region: OK promote / point-in-time recovery / restore from snapshot will create a new db instance which is not in my expectation. When DR, user need to update dns / db endpoint in application Aurora\u003e Aurora # Pros over non-aurora rds\u003e Pros over non-aurora rds # read replica auto scaling failover (failover master to read replica in same region) multi-az by default endpoint for writer and reader instance (also custom endpoint) Cons in aurora\u003e Cons in aurora # Bad integration in cross-region read replica\u003e Bad integration in cross-region read replica # For example:\nRegion1: master, read-replica Region2: master, read-replica The replication between the multi region db clusters is:\nmaster-slave replication between region1 master and read-replica master-slave replication between region2 master and read-replica master-slave replication between region1 master and region2 master When write record to region1 master, the record will be replicated to\nregion1 read-replica (by region1 master) region2 master (by region1 master) region2 read-replica (by region2 master) When writing a record to region2 master, the record will only be synchronised to region2 read-replica.\nAnd both region1 and region2 masters are writables!!!\nAWS \u0026ldquo;assume\u0026rdquo; people will use the region2 cluster as a readonly database.\nThis is not mentioned in aws doc. I found the answer from aws:repost instead.\nwhen create a cross-region read-replica, it will create another db cluster in the new region you cannot see the cross-region db in the same aws console endpoint cannot cross region, only within the region need to manually set binlog_format in the parameter group to before creating the read replica need to manually set read_only true in the parameter group at second region db cluster to prevent its writer instance from writable by other endpoint writer / reader instance depends on role so the writer endpoint in second region is incorrect and should be not used route53 + latency based routing policy will need to be set up to route both reader endpoints to utilise those read-replicas. but the reader endpoint does not include the master. if you want to utilise the second region master, You need to create a custom endpoint for this So AWS created Aurora Global Database to solve these drawbacks. But it does not support on small db instance type!\naurora serverless\u003e aurora serverless # features Pros\u003e Pros # scale up quickly (v1: 1min, v2: immediately) scale down quickly (v1: 15min, v2: 1min) save cost - can scale down to 0 when no active traffic MySQL \u0026amp; PostgreSQL compatible no need to manage the cluster Cons\u003e Cons # price 30% higher than same power\u0026rsquo;s ec2 instance cannot cross-region no public access ","date":"2022-02-21","permalink":"/posts/aws-aurora/","section":"Posts","summary":"RDS Read Replica\u003e RDS Read Replica # cross AZ: OK cross region: OK promote / point-in-time recovery / restore from snapshot will create a new db instance which is not in my expectation.","title":"Aurora"},{"content":"config aggregator\u003e config aggregator # aggreagate all account under organization\u003e aggreagate all account under organization # enable service role in organization\nset up iam with\nviewing the organization service role give config.amazonaws.com access for the config resource\nadditional iam right to view accounts in organization\nfrom management account or delegated admin to use this option\naggreagate specfic account\u003e aggreagate specfic account # authorization cfn stackset\u003e cfn stackset # add stack to stackset = deploy stack delete stack from stackset = delete the stack\norganziation\u003e organziation # use managed service mode enable service role in organization change choose account in organization\nother discount\u003e other discount # use iam service role\nadministrator role: from ac\ntrust cloudwatch can assume as execute role executor role: to ac\ntrust admintrator ac have access to create resource for example\nRaphael = Cloudformation Wayne = Administrator (trust Cloudformation) Hugo = Executor (trust Adminstrator) that will be\nWayne trust Raphael Hugo trust Wayne Hugo can do the job Wayne can use Hugo Wayne ask Raphael to ask Hugo to do the job = admin ac let cloudformation to assume hugo\u0026rsquo;s right to create resource in hugo account\n","date":"2022-02-21","permalink":"/posts/aws-config/","section":"Posts","summary":"config aggregator\u003e config aggregator # aggreagate all account under organization\u003e aggreagate all account under organization # enable service role in organization\nset up iam with\nviewing the organization service role give config.","title":"AWS Config"},{"content":"eb\u003e eb # eb init eb use web_dev eb list deploy code in repo\neb deploy web_dev edit config\neb config web_dev save config\neb config save apply save config\neb config put .elasticbeanstalk/saved_configs/Web-env-sc.cfg.yml Folder Structure\n▾ .ebextensions/ app.config ▾ .elasticbeanstalk/ ▸ saved_configs/ config.yml ▸ .git/ .gitignore index.php README swap name, configuration with not swap\n","date":"2022-02-21","permalink":"/posts/aws-eb/","section":"Posts","summary":"eb\u003e eb # eb init eb use web_dev eb list deploy code in repo\neb deploy web_dev edit config\neb config web_dev save config\neb config save apply save config","title":"AWS Elastic Beanstalk"},{"content":"aws org\u003e aws org # o = organization = it9 r = root = root ou = organization unit = dev / prod / uat / management / network ou path: o-3ywbznlomt/r-t9g3/ou-t9g3-knls7nau/\n","date":"2022-02-21","permalink":"/posts/aws-organization/","section":"Posts","summary":"aws org\u003e aws org # o = organization = it9 r = root = root ou = organization unit = dev / prod / uat / management / network ou path: o-3ywbznlomt/r-t9g3/ou-t9g3-knls7nau/","title":"AWS Organization"},{"content":"","date":"2022-02-21","permalink":"/tags/config/","section":"Tags","summary":"","title":"config"},{"content":"","date":"2022-02-21","permalink":"/tags/database/","section":"Tags","summary":"","title":"database"},{"content":"","date":"2022-02-21","permalink":"/tags/eb/","section":"Tags","summary":"","title":"eb"},{"content":"","date":"2022-02-21","permalink":"/tags/opsworks/","section":"Tags","summary":"","title":"opsworks"},{"content":"OpsWorks\u003e OpsWorks # stack = cookbooks layer = frontend, backend, api, rds (how to config to instance) app = source code. can deploy and redeploy Stack\u003e Stack # I am exploring this cookbook from aws The nodejs demo cookbook repo does not include its dependencies In the opsworks demo, it uses opsworks-linux-demo-cookbooks-nodejs.tar.gz as the cookbook source We need to convert the repo to the archive before using it Aftering changing the cookbook source, we need to fetch the cache by running commands on instances Inside the opsworks-linux-demo-cookbooks-nodejs.tar.gz, we have\nnodejs_demo recipes default.rb To run this cookbook, we set nodejs_demo::default in the layer\nDocker Support\u003e Docker Support # eb -\u0026gt; multi docker / docker + elb opsworks -\u0026gt; ecs with linux + ec2 Layer\u003e Layer # auto-healing = restart the instance when losing connection with opsworks agent\nelb\nsg\nebs\neip\ncloudwatch\ninstance\nrds service layer\nelb service layer\necs cluster layer\nApp\u003e App # source code env domain ssl db source code are placed at /svr/\u0026lt;app_name\u0026gt;\nInstance Types\u003e Instance Types # 24/7 = normal instance time = start at specific period load-based = asg Behaviors on auto-healing between these two instance type\nebs-backed = re-create the instance instance = stop and start Issue\u003e Issue # Instances are stuck in booting\u003e Instances are stuck in booting # Reason: incorrect instance iam role\nWe can find the error message at\ncat tail -f /var/logs/aws/opsworks/* My instance iam role is incorrect because it should trust ec2.amazonaws.com instead of opsworks.\nand I don\u0026rsquo;t realise that opsworks need 2 iam roles:\nservice role: use by opsworks. need to trust opsworks.amazonaws.com and passrole to ec2.amazonaws.com instance iam role: use by ec2. need to trust ec2.amazonaws.com and passrole to opsworks.amazonaws.com ","date":"2022-02-21","permalink":"/posts/aws-opsworks/","section":"Posts","summary":"OpsWorks\u003e OpsWorks # stack = cookbooks layer = frontend, backend, api, rds (how to config to instance) app = source code. can deploy and redeploy Stack\u003e Stack # I am exploring this cookbook from aws The nodejs demo cookbook repo does not include its dependencies In the opsworks demo, it uses opsworks-linux-demo-cookbooks-nodejs.","title":"Opsworks"},{"content":"","date":"2022-02-21","permalink":"/tags/orgnaization/","section":"Tags","summary":"","title":"orgnaization"},{"content":"Resources\u003e Resources # Hyperledger Fabric Solidity Notes\u003e Notes # what is blockchain\u003e what is blockchain # The blockchain solution in IoT / Supply chain are changing \u0026ldquo;Mesh\u0026rdquo; network to a \u0026ldquo;Star Network each parties access and write to the \u0026ldquo;blockchain network\u0026rdquo;. not more api integration between parties How to use it\u003e How to use it # How it work\u003e How it work # use cryptography methods to ensure the integrity of a public writable database proof of work, proof of stake, proff How to implement\u003e How to implement # solidity node.js truffle web3.js Any product (e.g crypto currency/ nft)\u003e Any product (e.g crypto currency/ nft) # Networks\u003e Networks # Use Cases Public\u003e Public # btc eth bsc (binance smark contract) Private (have acl)\u003e Private (have acl) # each parties setup servers to join the blockchain network\nall parties hold the data but they can set access right to the data\nHyperledger Fabric\nDecentralized Storage\u003e Decentralized Storage # IPFS Staking\u003e Staking # CrowdSale\u003e CrowdSale # user -\u0026gt; sale contract -\u0026gt; nft contract\nLottery\u003e Lottery # ashisherc/advanced-solidity-lottery-application NFT\u003e NFT # Auction ( dutch and English)\nmint\nCrowdSale\nERC1155\nLootBox\nopensea\nscv.finance\nLa Collection\nSwap / Farming\u003e Swap / Farming # ask people input proportion of coin into the pool, like eth:bnb. so people can trade between eth and bnb\nuse Oracles for pricing\u0026hellip;a smart contract which referencing different sources from other swap exchange \u0026amp; their coin pair pool size\npancakeswap(bsc)\nuniswap(eth)\nWhat is ECR20 721 1155\u003e What is ECR20 721 1155 # ECR20 = coin ECR721 = NFT ECR1155 = 1 contract with multi token, eg: game item What is smart contract\u003e What is smart contract # Code stored on blockchain Demo\u003e Demo # NFT Contract\u003e NFT Contract # NFT Example\nContract Wizard\nCommon Functions\nMintable (increase supply) Burnable (reduce supply) Pausable (freeze the contract, for upgrade) Access control: Roles (multi ac, multi actions) / Ownable (single ac, all actions) Upgradeability (Transpart / UUPS) - proxy pattern Timelock Votes NFT metadata\u003e NFT metadata # { \u0026#34;boosterId\u0026#34;: 10000000195586, \u0026#34;id\u0026#34;: \u0026#34;10000586756\u0026#34;, \u0026#34;txHash\u0026#34;: \u0026#34;0xff318896b78fd77aadf19f94b7434d1a0ea5ffddfc88b6966d92d75c69f80dd1\u0026#34;, \u0026#34;randomNumber\u0026#34;: \u0026#34;0xc55ec3b26d0bdec23fd2f9e29ae34d54e0d457ef1d413b5c537b0383330575f0\u0026#34;, \u0026#34;image\u0026#34;: \u0026#34;https://assets.polkamon.com/images/Unimons_T08C02H06B04G00.jpg\u0026#34;, \u0026#34;external_url\u0026#34;: \u0026#34;https://polkamon.com/polkamon/T08C02H06B04G00\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Mindful of a million things, the Unisheep are Polkamon creatures that can graze and gander the day away. Creating an environment to actively direct these creatures helps them focus on one particular topic.\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Unisheep\u0026#34;, \u0026#34;initialProbabilities\u0026#34;: { \u0026#34;horn\u0026#34;: 0.2, \u0026#34;color\u0026#34;: 0.345, \u0026#34;background\u0026#34;: 1, \u0026#34;glitter\u0026#34;: 0.99, \u0026#34;type\u0026#34;: 0.1847 }, \u0026#34;attributes\u0026#34;: [ { \u0026#34;trait_type\u0026#34;: \u0026#34;Type\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;Unisheep\u0026#34; }, { \u0026#34;trait_type\u0026#34;: \u0026#34;Horn\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;Spiral Horn\u0026#34; }, { \u0026#34;trait_type\u0026#34;: \u0026#34;Color\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;Green\u0026#34; }, { \u0026#34;trait_type\u0026#34;: \u0026#34;Background\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;Mountain Range\u0026#34; }, { \u0026#34;trait_type\u0026#34;: \u0026#34;Opening Network\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;Binance Smart Chain\u0026#34; }, { \u0026#34;trait_type\u0026#34;: \u0026#34;Glitter\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;No\u0026#34; }, { \u0026#34;trait_type\u0026#34;: \u0026#34;Special\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;No\u0026#34; }, { \u0026#34;display_type\u0026#34;: \u0026#34;date\u0026#34;, \u0026#34;trait_type\u0026#34;: \u0026#34;Birthday\u0026#34;, \u0026#34;value\u0026#34;: 1628902374 }, { \u0026#34;display_type\u0026#34;: \u0026#34;number\u0026#34;, \u0026#34;trait_type\u0026#34;: \u0026#34;Booster\u0026#34;, \u0026#34;value\u0026#34;: 10000000195586 } ], \u0026#34;opening_network\u0026#34;: \u0026#34;Binance Smart Chain\u0026#34;, \u0026#34;background_color\u0026#34;: \u0026#34;FFFFFF\u0026#34;, \u0026#34;animation_url\u0026#34;: \u0026#34;https://assets.polkamon.com/videos/Unimons_T08C02H06B04G00.mp4\u0026#34;, ... } Read Contract\u003e Read Contract # Owner account: 0xaB111a3EaFE79b3110162d0e8b6FF1102ed25E2A token id: 10000586756 Functions:\nbalanceOf (3) getApproved (4) Role: DEFAULT_ADMIN_ROLE / MINTER_ROLE (1,2) getRoleMember (6) ownerOf (11) Write Contract\u003e Write Contract # Functions:\nburn (2) mint (4) grantRole (3) safeTranferFrom setTokenURI Wallet\u003e Wallet # wordphase - BIP39 Connect from browser\u003e Connect from browser # PancakeSwap Remix IDE bscscan.com Contract deployment\u003e Contract deployment # Code Example will need a wallet and Blockchain endpoint References\u003e References # https://www.euromoney.com/learning/~/media/0E855B86EDF04F3C8EABAFC42917C8C6.png?la=en\u0026hash=B67568B63CBB2C0C7311DE742F5B9E48E86DC8B9 ","date":"2022-01-22","permalink":"/posts/blockchain-introduction/","section":"Posts","summary":"Resources\u003e Resources # Hyperledger Fabric Solidity Notes\u003e Notes # what is blockchain\u003e what is blockchain # The blockchain solution in IoT / Supply chain are changing \u0026ldquo;Mesh\u0026rdquo; network to a \u0026ldquo;Star Network each parties access and write to the \u0026ldquo;blockchain network\u0026rdquo;.","title":"Blockchain Introduction"},{"content":"","date":"2022-01-17","permalink":"/tags/202201/","section":"Tags","summary":"","title":"202201"},{"content":"","date":"2022-01-17","permalink":"/tags/booklist/","section":"Tags","summary":"","title":"booklist"},{"content":" 語理分析的思考方法 - 李天命 Salt Sugar Fat: How the Food Giants Hooked Us Sapiens: A Brief History of Humankind (人類大歷史) 機器學習書單 SRE Book 列夫·托爾斯泰作品集 ","date":"2022-01-17","permalink":"/posts/booklist/","section":"Posts","summary":" 語理分析的思考方法 - 李天命 Salt Sugar Fat: How the Food Giants Hooked Us Sapiens: A Brief History of Humankind (人類大歷史) 機器學習書單 SRE Book 列夫·托爾斯泰作品集 ","title":"Booklists"},{"content":"","date":"2022-01-17","permalink":"/tags/firefox/","section":"Tags","summary":"","title":"firefox"},{"content":" go to about:profile create a new profile Profile Manager - Create, remove or switch Firefox profiles\n","date":"2022-01-17","permalink":"/posts/multi-profiles-in-firefox/","section":"Posts","summary":"go to about:profile create a new profile Profile Manager - Create, remove or switch Firefox profiles","title":"Multiple profile in firefox"},{"content":" https://www.youtube.com/watch?v=_MwNgO0aJo0\nhttps://www.youtube.com/watch?v=namlZpQ6W_k\n","date":"2021-12-23","permalink":"/posts/aws-ground-station/","section":"Posts","summary":"https://www.youtube.com/watch?v=_MwNgO0aJo0\nhttps://www.youtube.com/watch?v=namlZpQ6W_k","title":"Aws Ground Station"},{"content":" https://www.youtube.com/watch?v=iwNg_fusT9A\nCopy on Write - copy will not perform immediately only when write operation is happend Snapshot - use the tech of copy on write to save some space. size few bytes to same size as original data ","date":"2021-12-23","permalink":"/posts/btrfs/","section":"Posts","summary":" https://www.youtube.com/watch?v=iwNg_fusT9A\nCopy on Write - copy will not perform immediately only when write operation is happend Snapshot - use the tech of copy on write to save some space. size few bytes to same size as original data ","title":"BTRFS"},{"content":"","date":"2021-12-23","permalink":"/tags/file-system/","section":"Tags","summary":"","title":"file-system"},{"content":"In this post, he wrote a web app to randomly generate the result with QRNG@ANU api.\nIt can generate Quantum random numbers. so interesting\u0026hellip;\n","date":"2021-12-23","permalink":"/posts/random-numbers/","section":"Posts","summary":"In this post, he wrote a web app to randomly generate the result with QRNG@ANU api.\nIt can generate Quantum random numbers. so interesting\u0026hellip;","title":"Random number"},{"content":"","date":"2021-12-23","permalink":"/tags/random-number/","section":"Tags","summary":"","title":"random-number"},{"content":"","date":"2021-12-23","permalink":"/tags/space/","section":"Tags","summary":"","title":"space"},{"content":"","date":"2021-12-23","permalink":"/tags/use-case/","section":"Tags","summary":"","title":"use-case"},{"content":"","date":"2021-12-14","permalink":"/tags/azure/","section":"Tags","summary":"","title":"azure"},{"content":"","date":"2021-12-14","permalink":"/tags/cloud-centre/","section":"Tags","summary":"","title":"cloud-centre"},{"content":" https://www.youtube.com/watch?v=tj3F0tX8eyc\n6:26 - 5 steps\nAssess\u003e Assess # Cloud Economic Assessment (8:20)\u003e Cloud Economic Assessment (8:20) # understand resources needed on-premise (mem, storage, bandwidth)\nPlan\u003e Plan # use cloud adoption framework (10:45) Migrate\u003e Migrate # Cloud Migration Strategies (4:30)\u003e Cloud Migration Strategies (4:30) # Rehost - create vm on cloud and move application to it\nRefactor - Containerize\nRearchitech \u0026amp; rebuild - Microservice\n5-week migration process (17:00)\u003e 5-week migration process (17:00) # Week T-3 plan Week T-2 build Week T-1 test Week T-0 go migration and cleanup Week T+1 transition Use azure migrate (17:59)\u003e Use azure migrate (17:59) # Setup migration Centre of Excellence (CoE) (20:18)\u003e Setup migration Centre of Excellence (CoE) (20:18) # Goven \u0026amp; Manage\u003e Goven \u0026amp; Manage # Monitor/ automation\u003e Monitor/ automation # Five discipline of cloud governance (21:51)\u003e Five discipline of cloud governance (21:51) # principles on how to manage / goven\nCost management\u003e Cost management # Bill insight / report Security baseline\u003e Security baseline # Patch manager Compliance software Resource consistency\u003e Resource consistency # Configuration Management (ansible) Infrastructure as code (terraform) Immutable application(container) Identity baseline\u003e Identity baseline # IAM policy Zero trust approach Deployment acceleration\u003e Deployment acceleration # Automation tools (scripts, ansible) Pipeline (jenkins) Optimise\u003e Optimise # Unifycloud - cloud pilot\u003e Unifycloud - cloud pilot # Static Code Analytics Cost Management App \u0026amp; DB Readiness Migration Effort Estimate ","date":"2021-12-14","permalink":"/posts/az-migration-strategies/","section":"Posts","summary":"https://www.youtube.com/watch?v=tj3F0tX8eyc\n6:26 - 5 steps\nAssess\u003e Assess # Cloud Economic Assessment (8:20)\u003e Cloud Economic Assessment (8:20) # understand resources needed on-premise (mem, storage, bandwidth)\nPlan\u003e Plan # use cloud adoption framework (10:45) Migrate\u003e Migrate # Cloud Migration Strategies (4:30)\u003e Cloud Migration Strategies (4:30) # Rehost - create vm on cloud and move application to it","title":"Coretek's cloud migration methodology"},{"content":"","date":"2021-12-14","permalink":"/tags/migration-centre/","section":"Tags","summary":"","title":"migration-centre"},{"content":"","date":"2021-12-14","permalink":"/tags/ml/","section":"Tags","summary":"","title":"ml"},{"content":"","date":"2021-12-14","permalink":"/tags/spark/","section":"Tags","summary":"","title":"spark"},{"content":" Playlist\nApache Spark on EKS\u003e Apache Spark on EKS # https://github.com/kubernetes/kubernetes/tree/release-1.3/examples/spark\n","date":"2021-12-14","permalink":"/posts/spark/","section":"Posts","summary":"Playlist\nApache Spark on EKS\u003e Apache Spark on EKS # https://github.com/kubernetes/kubernetes/tree/release-1.3/examples/spark","title":"Spark"},{"content":" https://www.youtube.com/watch?v=LK6KbAlQRIg\nprovide a GUI interface manage k8s clusters Cons (21:12)\u003e Cons (21:12) # Still using docker (should use cri-o) Only support old version k8s (1.18) Mutable approach (not using image to provision, it installs docker and setup the cluster for you) Very slow to create the cluster Pros\u003e Pros # Manage multiple clusters GUI ","date":"2021-12-13","permalink":"/posts/rancher/","section":"Posts","summary":" https://www.youtube.com/watch?v=LK6KbAlQRIg\nprovide a GUI interface manage k8s clusters Cons (21:12)\u003e Cons (21:12) # Still using docker (should use cri-o) Only support old version k8s (1.18) Mutable approach (not using image to provision, it installs docker and setup the cluster for you) Very slow to create the cluster Pros\u003e Pros # Manage multiple clusters GUI ","title":"Review of Rancher"},{"content":" https://www.youtube.com/watch?v=mUTGdSA60Ao\nintroduction - Cloud Centre of Excellence is a team that can help the organisation for cloud adoption role \u0026amp; responsible characteristic for the team member best practice Characteristic\u003e Characteristic # driving the organisation forward has ability to stand up - open-minded be confident why and how to do it - clear minded / big picture thinkers diverse and cross-functional Skill Set\u003e Skill Set # Cost management Project Management (Transparent) Cloud operation engineer (CI/CD) Data engineer (new currency, capture data from IoT, video stream) Application Security (IAM, Policy, Firewall) Best Practise\u003e Best Practise # Keep passion \u0026amp; excitement Start small (5-7 ppl in a team) Rotate people (training) Communication Scaling and reorganising Think of the failover and backup plan Cloud training for organisation Bill, cost optimization Empower the team (courage them trying new role, new service, bleeding-edge tech) Gain transparency (everyone knows the progress\u0026hellip;why you do that\u0026hellip;) Notes\u003e Notes # cloud makes anything as code -\u0026gt; code makes things more transparent -\u0026gt; the complexity of the project is reduced many jobs can be automated through cloud and they are automatically documented as code is it possible to create a roadmap / resources library for the whole organisation (like AWS workshop)? - everybody can gear up the knowledge through videos / materials follow the framework and make planing\u0026hellip;like scaling\u0026hellip;we can plan it on the first day when starting the business ","date":"2021-12-10","permalink":"/posts/cloud-center-of-excellenace/","section":"Posts","summary":"https://www.youtube.com/watch?v=mUTGdSA60Ao\nintroduction - Cloud Centre of Excellence is a team that can help the organisation for cloud adoption role \u0026amp; responsible characteristic for the team member best practice Characteristic\u003e Characteristic # driving the organisation forward has ability to stand up - open-minded be confident why and how to do it - clear minded / big picture thinkers diverse and cross-functional Skill Set\u003e Skill Set # Cost management Project Management (Transparent) Cloud operation engineer (CI/CD) Data engineer (new currency, capture data from IoT, video stream) Application Security (IAM, Policy, Firewall) Best Practise\u003e Best Practise # Keep passion \u0026amp; excitement Start small (5-7 ppl in a team) Rotate people (training) Communication Scaling and reorganising Think of the failover and backup plan Cloud training for organisation Bill, cost optimization Empower the team (courage them trying new role, new service, bleeding-edge tech) Gain transparency (everyone knows the progress\u0026hellip;why you do that\u0026hellip;) Notes\u003e Notes # cloud makes anything as code -\u0026gt; code makes things more transparent -\u0026gt; the complexity of the project is reduced many jobs can be automated through cloud and they are automatically documented as code is it possible to create a roadmap / resources library for the whole organisation (like AWS workshop)?","title":"A Roadmap to Cloud Centre of Excellence Adoption"},{"content":"","date":"2021-12-10","permalink":"/tags/bandwidth/","section":"Tags","summary":"","title":"bandwidth"},{"content":"","date":"2021-12-10","permalink":"/tags/blog/","section":"Tags","summary":"","title":"blog"},{"content":"","date":"2021-12-10","permalink":"/tags/ec2/","section":"Tags","summary":"","title":"ec2"},{"content":"Objective\u003e Objective # Test if the ec2 can reach 10Gbps bandwidth in same placement group Test if the 10Gbps is the limited by the ec2 instance type or aws AWS Limit\u003e AWS Limit # https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-network-bandwidth.html\nin single-flow traffic,\nplacement group: 10Gbps w/o placement group: 5Gbps flow = worker (thread) in the network driver. depends on the ec2 instance type.\nEC2 Instance Type Limit\u003e EC2 Instance Type Limit # https://aws.amazon.com/ec2/instance-types/m5/\nm5.8xlarge can reach 10Gbps\nSetup\u003e Setup # Created 3 m5.8xlarge instances which can reach 10Gbps bandwidth each (i use spot)\n2 in same placement group 1 in other subnet Tool\u003e Tool # iperf ( BENCHMARK NETWORK THROUGHPUT) Test\u003e Test # Not in same placement group and parallel 1\u003e Not in same placement group and parallel 1 # [root@ip-172-31-33-28 ~]# iperf -c 172.31.12.68 --parallel -i 2 -t 2 iperf: ignoring extra argument -- 2 ------------------------------------------------------------ Client connecting to 172.31.12.68, TCP port 5001 TCP window size: 812 KByte (default) ------------------------------------------------------------ [ 3] local 172.31.33.28 port 38880 connected with 172.31.12.68 port 5001 [ ID] Interval Transfer Bandwidth [ 3] 0.0- 2.0 sec 1.15 GBytes 4.93 Gbits/sec [root@ip-172-31-33-28 ~]# iperf -c 172.31.12.68 --parallel -i 2 -t 2 iperf: ignoring extra argument -- 2 In same placement group and parallel 1\u003e In same placement group and parallel 1 # [root@ip-172-31-7-236 ~]# iperf -c 172.31.12.68 --parallel -i 2 -t 2 iperf: ignoring extra argument -- 2 ------------------------------------------------------------ Client connecting to 172.31.12.68, TCP port 5001 TCP window size: 1.90 MByte (default) ------------------------------------------------------------ [ 3] local 172.31.7.236 port 50544 connected with 172.31.12.68 port 5001 [ ID] Interval Transfer Bandwidth [ 3] 0.0- 2.0 sec 2.22 GBytes 9.54 Gbits/sec Not in same placement group and parallel 2\u003e Not in same placement group and parallel 2 # [root@ip-172-31-7-236 ~]# iperf -c 172.31.12.68 --parallel 2 -i 1 -t 2 ------------------------------------------------------------ Client connecting to 172.31.12.68, TCP port 5001 TCP window size: 2.00 MByte (default) ------------------------------------------------------------ [ 4] local 172.31.7.236 port 50538 connected with 172.31.12.68 port 5001 [ 3] local 172.31.7.236 port 50536 connected with 172.31.12.68 port 5001 [ ID] Interval Transfer Bandwidth [ 4] 0.0- 1.0 sec 603 MBytes 5.06 Gbits/sec [ 3] 0.0- 1.0 sec 585 MBytes 4.91 Gbits/sec [SUM] 0.0- 1.0 sec 1.16 GBytes 9.96 Gbits/sec [ 4] 1.0- 2.0 sec 604 MBytes 5.07 Gbits/sec [ 4] 0.0- 2.0 sec 1.18 GBytes 5.06 Gbits/sec [ 3] 1.0- 2.0 sec 579 MBytes 4.85 Gbits/sec [SUM] 1.0- 2.0 sec 1.16 GBytes 9.92 Gbits/sec [ 3] 0.0- 2.0 sec 1.14 GBytes 4.88 Gbits/sec [SUM] 0.0- 2.0 sec 2.32 GBytes 9.94 Gbits/sec Outcome\u003e Outcome # w/i the placement group\u003e w/i the placement group # Flow Bandwidth/Flow Bandwidth 1 10Gbps 10Gbps 2 5Gbps 10Gbps we can use up all m5.8xlarge\u0026rsquo;s bandwidth in a single-flow traffic.\nBut the traffic splits 50%,50% in double-flow traffic.\nTherefore, 10Gbps is the max bandwidth of the m5.8xlarge.\noutside the placement group\u003e outside the placement group # Flow Bandwidth/Flow Bandwidth 1 5Gbps 5Gbps 2 5Gbps 10Gbps we need two flows to use up all m5.8xlarge\u0026rsquo;s bandwidth (5Gbps, 5Gbps)\nso it reaches the limit of AWS\u0026hellip;\n","date":"2021-12-10","permalink":"/posts/aws-ec2-bandwidth-test/","section":"Posts","summary":"Objective\u003e Objective # Test if the ec2 can reach 10Gbps bandwidth in same placement group Test if the 10Gbps is the limited by the ec2 instance type or aws AWS Limit\u003e AWS Limit # https://docs.","title":"EC2 Bandwidth test"},{"content":"","date":"2021-12-10","permalink":"/tags/markdown/","section":"Tags","summary":"","title":"markdown"},{"content":"I found this markdown software when watching this video\nI am interested in that map and think if I can organise my markdown notes with it.\nTutorials\u003e Tutorials # How To Use VIM Bindings in Obsidian | Beginners Guide\u003e How To Use VIM Bindings in Obsidian | Beginners Guide # https://www.youtube.com/watch?v=yX_Qdr9-7k\nI Tried Obsidian Note Taking for a Week\u0026hellip; (MD App Review, Tips, Features, Guide, and Setup)\u003e I Tried Obsidian Note Taking for a Week\u0026hellip; (MD App Review, Tips, Features, Guide, and Setup) # https://www.youtube.com/watch?v=TDhTpPIjsDg\nmost of the features do not attract to me Will prefer organising the notes with tags not folder Spacemacs is better for editing. Happy with marco, fuzzy file searching, find and replace, vim keybinding and git Template function (like marco) Use to preview and visualise my notes but not editing Graph view support tags to virtualize the notes so no need to refactor my Hugo notes (Graph View \u0026gt; Filters \u0026gt; Tags) ","date":"2021-12-10","permalink":"/posts/obsidian/","section":"Posts","summary":"I found this markdown software when watching this video\nI am interested in that map and think if I can organise my markdown notes with it.\nTutorials\u003e Tutorials # How To Use VIM Bindings in Obsidian | Beginners Guide\u003e How To Use VIM Bindings in Obsidian | Beginners Guide # https://www.","title":"Obsidian"},{"content":"","date":"2021-12-01","permalink":"/tags/bluetooth/","section":"Tags","summary":"","title":"bluetooth"},{"content":" https://www.youtube.com/watch?v=1I1vxu5qIUM\nTransmit an electromagnetic wave (binary) with different wavelength\neg: 121mm = 1 124mm = 0\nthe waves send in all directions\nBluetooth range: 2.4835GHz - 2.4GHz\nDivided in different sections (79 channels), each section has a pair of wavelengths representing 0 and 1\nthe bluetooth range is shared with some other device like a microwave oven\nlike the power of a microwave oven is too large for bluetooth.\nit may destroy the bluetooth device if you put them inside the microwave oven (danger)\nData Integrity\u003e Data Integrity # Packet\u003e Packet # Access Codes (72 bits) Header (54bits) Payload (vary, depends on the function) Frequency Hopping\u003e Frequency Hopping # Two bluetooth devices will have a set of channels to send data\nThey will change the channels frequently\nError Detection\u003e Error Detection # Noise Filtering\u003e Noise Filtering # Data Transmit\u003e Data Transmit # Frequency Shift Keying\u003e Frequency Shift Keying # carrier wave adjusts the wavelength to send 0 and 1\nPhase Shift Keying\u003e Phase Shift Keying # constant wavelength and attitude\nuse the wave function to determine 0 and 1 (like sin x = 0, cos x = 1, instead of A sin x and B sin X)\n","date":"2021-12-01","permalink":"/posts/bluetooth/","section":"Posts","summary":"https://www.youtube.com/watch?v=1I1vxu5qIUM\nTransmit an electromagnetic wave (binary) with different wavelength\neg: 121mm = 1 124mm = 0\nthe waves send in all directions\nBluetooth range: 2.4835GHz - 2.4GHz\nDivided in different sections (79 channels), each section has a pair of wavelengths representing 0 and 1","title":"Bluetooth"},{"content":"Upgradable contract\u003e Upgradable contract # https://www.youtube.com/watch?v=kWUDTZhxKZI\u0026list=WL\u0026index=19\nproxy pattern\u003e proxy pattern # user interact with the proxy contract\nthe proxy will point to another smart contract\nWhen upgrading the contract, the admin of the proxy contract points the proxy to another smart contract storage\nImplementation\u003e Implementation # Transparent UUPS ","date":"2021-12-01","permalink":"/posts/openzeppelin/","section":"Posts","summary":"Upgradable contract\u003e Upgradable contract # https://www.youtube.com/watch?v=kWUDTZhxKZI\u0026list=WL\u0026index=19\nproxy pattern\u003e proxy pattern # user interact with the proxy contract\nthe proxy will point to another smart contract\nWhen upgrading the contract, the admin of the proxy contract points the proxy to another smart contract storage","title":"OpenZeppelin"},{"content":"","date":"2021-12-01","permalink":"/tags/smart-contract/","section":"Tags","summary":"","title":"smart-contract"},{"content":"","date":"2021-12-01","permalink":"/tags/study/","section":"Tags","summary":"","title":"study"},{"content":"Zero to Full-Time Programmer in 5 Steps\u003e Zero to Full-Time Programmer in 5 Steps # Zero to Full-Time Programmer in 5 Steps\nstart from small consistently study every day The scoreboard that is changing my life\u003e The scoreboard that is changing my life # The scoreboard that is changing my life\nuse scoreboard to track your goal\ngoal vs actual\nWeekly review (like a logbook)\nlagging metric (review past performance)\nleading metric (measure future performance)\nHow I consistently study with a full time job\u003e How I consistently study with a full time job # https://www.youtube.com/watch?v=INymz5VwLmk\nthe key is consistency, motivation doesn\u0026rsquo;t last long (always show up, always try) keep tracking the process ME\u003e ME # small step every day never being late, start today If you feel hesitating, then just start it do not need to make things perfect or well-prepared to start on something Archive Goal\u003e Archive Goal # https://www.youtube.com/watch?v=gHSh1O1HUqA\nFind the way out instead of focusing on the obstacle About the dream job\u003e About the dream job # https://www.youtube.com/watch?v=mtgt1ElOo0U\nno dream job but can make your own dream career 【上篇】背單字絕不要○○！會20種語言的達人分享如何儘早擺脫初級階段！Feat. Steve Kaufmann先生 【外文學習法】 日本人Haru老師【#51】日語發音，全程中文字幕\u003e 【上篇】背單字絕不要○○！會20種語言的達人分享如何儘早擺脫初級階段！Feat. Steve Kaufmann先生 【外文學習法】 日本人Haru老師【#51】日語發音，全程中文字幕 # https://www.youtube.com/watch?v=aBjZHomO2N0\naccept the fact that you will forget what you learn Stanford Engineering Hero Lecture: Morris Chang in conversation with President John L. Hennessy\u003e Stanford Engineering Hero Lecture: Morris Chang in conversation with President John L. Hennessy # https://www.youtube.com/watch?v=wEh3ZgbvBrE\nlearn and think if you don\u0026rsquo;t learn, you have nothing to think 《領航者Visionaries》張忠謀 Morris Chang (II)：我不以輝煌作為人生目標\u003e 《領航者Visionaries》張忠謀 Morris Chang (II)：我不以輝煌作為人生目標 # https://www.youtube.com/watch?v=AYUBSr3Wons\nLifelong Learning systematically ","date":"2021-12-01","permalink":"/posts/idea-about-study/","section":"Posts","summary":"Zero to Full-Time Programmer in 5 Steps\u003e Zero to Full-Time Programmer in 5 Steps # Zero to Full-Time Programmer in 5 Steps\nstart from small consistently study every day The scoreboard that is changing my life\u003e The scoreboard that is changing my life # The scoreboard that is changing my life","title":"Study Tips"},{"content":" https://www.youtube.com/watch?v=j_j2MiAxUvY\u0026list=PL0MZ85B_96CEmmy0C6NF52ZCMNcY1Wryf\nMethods\u003e Methods # Wait until the transaction is completely settled\u003e Wait until the transaction is completely settled # not practical and safe\nHashlock\u003e Hashlock # The transaction is partially complete\nSettle once the sender publishes the key on the blockchain.\nCons\u003e Cons # hugo cannot refund if kevin does not response Timelock\u003e Timelock # hugo made a partial transaction that the btc can be claimed by kevin within n number of blocks if the transaction is expired, hugo can claim back the fund Cons\u003e Cons # the claims need to execute manually HTLC (Hash Time Lock Contract)\u003e HTLC (Hash Time Lock Contract) # Combine two locks: (and)\nclaim within n number of blocks the key is revealed on the blockchain Steps in a cross chain swap\u003e Steps in a cross chain swap # Timestamp: 55:36\nExample: btc -\u0026gt; eth swap from hugo to kevin\nassume that Hugo has a key (secret that only Hugo will know) 1234 with hash 4567\nHugo and Kevin will not make transactions directly\nThey will send the fund to the contract instead\nTHe contract will hold the eth and btc for Hugo and Kevin\nThere are 2 separate contracts\none is in btc (let\u0026rsquo;s say Contract A)\nthe other is in eth (let\u0026rsquo;s say Contract B)\nhugo sent btc to the Contract A in btc network\nTransaction:\nField Value From hugo Value 10btc To kevin Hash 4567 Kevin sent eth to the Contract B in eth network Field Value From kevin Value 134eth To hugo hash 9876 Hugo observed that kevin has funded eth to Contract B. then he will submit his key 1234 in btc network, btc send to kevin by the Contract A same as kevin, he submits his key 9876 in eth network, eth sent to hugo by the Contract B Problem\u003e Problem # semi-trusted we assume Hugo and Kevin can see each other on each chain if the access right is changed during the transaction\u0026hellip;let say hugo can revoke kevin on eth network..then the transaction fails (this happens on hyperledger as it can have access control but it is not possible on btc and eth) ","date":"2021-11-28","permalink":"/posts/cross-chain-swap/","section":"Posts","summary":"https://www.youtube.com/watch?v=j_j2MiAxUvY\u0026list=PL0MZ85B_96CEmmy0C6NF52ZCMNcY1Wryf\nMethods\u003e Methods # Wait until the transaction is completely settled\u003e Wait until the transaction is completely settled # not practical and safe\nHashlock\u003e Hashlock # The transaction is partially complete","title":"Atomic cross chain swap between Hyperledger Fabric and Ethereum"},{"content":"","date":"2021-11-28","permalink":"/tags/crosschain/","section":"Tags","summary":"","title":"crosschain"},{"content":"","date":"2021-11-28","permalink":"/tags/hyperledger/","section":"Tags","summary":"","title":"hyperledger"},{"content":"Building BLOCKCHAIN Apps With HYPERLEDGER COMPOSER\u003e Building BLOCKCHAIN Apps With HYPERLEDGER COMPOSER # https://www.youtube.com/watch?v=gAxK6zYrfxI\na framework of hyperledger\ntesting - mocha / chai webapp generator - Yeoman api generation - Rest Server + loopback api doc generation - swagger auth (rest api) - passport integration with IOT - red-node ","date":"2021-11-28","permalink":"/posts/hyperledger-composer/","section":"Posts","summary":"Building BLOCKCHAIN Apps With HYPERLEDGER COMPOSER\u003e Building BLOCKCHAIN Apps With HYPERLEDGER COMPOSER # https://www.youtube.com/watch?v=gAxK6zYrfxI\na framework of hyperledger\ntesting - mocha / chai webapp generator - Yeoman api generation - Rest Server + loopback api doc generation - swagger auth (rest api) - passport integration with IOT - red-node ","title":"Hyperledger composer"},{"content":"","date":"2021-11-28","permalink":"/tags/iot/","section":"Tags","summary":"","title":"iot"},{"content":" Official Website\nPlaylist\na visual editor to build workflow and ui application to interact with IoT\nwire the IoTs and user (with web dashboard) read data from IoT write data to IoT (control it, on and off etc) build a flow among IoTs and users for automation build dashboard UI all can be done by drag and drop\ncan be used with blockchain in the supply chain\u0026hellip;\nnode red pulls the data from sensors and submits to blockchain automatically\nUse cases\u003e Use cases # Temperature monitor dashboard\u003e Temperature monitor dashboard # listen to any data sent from the thermometer show the temperature on the dashboard Light switcher\u003e Light switcher # send switch on and off signal to the light bulb through dashboard Automation (like IFFF and Apple Shortcut but in the IoT version)\u003e Automation (like IFFF and Apple Shortcut but in the IoT version) # monitoring the environment with sensors (like humidity and temperature) trigger other IoT (like turn on the washing machine) Water Utility (SCADA)\u003e Water Utility (SCADA) # https://www.youtube.com/watch?v=FCfmWnxQkoc\ntanks and bump bump water from the well to task 1 and then bump to tank 2 alert when the tank reaches the critical level MQTT\u003e MQTT # lightweight network protocol for IoT\nHow to Get Started with MQTT What is an MQTT Broker Clearly Explained Basic Concept\u003e Basic Concept # IoTs subscribe to specific topic workshop/switch in MQTT Broker user publish workshop/switch to MQTT broker (via red node) IoTs receive the data from the MQTT broker and react to it (eg: turn on the switch\u0026hellip;) MQTT Broker\u003e MQTT Broker # Open source MQTT broker: Eclipse Mosquitto\nbidirectional protocol\u003e bidirectional protocol # subscribe and publish at the same time in the same IoTs\nTLS\u003e TLS # Basic authentication\u003e Basic authentication # CERT\u003e CERT # failover / backup\u003e failover / backup # retained message\u003e retained message # When adding a new device, the value is empty as no subscribed topic is sending data. this feature will fill it with the last record\npersistent session\u003e persistent session # when IoT disconnected to the broker\nno need to resubscribe the topic automatically connect back to the broker with network connection back Client status\u003e Client status # IoT sends birth death discount message when connected to broker\nWhen the connection is established, the broker sends a birth message to a set of topic\u0026rsquo;s subscribers There is a keep-alive timer at the broker. Broker will ping the device periodically to ensure the device is alive If the keep -alive timer is timeout, the broker will send a death message to the topic\u0026rsquo;s subscribers Also, red-node can ping the IoTs to check their status. publish the status of IoT topic when it is connected or disconnected or unexpected disconnected\nMQTT.fx = software to publish / subscribe to the broker\nwith PI\u003e with PI # https://www.youtube.com/watch?v=WxUTYzxIDns\nmentions MQTT broker but does not mention how to setup node-red is installed on the pi itself, and sends a message to the pin directly to control the light bulb video does not tell\u003e video does not tell # how to setup the MQTT (send data to a specific topic to the broker) how to receive data from the device without MQTT (he just shows how to control the light bulb) MQTT client setup\u003e MQTT client setup # PI\u003e PI # https://www.youtube.com/watch?v=WxUTYzxIDns\ninstall MQTT client connect to the MQTT server publish all events to the MQTT service with a specific topic ( need to write code) Device with preinstalled software\u003e Device with preinstalled software # https://www.youtube.com/watch?v=KqHVkUiPRzc\nThis is not using node-red but the concept is same\njust enable MQTT function How to capture MQTT data from device at node-red\u003e How to capture MQTT data from device at node-red # listen to all topics from MQTT and capture it to debug node trigger the event to send out (eg: clicking the wifi button) / wait for the device to publish a message to the topic (thermometer) find out the payload and topic name Enterpise Case study\u003e Enterpise Case study # How to Build Massive IoT with Klika Tech, Wirepas and AWS Building an End-to-End Industrial IoT (IIoT) Solution with AWS IoT - AWS Online Tech Talks Use case\u003e Use case # tracking the temperature, humidity, vibration in shipping pro-logistics: open the container door, turn on and turn off the light.. control the lighting track which the conference room is used locate things in office / warehouse Flow\u003e Flow # IoTs\ngateways\nMQTT Broker (aggregate all data)\nuser application wirepas network tools backend wirepas FW (RuuviTag)\nwirepas snap\n(AWS) wirepas service (WNT, WPE) (Dashboard) / Greengrass snap (Like Red-Node)\nAWS IoT SiteWise \u0026amp; AWS IoT Greengrass\u003e AWS IoT SiteWise \u0026amp; AWS IoT Greengrass # (Depends on the device manufactory)\nOnLogic Moxa greengrass\naccess point for the IoTs to send data to AWS\nAWS IoT Message Broker\u003e AWS IoT Message Broker # AWS version of MQTT\nAWS IoT Device Management\u003e AWS IoT Device Management # IoT version of System Manager\nbulk registration Patching Monitor the health / Search Integration with analytics tools Wireless mesh network with asset tracking on site\u003e Wireless mesh network with asset tracking on site # building gateways on site to let the IoTs can send data to cloud (SiteWise) the message can be repeated (extended) once they cover each other (that is why it is called a mesh network) 700000 devices in one mesh network cover Oslo ROI -\u0026gt; look at the maintanese and analytics and delivery RuuviTag\u003e RuuviTag # Pressure\nTemperature\nHumidity\nAccelerometer\nLocation\nI2C / SPI\nSteps\u003e Steps # Build SiteWise Assets / Model Build SiteWise Gateway (connect with KEPServerEX) Portal -\u0026gt; SiteWise Montior -\u0026gt; Create Projects IoT Core set a rule (How to process the data) to send data to analytics in analaytics there is a channel created -\u0026gt; use pipeline to pass the raw data to lamda and process the raw data -\u0026gt; store the processed data to data store -\u0026gt; create (SQL / Container custom code) in data sets and consumer in IoT Events (Detector model) ","date":"2021-11-28","permalink":"/posts/node-red/","section":"Posts","summary":"Official Website\nPlaylist\na visual editor to build workflow and ui application to interact with IoT\nwire the IoTs and user (with web dashboard) read data from IoT write data to IoT (control it, on and off etc) build a flow among IoTs and users for automation build dashboard UI all can be done by drag and drop","title":"Node Red"},{"content":"","date":"2021-11-28","permalink":"/tags/node-red/","section":"Tags","summary":"","title":"node-red"},{"content":"Day1\u003e Day1 # Setup\u003e Setup # https://www.eksworkshop.com/020_prerequisites/ec2instance https://www.eksworkshop.com/030_eksctl/launcheks https://www.eksworkshop.com/beginner/060_helm/helm_intro/install Tasks\u003e Tasks # Deploy simple application Fargate Notes\u003e Notes # There are 2 modes in EKS:\nEC2 Fargate They can coexist in EKS.\nAlso we can only fargate only. When we deploy plugins to the cluster, It will be deployed to the control panel. (hidden from user)\nHow to use fargate in EKS:\u003e How to use fargate in EKS: # create a fargate profile which is associated with a namespace deploy yml to that namespace Pros and Con of fargate\u003e Pros and Con of fargate # can be scaled up quickly\u003e can be scaled up quickly # it is a micro vm as similar as pod one fargate instance per pod lower cost of fargate\u003e lower cost of fargate # Fargate will be cheaper in some situations. no need to rent large ec2 instance. it is good if you are doing batch jobs with EKS no need to maintain the ec2 instance (patch it, upgrade the kubelet) not well integrate with other AWS services\u003e not well integrate with other AWS services # like efs?\u0026hellip;need to further study on it.\nDay3\u003e Day3 # Tasks\u003e Tasks # https://www.eksworkshop.com/intermediate/245_x-ray/microservices https://www.eksworkshop.com/intermediate/246_monitoring_amp_amg https://www.eksworkshop.com/intermediate/250_cloudwatch_container_insights https://www.eksworkshop.com/beginner/170_statefulset https://www.eksworkshop.com/beginner/190_efs Alternative of argo\u003e Alternative of argo # https://www.eksworkshop.com/intermediate/260_weave_flux https://www.eksworkshop.com/intermediate/220_codepipeline ","date":"2021-11-27","permalink":"/posts/aws-workshop/","section":"Posts","summary":"Day1\u003e Day1 # Setup\u003e Setup # https://www.eksworkshop.com/020_prerequisites/ec2instance https://www.eksworkshop.com/030_eksctl/launcheks https://www.eksworkshop.com/beginner/060_helm/helm_intro/install Tasks\u003e Tasks # Deploy simple application Fargate Notes\u003e Notes # There are 2 modes in EKS:\nEC2 Fargate They can coexist in EKS.","title":"AWS EKS Workshop"},{"content":"Pros with AWS Managed Blockchain\u003e Pros with AWS Managed Blockchain # no need to build the ledger by themself focus on the smart contract a single blockchain network can effectively work cross multiple parties An Overview on Blockchain Services from AWS\u003e An Overview on Blockchain Services from AWS # https://www.youtube.com/watch?v=WAIOBeQA2QQ\nledger database (immutable transaction) consenus decentlization smart contract ease for audit Hyperledger Fabric VS eth\u003e Hyperledger Fabric VS eth # authentication access control private ledger ","date":"2021-11-27","permalink":"/posts/aws-managed-blockchain/","section":"Posts","summary":"Pros with AWS Managed Blockchain\u003e Pros with AWS Managed Blockchain # no need to build the ledger by themself focus on the smart contract a single blockchain network can effectively work cross multiple parties An Overview on Blockchain Services from AWS\u003e An Overview on Blockchain Services from AWS # https://www.","title":"AWS Managed Blockchain"},{"content":"","date":"2021-11-27","permalink":"/tags/eks/","section":"Tags","summary":"","title":"eks"},{"content":"Control Panel Fee\u003e Control Panel Fee # AKS: 0 EKS: fixed charge 0.1 USD / Hour Integration\u003e Integration # I think AKS is better, most of the provisioning plugins can be enabled by one click\nIn aws, you need to\ninstall plugins setup iam service account for the plugins to access AWS service application routing\u003e application routing # For AKS,\nit is just an checkbox option For EKS, you need to\nsetup IAM OICD setup external dns setup ALB contrller auto scaler\u003e auto scaler # Same as application routing -\u0026gt; auto scaler. enable and use\nin AWS, we need to setup cluster autoscaler\nvolume provisioning\u003e volume provisioning # AKS supports volume provisioning with Azure File (EFS) \u0026amp; Azure Disk (EBS), it has many pre-defined volume class\u0026hellip;we do not need to install any plugin\nFor EKS, we need CSI driver\nebs efs Virtual node\u003e Virtual node # It is AKS things, like EKS\u0026rsquo;s fargate\u0026hellip;but I cannot find the price..\n2021-12-10 updates:\u003e 2021-12-10 updates: # I have asked on Microsoft Q\u0026amp;A: https://docs.microsoft.com/en-us/answers/questions/659456/price-of-ake-virtual-node-and-aks-without-node-poo.html\nneed min 1 system node group can start / stop the cluster the VM will stop charging when the cluster is stop charge of other provisioned services like LB and Azure File are unknown\u0026hellip; virtual node will charge as container service (like fargate) ","date":"2021-11-27","permalink":"/posts/eks-vs-aks/","section":"Posts","summary":"Control Panel Fee\u003e Control Panel Fee # AKS: 0 EKS: fixed charge 0.1 USD / Hour Integration\u003e Integration # I think AKS is better, most of the provisioning plugins can be enabled by one click","title":"EKS vs AKS"},{"content":"","date":"2021-11-27","permalink":"/tags/hk/","section":"Tags","summary":"","title":"hk"},{"content":"","date":"2021-11-27","permalink":"/tags/movie/","section":"Tags","summary":"","title":"movie"},{"content":" https://youtu.be/TgsTCzLRQcs?t=1905\nMay You Stay Forever Young\u003e May You Stay Forever Young # Taipei golden horse film festival 2021\nTrailer\ntwo girls arrested in 721 one sexuel abused by HK police force and try to commit suicide another young teanager insists on looking for her. She is arrested as she turns back to him in a confrontation with riot police Revolution of Our Times\u003e Revolution of Our Times # IMDB\nTrailer\nConnection with mashipo\u003e Connection with mashipo # How an elderly Hong Kong pro-democracy protester also fought in vain to save his village from developers\nWong understands people because he has confronted the government before in Mashipo young people help them to protect his home in mashipo\u0026hellip;so he thinks he has responsibility to protect them back The FA , who praises the police force, let him enter the Prince Edward station at 831\u003e The FA , who praises the police force, let him enter the Prince Edward station at 831 # still a high school student have dramatic changes after 831 (become negative\u0026hellip;) interview with his teacher and classmate Protector stayed in Legislative Council\u003e Protector stayed in Legislative Council # a girl who said 齊上齊落(all together not one less), turn back bring people out from the council ppl should participate more\u0026hellip;not just support them\u0026hellip; HK is not important; HKER is more important TW use 40 years to have democracy - need patient Repressed anger as it does not help Interview with people who is already in prison or left HK\u003e Interview with people who is already in prison or left HK # Ho Kwai-lam Aerial photography - protest confront with riot police at MongKok\u003e Aerial photography - protest confront with riot police at MongKok # what is “be water” Drifting Petals\u003e Drifting Petals # Taipei golden horse film festival 2021\nTrailer\n","date":"2021-11-27","permalink":"/posts/movies-about-hk/","section":"Posts","summary":"https://youtu.be/TgsTCzLRQcs?t=1905\nMay You Stay Forever Young\u003e May You Stay Forever Young # Taipei golden horse film festival 2021\nTrailer\ntwo girls arrested in 721 one sexuel abused by HK police force and try to commit suicide another young teanager insists on looking for her.","title":"Movies about Hong Kong"},{"content":"","date":"2021-11-27","permalink":"/tags/workshop/","section":"Tags","summary":"","title":"workshop"},{"content":"","date":"2021-11-26","permalink":"/tags/auth/","section":"Tags","summary":"","title":"auth"},{"content":"Resources\u003e Resources # Exam Landing Page Sample Questions Exam Guide udemy course udemy mock exam whizlabs test \u0026amp; hand-on labs TODO\u003e TODO # take notes revision whizlabs mocks (by topic) mock 1 (udemy) mock 2,3,4 (whizlabs) examtopic udemy quiz + exercise Revision\u003e Revision # The numbers below are the page no of the udemy course\u0026rsquo;s pdf: AWS Certified Networking Specialty Slides v1.1.\nSummary\u003e Summary # DNS (151) Advanced Networking (188) VPC Endpoint (292, 298) Site-to-site VPN (347, 401, 407) VPN Tunnels and Routing (348, 352) DX Gateway with VGW (532) DX with TGW (547) DX Billing (624) DX (630) Troubleshooting in DX (626) Gateway Load Balancer (816) Q \u0026amp; A\u003e Q \u0026amp; A # ALB (677) DX (625) Good to know\u003e Good to know # SG (46) BYOIP (90) VPC Traffic Mirroring (105) DHCP Option Sets (124) TGW (251) AWS Site-to-Site VPN (403) Network Load Balancer (656) Comparisons\u003e Comparisons # private, public and EIP (33) IPv4, IPv6 (36) NACL, SG (48) NAT Gateway, Instance (64) VPC Peering vs Transit Gateway Cloudfront function vs lambda@edge (703) AWS global accelerator vs Cloudfront (711) Exam Essential\u003e Exam Essential # VPC Fundamentals (67) Advanced VPC (190) VPC Peering Endpoint (301) AWs Site-to-Site VPN (402) Direct Connect (632) Firewall (806) Gateway Load Balancer (817) Revision by topic\u003e Revision by topic # Playlists\u003e Playlists # global accelerator api gateway client vpn cloudformation cloudfront cloudwatch cloudhsm cognito config dx edge computing elb firehose guarduty inspector lambda organization route53 s3 sqs ssm vpc vpn waf workspace References\u003e References # accelerator - custom routing accelerator - custom routing alb - limit request to clondfront only aurora - regional failover aws organization - SCP cloudfront - origin types cloudfront - signed url vs signed cookies cloudfront - troubleshooting cloudfront - will forward the response when first byte arrives from the origin cloudhsm - auto ha cloudtrail - 5 trails for region cloudtrail - log encryption with KMS cloudtrail - use integrity validation to check the log was modified, delete or change cloudwatch - namespace valid characters cloudwatch - no need to config sns to provide data every minute cloudwatch - skip metrics config - managed rules config - renaming the delivery channel config - requirements config - type of triggers config - viewing configuration compliance costs model costs model 2 dhcp - cannot modify and one vpc one dhcp options dx - dx in public regions can access any other public region dx - faq dx - lag requirement 1 dx - lag requirement 2 dx - public vif dx - public vif connectivity dx - quotas dx - requirements for virtual interfaces dx - vif dx - virtual Interfaces to Direct Connect connections or LAG bundles dx - vpn as backup ec2 - http proxy ec2 - network performance ec2 - retrieve instance metadata eip - hostname will be changed once eip attached eip - public ip change every time stop and start the instance eip - public ip will be released once eip attached eip - reverse dns record for mailserver eni - limitation flow log local zone - alb limitation local zone - supported service ms ad - limitation - not compatible with exchange and skype ms ad - requirements nacl - default rule# in nacl nat - need egress-only-internet-gateway for ipv6 to ipv6 commnuication nat - only support tcp udp and icmp nat - pricing nat - tcp connectin fail as tcp does not support ip fragmentation placement group - can add / move / remove instance from group placement group - can launch with diff instance type but not recommended placement group - can span vpcs placement group - cannot merge placement group placement group - stop and start is fine but may have insufficient capacity error without capacity reservation quicksight - private connection with rds redishift - private connectivity with enhanced vpc routing route53 - a/p failover route53 - aws services which support alias records route53 - can attach private hosted zone with overlapping namespace in same vpc route53 - dns resolution b/w on-premise and aws with ad route53 - dns server with custom domain (white-labe and reusable delegation set) route53 - health check rules route53 - system rules when forwarding less specific domain simple ad - requirement sns tgw - peering tgw - quotas tgw - route table priority vpc - aws cidr tier vpc - aws does not support broadcast in vpc vpc - ipv4 subnet cidr prefix vpc - ipv6 subnet cidr prefix vpc - jumbo frame packet drop as Don't Fragment flag is set but the network does not support higher MTU vpc - multicast support vpc - reserved address in cidr vpc - route table qutoa vpc - route table troubleshoot vpc - usage of enableDnsHostnames and enableDnsSupport vpce - service endpoint support ipv4 over tcp only vpce - tagging is suported vpce - use prefix list in security group vpn - ipv6 support on tgw but not vgw vpn - static route a/p mode (priority) vpn - troubleshooting vpn - why cannot overlap the cidr vpn -ipsec encryption algorithms waf - config count action to test (monitor mode) waf - rule statement list workspaces - requirements appliance in shared vpc cloudhub mult vpc with single customer gateway multi-vpc network infrastructure route table options shared vpc transit vpc Virtual Private Cloud Connectivity Options vpc peering with cidr overlap vpc with subnet overlapping VPC Fundamentals (17)\u003e VPC Fundamentals (17) # What is TCP/IP?\u003e What is TCP/IP? # Answer application (data) transport (segment) network (packet) link (frame) physical (frame) What is OSI? (306)\u003e What is OSI? (306) # Answer application presentation session transport network link physical ##### How to calculate the no of address in a CIDR (like 192.168.0.1/28)? (18) Answer 2^(32-28)=16 Can we override the main route table in VPC? (26)\u003e Can we override the main route table in VPC? (26) # Answer override it at subnet level Which IPs are reserved for a vpc? (29)\u003e Which IPs are reserved for a vpc? (29) # Answer 5\n0=network 1=router 2=dns 3=reserved last=broadcast What is the default behaviour of SG (inbound and outbound)? (46)\u003e What is the default behaviour of SG (inbound and outbound)? (46) # Answer inbound: block all outbound: allow all How is the NAT gateway charged? (59)\u003e How is the NAT gateway charged? (59) # Answer hourly charge data transfer fee Can we apply SG on a NAT gateway? (59)\u003e Can we apply SG on a NAT gateway? (59) # Answer no What is the port range NAT gateway needs for outbound connection? (59)\u003e What is the port range NAT gateway needs for outbound connection? (59) # Answer 1024-65535 Why don\u0026rsquo;t we need cross-AZ failover in NAT ? (62)\u003e Why don\u0026rsquo;t we need cross-AZ failover in NAT ? (62) # Answer because if az down, nat and app are down too What is the most important setting to setup NAT with EC2? (63)\u003e What is the most important setting to setup NAT with EC2? (63) # Answer disable the source and destination check Advanced VPC (72)\u003e Advanced VPC (72) # What is the limitation of adding a secondary CIDR? (79)\u003e What is the limitation of adding a secondary CIDR? (79) # Answer aws defines cidr in 3 classes. 172 169 10192.168 172.16 10.0 if the primary cidr is in one of the classes, the secondary cidr must be in the same class or not in any class with an equal / more specific prefix if the primary cidr is not in one of the class, the secondary cidr must not in any class with equal / more specific prefix 5ipv4 1ipv6 How many private, public and elastic ip and sg can one ENI have? (81, 85)\u003e How many private, public and elastic ip and sg can one ENI have? (81, 85) # Answer private - depends on the instance type public - 1 elastic - 11 eip per private ip What is the use case of dual home setup with multiple ENIs? (84)\u003e What is the use case of dual home setup with multiple ENIs? (84) # Answer 1 eni for internal traffic 1 eni for internet traffic What are the prerequisites of BYOIP? (89, 90)\u003e What are the prerequisites of BYOIP? (89, 90) # Answer you should own that ip ip should have a good record each account can bring 5 ips (ipv4 \u0026 ipv6) ROA ipv4: /24; ipv6: /48 pub; /56 pri What type of Flow Logs can we capture from a VPC? (92)\u003e What type of Flow Logs can we capture from a VPC? (92) # Answer vpc subnet eni Where does the Flow Logs Action field come from? (97)\u003e Where does the Flow Logs Action field come from? (97) # Answer security group / nacl What type of record cannot be captured in a flow log? (98)\u003e What type of record cannot be captured in a flow log? (98) # Answer traffic from aws dns / metadata dhcp, windows licence activation server What can be the source \u0026amp; target of a VPC Traffic Mirror? (102, 105)\u003e What can be the source \u0026amp; target of a VPC Traffic Mirror? (102, 105) # Answer source: eni target: eni, nlb What port does the VPC Traffic Mirror require? (105)\u003e What port does the VPC Traffic Mirror require? (105) # Answer udp 4789 How to set a custom domain / dns on an EC2 instance? (113)\u003e How to set a custom domain / dns on an EC2 instance? (113) # Answer change the dhcp-option VPC DNS and DHCP\u003e VPC DNS and DHCP # What does enableDNSSupport do? (117)\u003e What does enableDNSSupport do? (117) # Answer resolve the dns What does enableDNSHostname do? (117)\u003e What does enableDNSHostname do? (117) # Answer assign hostname to the ec2 instance How to resolve the hostname in VPC peering and TGW? (132)\u003e How to resolve the hostname in VPC peering and TGW? (132) # Answer enable dns support How to resolve the hostname in Hybrid cloud (AWS to on-premise, on-premise to AWS) (old \u0026amp; new way)? (132-150)\u003e How to resolve the hostname in Hybrid cloud (AWS to on-premise, on-premise to AWS) (old \u0026amp; new way)? (132-150) # Answer old:\naws -\u003e on-premise: setup ad / dns server on aws; set dhcp to that server; forward query to that server in on-premise dns on-premise -\u003e aws: setup ad / dns server on aws; use that server in on-premise new: aws -\u003e on-premise: setup outbound endpoint to on-premise dns, set forward rule to that endpoint on-premise -\u003e aws: setup inbound endpoint, forward query to that endpoint in on-premise dns VPC Network Performance and Optimization\u003e VPC Network Performance and Optimization # Formula of throughput (154)\u003e Formula of throughput (154) # What is Jumbo frame? (154)\u003e What is Jumbo frame? (154) # Answer MTU \u003e 1500 (9001 / 8500) What is Path MTU Discovery? (155)\u003e What is Path MTU Discovery? (155) # Answer check the max MTU support between 2 routes What are the advantages of using placement group - cluster (162)\u003e What are the advantages of using placement group - cluster (162) # Answer low latency higher bandwidth What is the bandwidth limitation between VPC, EC2 instances, VPN and DX (179-183)?\u003e What is the bandwidth limitation between VPC, EC2 instances, VPN and DX (179-183)? # Answer vpc = 100% to igw / other region = 50% ec2 = 5gbps 10gbps (placement group) single flow vpn (tgw) 1.25gbps per tunnel and sum up to 25gbps vpn (vgw) 1.25gbps dx (vgw/tgw) = 1/10/100gbps VPC Peering\u003e VPC Peering # What is the limitation of VPC Peering? (207)\u003e What is the limitation of VPC Peering? (207) # Answer non-transitive cannot access igw, nat, vpn, peer, gateway endpoint TGW\u003e TGW # What is the difference between vgw and tgw? (211)\u003e What is the difference between vgw and tgw? (211) # Answer vgw is non-transitive cannot access igw, nat, vpn, peer, gateway endpoint what attachments does TGW support? (211)\u003e what attachments does TGW support? (211) # Answer vpn vpc tgw dxgw What is the special route in TGW? (230)\u003e What is the special route in TGW? (230) # Answer custom route table for each tgw attachment VPC Endpoint\u003e VPC Endpoint # How many types of VPC endpoint? What is the difference between them? (258)\u003e How many types of VPC endpoint? What is the difference between them? (258) # Answer interface = has eni attached gateway = use route table private link = attach to alb or nlb all has vpc endpoint policy Can on-premises network access to the gateway endpoint with VPN/DX? (273)\u003e Can on-premises network access to the gateway endpoint with VPN/DX? (273) # Answer no What kind of source can be used in a private link? (281)\u003e What kind of source can be used in a private link? (281) # Answer nlb and alb How to use hostname in interface endpoint? (288)\u003e How to use hostname in interface endpoint? (288) # Answer enable private dns How to use the interface endpoint without private DNS? (290)\u003e How to use the interface endpoint without private DNS? (290) # Answer use the dns name with \"region\" When should we use a private link? When should we use VPC Peering (297)\u003e When should we use a private link? When should we use VPC Peering (297) # Answer Many clients need to access the service from different accounts (\u003e125) use peering when there are many services that need to be accessed AWS Site-to-Site VPN\u003e AWS Site-to-Site VPN # What port does IPsec need? (309)\u003e What port does IPsec need? (309) # Answer udp/500 Range of public and private ASN (313)\u003e Range of public and private ASN (313) # Answer 0-65525 64512-65534 4 common BGP Param for routing (319)\u003e 4 common BGP Param for routing (319) # Answer weight as_path med local-preference What type of VPN does AWS support Site-to-Site VPN? (327)\u003e What type of VPN does AWS support Site-to-Site VPN? (327) # Answer ipsec What routing method does AWS support in Site-to-Site VPN? (329)\u003e What routing method does AWS support in Site-to-Site VPN? (329) # Answer bgp / static What port is required in VPN for NAT-T? (335)\u003e What port is required in VPN for NAT-T? (335) # Answer udp/4500 VPN Tunnels and Routing\u003e VPN Tunnels and Routing # How to set Active/Active tunnel (351)\u003e How to set Active/Active tunnel (351) # Answer advertise the same prefix. use BGP and ECMP for load balancing \u003c- this will cause the traffic to go randomly advertise diff prefix. more specific prefixes will be prefered or use AS_PATH or MED to control the traffic from aws What is DPD? Port to send messages? default timeout? timeout action? (354)\u003e What is DPD? Port to send messages? default timeout? timeout action? (354) # Answer dead peer detection. detect the dead peer tunnel. 30s. action: clear, restart, none How to prevent a tunnel from terminating due to inactivity (355)\u003e How to prevent a tunnel from terminating due to inactivity (355) # Answer ping the aws network each 5 seconds to prevent idle tunnel How to monitor the tunnel status? (357)\u003e How to monitor the tunnel status? (357) # Answer set alarm with cloudwatch metrics DX\u003e DX # What is the fibre mode of DX? (450)\u003e What is the fibre mode of DX? (450) # Answer single what is the data link requirement of DX (450)\u003e what is the data link requirement of DX (450) # Answer fibre 802.1vlan LX What are the requirements for a customer router? (450)\u003e What are the requirements for a customer router? (450) # Answer bgp What is the port of BGP? (457)\u003e What is the port of BGP? (457) # Answer 179/tcp What is BFD? How to enable it? (458)\u003e What is BFD? How to enable it? (458) # Answer auto failover dx is enabled by default. enable it on the customer router lower the failure detection time How long does it take for failover with and without BFD? (458, 597, 598)\u003e How long does it take for failover with and without BFD? (458, 597, 598) # Answer 300ms * 3times = 1s default 30s * 3 = 90s; DPD = method to detect the failure (for ipsec+bgp). BFD = method to lower the failure detection (for bgp) How can a user whitelist the ips range from AWS? (459)\u003e How can a user whitelist the ips range from AWS? (459) # Answer whitelist the ip in ip-range.json How many VIF can a hosted connection have? (465)\u003e How many VIF can a hosted connection have? (465) # Answer 1 What Ip type does DX support? (498)\u003e What Ip type does DX support? (498) # Answer ipv4 and ipv6 How many route prefixes can the public vif advertise? (503)\u003e How many route prefixes can the public vif advertise? (503) # Answer 1000 How many route prefixes can a private vif advertise? (507)\u003e How many route prefixes can a private vif advertise? (507) # Answer 100 What is the limitation of vif and vgw location? (507)\u003e What is the limitation of vif and vgw location? (507) # Answer in the same region What service cannot access inside a VPC with DX? (510)\u003e What service cannot access inside a VPC with DX? (510) # Answer nat, nat + igw, peered vpc, gateway endpoint ,dns What is the usage of DX gateway? (517)\u003e What is the usage of DX gateway? (517) # Answer connect dx and vpc in any regions (pri/transit vif) How many vif can a DX connection have? (526)\u003e How many vif can a DX connection have? (526) # Answer 50(pub + pri) How many transit vif can a DX connection have? (547)\u003e How many transit vif can a DX connection have? (547) # Answer 1(transit) How many DX gateways can a vif have? (530)\u003e How many DX gateways can a vif have? (530) # Answer 1 How many vif can a DX gateway connect to? (530)\u003e How many vif can a DX gateway connect to? (530) # Answer 30 How many vgw can a DX gateway have? (526,547)\u003e How many vgw can a DX gateway have? (526,547) # Answer 10 How many tgw can a Dx gateway have? (538)\u003e How many tgw can a Dx gateway have? (538) # Answer 3 What is the charge for using Dx gateway? (533)\u003e What is the charge for using Dx gateway? (533) # Answer no How many routes can be advised per TGW through the DX gateway? (539)\u003e How many routes can be advised per TGW through the DX gateway? (539) # Answer 100 (bgp) what is ECMP (251, 556) - only support BGP\u003e what is ECMP (251, 556) - only support BGP # Answer load balancing between different dx connection the same dx location how to setup Active-Active with public vif and public ASN (554, 558)\u003e how to setup Active-Active with public vif and public ASN (554, 558) # Answer advertise same prefix route use ECMP (by aws) how to setup Active-Active with private vif and public ASN (558)\u003e how to setup Active-Active with private vif and public ASN (558) # Answer no how to setup Active-Passive with public vif and public ASN (561)\u003e how to setup Active-Passive with public vif and public ASN (561) # Answer advertise same prefix route as_path / med for incoming traffic to aws local-pref for outgoing traffic to aws how to setup Active-Passive with public vif and public ASN (561)\u003e how to setup Active-Passive with public vif and public ASN (561) # Answer advertise more specific prefix route how to constrain the adviser routes to on-premises with public vif (567)\u003e how to constrain the adviser routes to on-premises with public vif (567) # Answer on-premises routes advertise the bgp communities to ask aws to advertise specific routes only (region / inter-region / global) how to filter the adviser routes from aws in on-premises with public vif (567)\u003e how to filter the adviser routes from aws in on-premises with public vif (567) # Answer based on the bgp communities from aws. it tells where the route comes from what is the routing policy with private vif (577, 582)\u003e what is the routing policy with private vif (577, 582) # Answer advise more specific route bgp: as_path / med or local preference (same dx location) bgp-communities What is LAG? (584)\u003e What is LAG? (584) # Answer aggregate the dx connection to a larger bandwidth How many DX connections can a LAG have? (584)\u003e How many DX connections can a LAG have? (584) # Answer 4 What is the bandwidth requirement of a LAG? (584)\u003e What is the bandwidth requirement of a LAG? (584) # Answer same bandwidth and on the same customer device What is the operation connection attribute in LAG? (589)\u003e What is the operation connection attribute in LAG? (589) # Answer no of connection that aws treating the LAG is up and running how to increase the resiliency of DX? (3 methods) (590)\u003e how to increase the resiliency of DX? (3 methods) (590) # Answer multi device (development) multi location (resiliency) multi device and multi dx location (max) one more -\u003e dx over vpn how to encrypt DX traffic? (2 methods) (601)\u003e how to encrypt DX traffic? (2 methods) (601) # Answer dx over vpn (ipsec) macsec When will the Dx connection be charged? (Hosted and Dedicated connection) (621)\u003e When will the Dx connection be charged? (Hosted and Dedicated connection) (621) # Answer dedicated: once aws create the connection for you (aws did their job, give you the LOA) hosted: once you accept the connection What will be charged in Dx? (615)\u003e What will be charged in Dx? (615) # Answer data out and port charge Who will be charged for the DTO fee? (622)\u003e Who will be charged for the DTO fee? (622) # Answer the resource owner to send out the traffic ELB\u003e ELB # What Layer ALB, CLB, NLB, GLB belong to? (639)\u003e What Layer ALB, CLB, NLB, GLB belong to? (639) # Answer alb 7 clb 7/4 nlb 4 glb 3 What protocol does ALB, CLB, NLB, GLB support? (641)\u003e What protocol does ALB, CLB, NLB, GLB support? (641) # Answer alb http https clb http https tcp tls nlb tcp udp tls glb ip Why does NLB have less latency than ALB? (652)\u003e Why does NLB have less latency than ALB? (652) # Answer do not need to read the packets (we only check the ip port and protocol) What kind of target can be used in the ALB and NLB target group? (653)\u003e What kind of target can be used in the ALB and NLB target group? (653) # Answer alb: ip ec2 lambda nlb: ip ec2 What kind of protocol is supported in the health check? (653)\u003e What kind of protocol is supported in the health check? (653) # Answer tcp What are the connection idle timeouts of ALB, NLB, CLB? (657)\u003e What are the connection idle timeouts of ALB, NLB, CLB? (657) # Answer alb 60 nlb 350for tcp 120 for udp cannot be configured clb 60 What routing algorithm supported by ALB, NLB, CLB (659)\u003e What routing algorithm supported by ALB, NLB, CLB (659) # Answer alb, clb: least outstanding, round-robin nbl: hash flow hash How to keep the client ip in ALB, NLB (671, 672)\u003e How to keep the client ip in ALB, NLB (671, 672) # Answer alb \u0026 clb: x-forwarded-For nlb: proxy protocol 2 clb: proxy protocol 1 How to keep the client on the same instance for a period of time? (661)\u003e How to keep the client on the same instance for a period of time? (661) # Answer sticky session What is Cross-Zone Load Balancing? (663)\u003e What is Cross-Zone Load Balancing? (663) # Answer by default enabled in alb, the traffic are load balanced to all instances evenly disabled in nlb disabled in clb (api), enabled in clb (console) What is SNI and which ELB does it support? (667)\u003e What is SNI and which ELB does it support? (667) # Answer sni = domain name in certmulti ssl certs in one web server alb and nlb support it What is Connection Draining (670)\u003e What is Connection Draining (670) # Answer remove opening connections from died instance (auto scaling group) Cloudfront\u003e Cloudfront # What services / source can be Cloudfront\u0026rsquo;s origin? (683)\u003e What services / source can be Cloudfront\u0026rsquo;s origin? (683) # Answer any public ip s3 media package and mediastore container Why public access is needed in cloudfront\u0026rsquo;s origin (683)\u003e Why public access is needed in cloudfront\u0026rsquo;s origin (683) # Answer Cloudfront does the health check from the internet What is the origin group (687)\u003e What is the origin group (687) # Answer primary \u0026 secondary of group of origin for failover one primary and one secondary origin for failover create a origin group, select primary \u0026 secondary origin, then select the group in cloudfront origin How to change custom header / behaviour in Cloudfront? (698)\u003e How to change custom header / behaviour in Cloudfront? (698) # Answer use lambda@edge or cloudfront actionfunction How to restrict the content to specific geolocation? (696)\u003e How to restrict the content to specific geolocation? (696) # Answer map the origin to geolocation the allow / block list choose either whitelist / blacklist and the countries for restriction What is AWs global accelerator? What does it work (709)\u003e What is AWs global accelerator? What does it work (709) # Answer use anycast(2ips). will send traffic to aws edge locations and then reach to your service through aws backbone network What is the difference between AWS global accelerator and cloudfront? (711)\u003e What is the difference between AWS global accelerator and cloudfront? (711) # Answer cloudfront only supports http / https. accelerator using a transport layer so it can be udp volip mqtt Route53\u003e Route53 # What is the longest and shortest TTL in route53 (721)\u003e What is the longest and shortest TTL in route53 (721) # Answer 60s 24hr What are the alias targets in route53 (724)\u003e What are the alias targets in route53 (724) # Answer s3 beanstalk vpce accelerator api gateway elb cloudfront other route53 record(same hosted zone) How to bind an RDS DB instance in route53 (748)\u003e How to bind an RDS DB instance in route53 (748) # Answer cname Routing Policies in Route53? (725)\u003e Routing Policies in Route53? (725) # Answer simple multivalue failover latency weighted geolocation geoproximty How does Route53 perform the health check in a private VPC? (732)\u003e How does Route53 perform the health check in a private VPC? (732) # Answer setup a health check with cloudwatch alert How to setup hybrid DNS (754)\u003e How to setup hybrid DNS (754) # Answer use route53 forwarderresolver. create outbound(forward on-premise domain query to on-premise dns) and inbound (for on-premise forwarderresolver to forward the query to aws vpc domain) endpoint How does AWS ensure the HA in route53? (807)\u003e How does AWS ensure the HA in route53? (807) # Answer random sharding anycast striping Network firewall\u003e Network firewall # What layer SG, NACL, network firewall, WAF, shield?\u003e What layer SG, NACL, network firewall, WAF, shield? # Answer sg = 7 3/4 nacl = 4 3/4 network firewall 7-4 7-3 waf = 7 shield = 3 What is the VPC level in SG and NACL? (766)\u003e What is the VPC level in SG and NACL? (766) # Answer sg = vpc instance nacl = subnet When should we use nacl instead of sg? (768, 769)\u003e When should we use nacl instead of sg? (768, 769) # Answer to block something (sg cannot block traffic) When should we use WAF instead of lacl ? (770)\u003e When should we use WAF instead of lacl ? (770) # Answer block many ips / handle ddos block ip for the application cloudfront / alb behind Stateless and stateful of sg, nacl, network firewall, shield? (773)\u003e Stateless and stateful of sg, nacl, network firewall, shield? (773) # Answer sg = stateful nacl = stateless network firewalld = both (stateless -\u003e stateful) shield = stateless How can a SYN cookie prevent DDos in packet flooding?\u003e How can a SYN cookie prevent DDos in packet flooding? # Gateway Load balancer\u003e Gateway Load balancer # what is the use case of gateway load balancer (813)\u003e what is the use case of gateway load balancer (813) # Answer applicane (ip ec2) what port does gateway load balancer need for GENEVE (817)\u003e what port does gateway load balancer need for GENEVE (817) # Answer GENEVE UDP 6081 Other\u003e Other # Centralised VPC Interface endpoint 295\u003e Centralised VPC Interface endpoint 295 # access s3 with endpoint\u003e access s3 with endpoint # use a private link to access a web server (NLB/EC2)\u003e use a private link to access a web server (NLB/EC2) # VPC peering connections (125)\u003e VPC peering connections (125) # VPC peering allows using Security group (301)\u003e VPC peering allows using Security group (301) # how many ip can BYOIP bring in (90)\u003e how many ip can BYOIP bring in (90) # How many VPC CIDRs can a VPC have? (72)\u003e How many VPC CIDRs can a VPC have? (72) # dns of vpc .2 169.254.169.253 (117)\u003e dns of vpc .2 169.254.169.253 (117) # Why is edge location safe? (711)\u003e Why is edge location safe? (711) # What must be enabled to use route53 in vpc?\u003e What must be enabled to use route53 in vpc? # Demo (266)\nHand-on Labs\u003e Hand-on Labs # https://www.whizlabs.com/learn/course/aws-advanced-networking-speciality/195\nEC2\u003e EC2 # 30 CloudFront\u003e CloudFront # 1 2 10 ALB\u003e ALB # 3 4 5 WAF\u003e WAF # 11 12 ACL\u003e ACL # 18 29 VPN\u003e VPN # 21 Endpoint\u003e Endpoint # 23 24 25 Flow Log\u003e Flow Log # 26 Container\u003e Container # 28 VPCs Connectivities\u003e VPCs Connectivities # Transit gateway peering attachments What is VPC peering? Transit Gateway vs VPC peering Solution / Situation same region cross account cross region VPC Peering OK OK OK TGW Peering OK OK OK TGW (attach VPC) OK OK NO AWS Managed VPN OK OK OK EC2 Based VPN OK OK OK Private Link is something like tgw peer but for specific service only. It can cross regions and accounts.\nScope\u003e Scope # Resource Scope VGW VPC TGW VPC NLB VPC ALB VPC Route53 VPC NAT Subnet Common Pattern\u003e Common Pattern # Routing (582) Site-to-Site Connection routing\u003e Site-to-Site Connection routing # Static - Active/ Active Tunnels (349) Static - Active/ Passive Tunnels (350) Dynamic - Active/ Active Tunnels (351) TGW\u003e TGW # Centralised NAT gateway (241) Centralised NAT instance (243) Centralised NAT instance + VPN (244) Centralised VPC interface endpoint + VPN (245) Hybrid VPN (247) Hybrid DX (248) VPC Peering\u003e VPC Peering # Failed cases\u003e Failed cases # Page Title 207 CIDR overlap or transitive routing 208 from DX or VPN 209 to IGW Site-to-site VPN / DX with VGW\u003e Site-to-site VPN / DX with VGW # Use cases\u003e Use cases # Page Title 335 Site-to-site Connection with NAT-Traversal 361 Single Site-to-site Connection with VGW 363 Multiple Site-to-site Connection with VGW 365 Redundant VPN connections for HA Failed cases\u003e Failed cases # Page Title 340 Site-to-site VPN to IGW 341 Site-to-site VPN to NAT 343 Site-to-site VPN to VPC Peering 344 Site-to-site VPN to VPC Gateway endpoint Successful cases\u003e Successful cases # Page Title 342 Site-to-site VPN to NAT Instance 345 Site-to-site VPN to VPC Interface endpoint 345 Site-to-site VPN to on-premise NAT to Internet Site-to-site VPN with TGW\u003e Site-to-site VPN with TGW # Page Title 362 Multiple Site-to-site Connection with TGW VPN CloudHub\u003e VPN CloudHub # 369 EC2 Based VPN\u003e EC2 Based VPN # Page Title 375 Single instance 376 HA 378 Horizontal scaling - VPN EC2 per subnet 379 Horizontal scaling - Split traffic Transit VPC\u003e Transit VPC # Page Title 391 Transit VPC 398 global regions with single Transit Hub 398 global regions with multiple Transit Hub (GRE) 549 Direct Connect and Transit VPC DX\u003e DX # Use cases\u003e Use cases # Page Title 591 VPN as a backup 592 Dual Devices 593 Dual locations 594 Dual locations with DX connection backup 602 VPN over DX Failed cases\u003e Failed cases # Page Title 530 DX Gateway with multiple customer sites 542 DX Gateway with multiple TGWs Successful cases\u003e Successful cases # Page Title 546 DX Gateway with multiple customer sites Network Firewall\u003e Network Firewall # Use cases\u003e Use cases # Centralised: single firewall subnet / vpc. connect with tgw (791) Distributed: firewall subnet per vpc (790) Limitation\u003e Limitation # VPC Limit\nPeering\u003e Peering # VPC Peering: 125\nRouting\u003e Routing # Resource Limit Private VIFs 100 Public VIFs 1000 MTU (159)\u003e MTU (159) # Resource Limit VPC 9001 VPC Peering 1500 DX 9001 TGW (to DX) 8500 TGW (to VPN) 1500 VPC Endpoint 1500 NAT 1500 IGW 1500 VPN 1500 Bandwidth\u003e Bandwidth # Quotas for your TGW Aggregated throughput limit for VGW NAT gateways VPC (180)\u003e VPC (180) # Resource Limit VPC Peer no IGW no NAT 5-45 Gbps TGW depends VGW 1.25Gbps VGW (to DX) depends on DX TGW\u003e TGW # 1 VPN can have 2 tunnels Resource Limit per resource VPC 50Gbps DX 50Gbps TGW 50Gbps VPN tunnel 1.25Gbps VPN Tunnel\u003e VPN Tunnel # 1.25Gbps\nEC2\u003e EC2 # 100G networking in AWS, a network performance deep dive EC2 network performance rules:\nsingle flow limit within regions and other Situation Limit w/i the region 100% to other regions 50% igw 50% dx 50% Single Flow\u003e Single Flow # Situation Limit w/i placement group 10Gbps other 5Gbps Aggregated\u003e Aggregated # General Purpose Network Performance Situation Limit w enhanced networking 100Gbps w/o enhanced networking 25Gbps Network Driver Limit Intel 82599VF 10Gbps ENA 100Gbps EFA 100Gbps ","date":"2021-11-26","permalink":"/posts/exam-aws-networking/","section":"Posts","summary":"Resources\u003e Resources # Exam Landing Page Sample Questions Exam Guide udemy course udemy mock exam whizlabs test \u0026amp; hand-on labs TODO\u003e TODO # take notes revision whizlabs mocks (by topic) mock 1 (udemy) mock 2,3,4 (whizlabs) examtopic udemy quiz + exercise Revision\u003e Revision # The numbers below are the page no of the udemy course\u0026rsquo;s pdf: AWS Certified Networking Specialty Slides v1.","title":"AWS Certified Advanced Networking - Specialty"},{"content":"Trust Over IP: Hyperledger Ursa, Indy, and Aires\u003e Trust Over IP: Hyperledger Ursa, Indy, and Aires # https://www.youtube.com/watch?v=FfuhlF9ZYPM\nNotes\u003e Notes # prove the credential without CA trusted issuer (eg: gov) issues the identity token (eg: driving licence) -\u0026gt; send it to a crypto account (like sending btc to someone) on the blockchain, it shows a record that the account owns that licence. (like you can check your btc balance / NFS on chain explorer) anyone can request that you own that account (like an approval transaction from dapp). You can decide to share your data to what extent send the data to requester and it can be verified as it is signed by issuer blockchain is used to prove the ownership of your identities data can be stored off chain, sign it and verify with issuer\u0026rsquo;s account hacking personal accounts become useless -\u0026gt; not cost effective, you can hack a single person each time stealing / leading personal data is useless -\u0026gt; no proof, no one will accept Example\u003e Example # british columbia\nComprehensive review of 21 use cases of Hyperledger\u003e Comprehensive review of 21 use cases of Hyperledger # https://youtu.be/VQTARQSuUFU?t=1440\nUse cases (some are still in proof of concept)\u003e Use cases (some are still in proof of concept) # end to end tracking (supply chain, different parties can participate eg: bank insurance company consultant retail\u0026hellip;) 2.identity management - eg: allow 3rd parties to access your data with requests, prevent fraud, credit checking, hash your passport/fingerprint so you can verify it is your identity contract automation- like promotion in company (HR), consultant contract voting tokenisations - cross parties reward program(asiamiles), assets management (use in trading, convert different kinds of assets like house stock cash to token) immutable record- law enforcement investigation, accounting, credit check, insurance claim, medical history, law\u0026hellip; Hyperledger Fabric Business Use Cases\u003e Hyperledger Fabric Business Use Cases # https://youtu.be/1ps4PJtFRfY?t=544\nIBM food trust\u003e IBM food trust # safety sustainability cost optimization (inventory / transportation ) tracking Tradelens -\u0026gt; (supply chain for trading)\u003e Tradelens -\u0026gt; (supply chain for trading) # costs 1.8T / year -\u0026gt; potential saving ~10% costs IBM blockchain world wire\u003e IBM blockchain world wire # world wire solution with blockchain use stellar protocol Singapore Exchange: Clearing Financial Transactions on Amazon Managed Blockchain\u003e Singapore Exchange: Clearing Financial Transactions on Amazon Managed Blockchain # https://www.youtube.com/watch?v=nUre2ELySdo\nblockchain is used to handle the payment and settlement of securities\nthe method they used called DVP (delivery versus payment) = a way to ensure cash and securities can simultaneously exchange\nclient can be\nbuyer = buy securities with eth seller = sell securities with eth In the architecture,\nhyperledger = securities eth client = cash Transaction will come across with both ledgers\narbitrator and observer with receive activities on chain\narbitrator = mediator = handle dispute observer = regulator = do regulatory report Pros with AWS Managed Blockchain\u003e Pros with AWS Managed Blockchain # no need to build the ledger by themself focus on the smart contract a single blockchain network can effectively work across multiple parties An Overview on Blockchain Services from AWS\u003e An Overview on Blockchain Services from AWS # https://www.youtube.com/watch?v=WAIOBeQA2QQ\nPros of blockchain network\u003e Pros of blockchain network # ledger database (immutable transaction) consensus decentralisation smart contract ease for audit Hyperledger fabric VS ETH\u003e Hyperledger fabric VS ETH # authentication access control easy to add user easy to duel with settlement (bank) faster than centialization private ledger use case\u003e use case # trading supply case Enhanced Transparency and Frictionless Supply Chain with Blockchain\u003e Enhanced Transparency and Frictionless Supply Chain with Blockchain # https://www.youtube.com/watch?v=2NqbUb4rT88\u0026list=WL\u0026index=46\nB2B Business\nbuy and sell order between Bottlers\nvisibility cross supply chain reduce manual process and communicate between sellers and buyers dispute resolution (smark contract) -\u0026gt; I think we can use TimeLock to do partial transaction easy to scale no need to build a different api for each system (mesh network n(n-1)/2) ","date":"2021-11-26","permalink":"/posts/blockchain-use-case/","section":"Posts","summary":"Trust Over IP: Hyperledger Ursa, Indy, and Aires\u003e Trust Over IP: Hyperledger Ursa, Indy, and Aires # https://www.youtube.com/watch?v=FfuhlF9ZYPM\nNotes\u003e Notes # prove the credential without CA trusted issuer (eg: gov) issues the identity token (eg: driving licence) -\u0026gt; send it to a crypto account (like sending btc to someone) on the blockchain, it shows a record that the account owns that licence.","title":"Blockchain use cases"},{"content":"","date":"2021-11-26","permalink":"/tags/cka/","section":"Tags","summary":"","title":"cka"},{"content":"","date":"2021-11-26","permalink":"/tags/etcd/","section":"Tags","summary":"","title":"etcd"},{"content":"with IAM\u003e with IAM # user -\u0026gt; with API_KEY \u0026amp; API_TOKEN audience attached iam role (eg: ec2 instance) update the iam role and user mapping in configmap aws-auth OIDC\u003e OIDC # OIDC built in at the eks cluster\nOICD can request AWS IAM to issue the web identity token the token will be used to assume role and access AWS service Item Value OICD oidc.hhuge9.com IAM user hugotse OIDC user kevintse IAM role S3Admin To assume a role in EKS\u003e To assume a role in EKS # Associate OIDC to AWS IAM - that why it is called associate-iam-oidc-provider. Create an role which has a trust relationshop to the service account from OIDC to assume with Web Identity Token - so who in OIDC can use web identity token to access aws service Difference between assume-role and assume-role-with-web-identity\u003e Difference between assume-role and assume-role-with-web-identity # hugotse can use aws sts assume-role to assume S3Admin kevintse can use aws sts assume-role-with-web-identity with web-identity to assume S3Admin How a pod to access aws service\u003e How a pod to access aws service # EKS has some webhooks, it will monitor our changes on cluster\nYou can create a custom admission webhooks, deny / modify specfic requests of api servers\nWhen the admission webhook detects a service account associated to a pod and that account annotate with iam arn, it will\nadmission webhook -\u0026gt; send OICD token to AWS IAM AWS IAM -\u0026gt; verify the OICD token and return web identity token to the webhook admission webhook save the web identity token and mount it as a volume in the pod Application -\u0026gt; use the web identity token with aws sts assume-role-with-web-identity to access AWS Service Where does the OICD token come from?\u003e Where does the OICD token come from? # with Service Account Token Volume Projection, any pod attached with a service account will automatically mount with a OICD token\nWays to assume role\u003e Ways to assume role # if you are an iam user, use access key if you are in aws services (like ec2 instance), attach iam role or with oidc, use web identity token oidc user get web identity token from aws iam with oidc token oidc user use the token to assume role Permission needs to assume role\u003e Permission needs to assume role # by a iam user\u003e by a iam user # user hugotse have right to use assume-role (call aws sts assume-role) a role has a trusted relationship to user hugotse to assume role by a oicd user\u003e by a oicd user # oidc user kevintse have right to use assume-role-with-web-identity a role has a trusted relationship with oicd user kevintse to assume role with web identity ","date":"2021-11-26","permalink":"/posts/k8s-authentication/","section":"Posts","summary":"with IAM\u003e with IAM # user -\u0026gt; with API_KEY \u0026amp; API_TOKEN audience attached iam role (eg: ec2 instance) update the iam role and user mapping in configmap aws-auth OIDC\u003e OIDC # OIDC built in at the eks cluster","title":"k8s Authentication"},{"content":"Things to turn off before exam\u003e Things to turn off before exam # Bandwidth saving\u003e Bandwidth saving # google drive sync any backgorund updates wifi connection in other devices Prevent from autoupdate\u003e Prevent from autoupdate # software update (mbp, iphone) Others\u003e Others # firewall disable all chrome plugins backup plan\u003e backup plan # sim card - two extra network: EE, giffgaff spare computer (MBP) Tips\u003e Tips # When use wc -\u0026gt; be careful the header, need total - 1\u003e When use wc -\u0026gt; be careful the header, need total - 1 # Store temporary commands / note / result in a fixed location like /root/tmp\u003e Store temporary commands / note / result in a fixed location like /root/tmp # Even in remote host; we use that fixed location to store our results then get back the result from it\u003e Even in remote host; we use that fixed location to store our results then get back the result from it # remote$ cat tmp remote$ exit remote$ exit local$ ssh remote cat tmp Do not use tmux\u003e Do not use tmux # hard to copy and paste and scroll\nOne question one yml\u003e One question one yml # Name with q\u0026lt;no\u0026gt;.yml like q10.yml is the yml of 10th question\nWrite all resources on same yml in same question\u003e Write all resources on same yml in same question # Set .vimrc\u003e Set .vimrc # set sw=2 ts=2 et set hidden Use alias\u003e Use alias # export do=\u0026#39;--dry-run=client -o yaml\u0026#39; export now=\u0026#39;--sort-by=\u0026#34;{.metadata.creationTimestamp}\u0026#34;\u0026#39; Use variable in command\u003e Use variable in command # n=namespace nn=name i=image s=server Then you will find that we can reuse many command history. like\nCreate po yml\u003e Create po yml # k run -n $n $nn --image $i $do Get po\u003e Get po # k get po $nn -n $n Test service account\u003e Test service account # k auth can-i --as=system:serviceaccount:$n:$nn When working on next question, start a new vim (kill the old session)\u003e When working on next question, start a new vim (kill the old session) # Keep a single vim session in bash\u003e Keep a single vim session in bash # back and fore from vim and bash many times is needed using multiple vim session are so confusing that also why working on single yaml per question is encouraged Create (touch /opt/xxxxx/q11/answer) the answer file first\u003e Create (touch /opt/xxxxx/q11/answer) the answer file first # needed to submit anser to specfic file just touch the file -\u0026gt; open it with vim -\u0026gt; put it in background (c-z) ","date":"2021-11-26","permalink":"/posts/exam-cka/","section":"Posts","summary":"Things to turn off before exam\u003e Things to turn off before exam # Bandwidth saving\u003e Bandwidth saving # google drive sync any backgorund updates wifi connection in other devices Prevent from autoupdate\u003e Prevent from autoupdate # software update (mbp, iphone) Others\u003e Others # firewall disable all chrome plugins backup plan\u003e backup plan # sim card - two extra network: EE, giffgaff spare computer (MBP) Tips\u003e Tips # When use wc -\u0026gt; be careful the header, need total - 1\u003e When use wc -\u0026gt; be careful the header, need total - 1 # Store temporary commands / note / result in a fixed location like /root/tmp\u003e Store temporary commands / note / result in a fixed location like /root/tmp # Even in remote host; we use that fixed location to store our results then get back the result from it\u003e Even in remote host; we use that fixed location to store our results then get back the result from it # remote$ cat tmp remote$ exit remote$ exit local$ ssh remote cat tmp Do not use tmux\u003e Do not use tmux # hard to copy and paste and scroll","title":"Note for CKA Exam"},{"content":" Control Panel svr1 svr2 svr3 svr1\u003e svr1 # Stop k8s\u003e Stop k8s # systemctl stop kubelet crictl rm -f $(crictl ps -q) start the etcd in single node\u003e start the etcd in single node # vim /etc/kubernetes/manifests/etcd.yaml - --force-new-cluster=true Add svr2 etcd to cluster\u003e Add svr2 etcd to cluster # kubectl exec -it etcd-svr1.hhuge9.com -nkube-system -- sh -c \u0026#39;ETCDCTL_API=3 etcdctl --endpoints 127.0.0.1:2379 --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key --cacert=/etc/kubernetes/pki/etcd/ca.crt member add svr2.hhuge9.com --peer-urls=\u0026#34;https://192.168.0.102:2380\u0026#34;\u0026#39; Save the output\u003e Save the output # export ETCD_NAME=\u0026#34;svr2.hhuge9.com\u0026#34; export ETCD_INITIAL_CLUSTER=\u0026#34;svr1.hhuge9.com=http://192.168.0.101:2380,svr2.hhuge9.com=http://192.168.0.102:2380\u0026#34; export ETCD_INITIAL_CLUSTER_STATE=existing systemctl start kubelet svr2\u003e svr2 # Stop k8s and remove etcd data\u003e Stop k8s and remove etcd data # systemctl stop kubelet crictl rm -f $(crictl ps -q) rm -rf /var/lib/etcd Update etcd config (with the output in svr1)\u003e Update etcd config (with the output in svr1) # vim /etc/kubernetes/manifests/etcd.yaml - --initial-cluster=svr1.hhuge9.com=https://192.168.0.101:2380,svr2.hhuge9.com=https://192.168.0.102:2380 - --initial-cluster-state=existing systemctl start kubelet ","date":"2021-11-26","permalink":"/posts/etcd/","section":"Posts","summary":"Control Panel svr1 svr2 svr3 svr1\u003e svr1 # Stop k8s\u003e Stop k8s # systemctl stop kubelet crictl rm -f $(crictl ps -q) start the etcd in single node\u003e start the etcd in single node # vim /etc/kubernetes/manifests/etcd.","title":"Recover etcd"},{"content":"","date":"2021-10-21","permalink":"/categories/","section":"Categories","summary":"","title":"Categories"},{"content":"Procedures\u003e Procedures # create private key create csr (distinguished_name) sign csr by CA (x509v3) Commands\u003e Commands # Gen private key\u003e Gen private key # openssl genrsa \\ -out ca.key Self signed crt\u003e Self signed crt # openssl req -x509 \\ -config openssl.cnf -new \\ -out ca.crt \\ -key ca.key gen csr\u003e gen csr # openssl req \\ -config openssl.cnf -new \\ -out server.csr \\ -key server.key sign crt\u003e sign crt # openssl x509 \\ -extfile openssl.cnf \\ -extensions svr_cert \\ -in server.csr \\ -out server.crt \\ -CA ca.crt \\ -CAkey ca.key \\ -CAcreateserial When the CA is first time to sign a cert, you need to create serial file ca.crl with -CAcreateserial Next time we sign a new cert, we will need to update the serial file with -CAserial ca.crl Outputs\u003e Outputs # Inspect the cert / key\u003e Inspect the cert / key # openssl rsa -in ca.key -noout -text openssl x509 -in ca.crt -noout -text extract the pub key\u003e extract the pub key # openssl rsa -in ca.key -pubout openssl x509 -in ca.crt -noout -pubkey Verify\u003e Verify # crt is issue by ca\u003e crt is issue by ca # openssl verify -CAfile ca.crt server.crt key and crt are match (modulus)\u003e key and crt are match (modulus) # openssl rsa -in ca.key -nout -modulus | openssl md5 openssl x509 -in ca.crt -nout -modulus | openssl md5 trust self signned ca\u003e trust self signned ca # we need to place the ca.crt to a location and call command below to trusted the cert (the programme will create a soft link)\nwe can man update-ca-certificate to find the location. eg: /usr/local/share/ca-certificates\ncp ca.crt /usr/local/share/ca-certificates update-ca-certificate trusted the cert in firefox and chrome acutally work but we need to restart the pc..\ntwo output should be same\nCAxxxx \u0026lt;- Capital in CA, lowercase in next word -in -out \u0026lt;- represent the module\nConfig\u003e Config # cp /etc/ssl/openssl.cnf openssl.cnf vi openssl.cnf Remove all empty and commended lines and start update the openssl.cnf\nHOME = . oid_section = new_oids [ new_oids ] tsa_policy1 = 1.2.3.4.1 tsa_policy2 = 1.2.3.4.5.6 tsa_policy3 = 1.2.3.4.5.7 [ ca ] [ CA_default ] policy = policy_match [ policy_match ] countryName = match stateOrProvinceName = match organizationName = match organizationalUnitName = optional commonName = supplied emailAddress = optional [ policy_anything ] countryName = optional stateOrProvinceName = optional localityName = optional organizationName = optional organizationalUnitName = optional commonName = supplied emailAddress = optional [ req ] default_bits = 2048 default_keyfile = privkey.pem distinguished_name = req_distinguished_name attributes = req_attributes string_mask = utf8only [ req_distinguished_name ] 0.organizationName = Organization Name (eg, company) 0.organizationName_default = hhuge9.com commonName = Common Name (e.g. server FQDN or YOUR name) commonName_max = 64 [ req_attributes ] [ v3_req ] basicConstraints = CA:FALSE keyUsage = critical, digitalSignature, keyEncipherment [ v3_ca ] basicConstraints = critical,CA:true keyUsage = critical, keyCertSign subjectKeyIdentifier=hash authorityKeyIdentifier=keyid:always,issuer [ svr_cert ] basicConstraints=CA:FALSE keyUsage = critical, digitalSignature, keyEncipherment extendedKeyUsage = serverAuth subjectKeyIdentifier=hash authorityKeyIdentifier=keyid,issuer [ usr_cert ] basicConstraints=CA:FALSE keyUsage = critical, digitalSignature, keyEncipherment extendedKeyUsage = clientAuth subjectKeyIdentifier=hash authorityKeyIdentifier=keyid,issuer The most important setting is session: v3_req v3_ca svr_cert and usr_cert\nNormally, Cert need req_distinguished_name session only.\nFor example, we use domain in CN (Common Name) in web server cert\nIn k8s, we will use CN as username, O (Orgnazation) as group for auth\nIt can be done by commands without config.\nThose information is defined in req stage. (not sign stage)\nIf you want to use config, you can:\n[ req_distinguished_name ] 0.organizationName = Organization Name (eg, company) 0.organizationName_default = hhuge9.com commonName = Common Name (e.g. server FQDN or YOUR name) commonName_max = 64 Or in command, we can\nopenssl req \\ -config openssl.cnf -new \\ -out server.csr \\ -key server.key \\ -subj \u0026#39;/O=hhuge9.com/CN=redis.hhuge9.com\u0026#39; But when we want to create a CA, cert for TLS auth. We need x503v3 extension to define additional information.\nThat is why we see -config and -extensions in sign command.\nTo check the documentation of the extension, we can man openssl x509 \u0026gt; go to bottom \u0026gt; Also see x509v3_config \u0026gt; man x509v3_config\nCSR do not need the extension. -config is optional.\nThis three are most important:\nFor non CA\nbasicConstraints = CA:FALSE keyUsage = critical, digitalSignature, keyEncipherment For CA\nbasicConstraints = CA:TRUE keyUsage = critical, keyCertSign For server cert\nextendedKeyUsage = serverAuth For client cert\nextendedKeyUsage = clientAuth If we want to contraint the cert be used by specfic host, we can add subjectAltName\nsubjectAltName = @alt_name [ alt_name ] DNS.0 = redis-master.hhuge9.com DNS.1 = redis01.hhuge9.com IP.0 = 192.168.2.100 ","date":"2021-10-21","permalink":"/posts/openssl/","section":"Posts","summary":"Procedures\u003e Procedures # create private key create csr (distinguished_name) sign csr by CA (x509v3) Commands\u003e Commands # Gen private key\u003e Gen private key # openssl genrsa \\ -out ca.key Self signed crt\u003e Self signed crt # openssl req -x509 \\ -config openssl.","title":"How to create ssl cert for tls auth"},{"content":"","date":"2021-10-21","permalink":"/categories/k8s/","section":"Categories","summary":"","title":"k8s"},{"content":"","date":"2021-10-21","permalink":"/tags/openssl/","section":"Tags","summary":"","title":"openssl"},{"content":"","date":"2021-10-21","permalink":"/categories/openssl/","section":"Categories","summary":"","title":"openssl"},{"content":"","date":"2021-10-09","permalink":"/tags/chrome/","section":"Tags","summary":"","title":"chrome"},{"content":"I am preparing the CKA exam. The exam requires us to complete tasks using web terminal on Chrome.\nMy concern is that Ctrl+w is a common shortcut in terminal (delete word) but it also is the \u0026ldquo;close tab\u0026rdquo; shortcut of chrome. (Windows)\nThere is a workaround on this issue. Comment under this article mentioned that we can define ctrl+w in chrome://extensions/shortcuts to override the default behavior on ctrl+w. This will prevent the tab be closed when hit ctrl+w accidently.\n","date":"2021-10-09","permalink":"/posts/disable-ctrl-w-on-chrome/","section":"Posts","summary":"I am preparing the CKA exam. The exam requires us to complete tasks using web terminal on Chrome.\nMy concern is that Ctrl+w is a common shortcut in terminal (delete word) but it also is the \u0026ldquo;close tab\u0026rdquo; shortcut of chrome.","title":"Disable Ctrl+w on Chrome"},{"content":"","date":"2021-10-09","permalink":"/categories/other/","section":"Categories","summary":"","title":"other"},{"content":" https://www.w3.org/TR/NOTE-datetime\n2019-07-16T12:00:00Z = utc time\nYYYY-MM-DDThh:mm:ss.sTZD\nTZD can be Z or +hh:mm or -hh:mm\nZ = UTC\n+08:00 = UTC+8\n","date":"2021-09-17","permalink":"/posts/time-format/","section":"Posts","summary":"https://www.w3.org/TR/NOTE-datetime\n2019-07-16T12:00:00Z = utc time\nYYYY-MM-DDThh:mm:ss.sTZD\nTZD can be Z or +hh:mm or -hh:mm\nZ = UTC\n+08:00 = UTC+8","title":"Time Format"},{"content":"Facing this warning in a repo\nwarning: ignoring broken ref refs/remotes/origin/HEAD https://stackoverflow.com/questions/45811971/warning-ignoring-broken-ref-refs-remotes-origin-head\nAfter reading this article, seem the reference of refs/remotes/origin/HEAD is broken.\n$ git symbolic-ref refs/remotes/origin/HEAD refs/remotes/origin/master I have changed the branch from master to main before. master branch is no longer exist. I think that is the reason of the warning.\ngit symbolic-ref refs/remotes/origin/HEAD refs/remotes/origin/main Then the warning is fixed.\n","date":"2021-08-25","permalink":"/posts/broken-git-ref/","section":"Posts","summary":"Facing this warning in a repo\nwarning: ignoring broken ref refs/remotes/origin/HEAD https://stackoverflow.com/questions/45811971/warning-ignoring-broken-ref-refs-remotes-origin-head\nAfter reading this article, seem the reference of refs/remotes/origin/HEAD is broken.\n$ git symbolic-ref refs/remotes/origin/HEAD refs/remotes/origin/master I have changed the branch from master to main before.","title":"Broken Git Ref"},{"content":"","date":"2021-08-25","permalink":"/tags/git/","section":"Tags","summary":"","title":"git"},{"content":"","date":"2021-08-25","permalink":"/categories/git/","section":"Categories","summary":"","title":"git"},{"content":"Recently find that it is too slow to start a new bash.\nAfter debugging, I realize that nvm is the main reason for that.\nFinally, disabled nvm to solve the problem.\n","date":"2021-08-25","permalink":"/posts/slow-bash-startup/","section":"Posts","summary":"Recently find that it is too slow to start a new bash.\nAfter debugging, I realize that nvm is the main reason for that.\nFinally, disabled nvm to solve the problem.","title":"Slow Bash Startup"},{"content":"I used the emacs keybind to maniplicate the string on command line but it does not work at chrome on macos.\nI am unhappy with that until I find the solution on this post\nhttps://stackoverflow.com/questions/20146972/is-there-a-way-to-make-alt-f-and-alt-b-jump-word-forward-and-backward-instead-of sudo mkdir -p ~/Library/Keybindings/ sudo vi ~/Library/Keybindings/DefaultKeyBinding.dict { \u0026#34;~d\u0026#34; = \u0026#34;deleteWordForward:\u0026#34;; \u0026#34;^w\u0026#34; = \u0026#34;deleteWordBackward:\u0026#34;; \u0026#34;~f\u0026#34; = \u0026#34;moveWordForward:\u0026#34;; \u0026#34;~b\u0026#34; = \u0026#34;moveWordBackward:\u0026#34;; } ","date":"2021-08-20","permalink":"/posts/move-forward-backward-in-chrome/","section":"Posts","summary":"I used the emacs keybind to maniplicate the string on command line but it does not work at chrome on macos.\nI am unhappy with that until I find the solution on this post","title":"Move Forward Backward in Chrome"},{"content":"","date":"0001-01-01","permalink":"/authors/","section":"Authors","summary":"","title":"Authors"},{"content":"","date":"0001-01-01","permalink":"/series/","section":"Series","summary":"","title":"Series"}]