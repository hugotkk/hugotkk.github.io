[{"content":"","date":"10 July 2024","externalUrl":null,"permalink":"/tags/azure/","section":"Tags","summary":"","title":"azure","type":"tags"},{"content":" Access control # Azure Key Vault offers two main methods for controlling access:\nAccess Policies (legacy) Role-Based Access Control (RBAC) - recommended Access Policies # Applied at the Key Vault level Permissions granted affect all keys within the vault Less granular control compared to RBAC RBAC # Can be set at either the Key Vault or Object level Allows for more granular permissions (e.g., granting access to Key1 but not Key2) Recommended over Access Policies for better security and management Enabling Key Vault for Specific Services # To use Key Vault with certain Azure services, we need to enable specific permissions:\nFor Azure Disk Encryption:\nNavigate to the Key Vault\u0026rsquo;s Access Policies Under \u0026ldquo;Enable Access to\u0026rdquo;, select \u0026ldquo;Azure Disk Encryption for volume encryption\u0026rdquo; For ARM Template deployments:\nEnable \u0026ldquo;Access Azure Resource Manager for template deployment\u0026rdquo; Integrate with App Serivce # To integrate Key Vault with an App Service:\nCreate a Managed Identity for App. Configure IAM settings in Key Vault. (eg: Key Vault Secrets User) Add Application Setting to reference the key Vault Secret DB_PASSWORD=@Microsoft.KeyVault(SecretUri=https://mykeyvault.vault.azure.net/secrets/DB_PASSWORD/) In the Application, we can access the secret as environment variable \u0026lt;?php // Connect to database $servername = \u0026#34;your_server_name\u0026#34;; $username = \u0026#34;your_username\u0026#34;; $password = getenv(\u0026#39;DB_PASSWORD\u0026#39;); $dbname = \u0026#34;your_database_name\u0026#34;; // Create connection $conn = new mysqli($servername, $username, $password, $dbname); // Check connection if ($conn-\u0026gt;connect_error) { die(\u0026#34;Connection failed: \u0026#34; . $conn-\u0026gt;connect_error); } echo \u0026#34;Connected successfully\u0026#34;; ?\u0026gt; Failover # It supports failover within a region and across regions.\nHowever,\nDuring regional failover, the key vault becomes read-only. DELETE operations cannot be performed during this time. The read-only state ensures data consistency across regions during the failover.\nBackup and Restore # Limitations:\nBackups can only be restored within the same geography.\nFor example, a backup from UK West can be restored to UK East, but not to US East.\nBackups can only be restored within the same Azure subscription.\nBackups are encrypted and can only be decrypted within Azure.\nThere is a time limit for restoring backups\nKeys, certificates, and secrets need to be backed up individually. There is no single operation to back up the entire key vault at once.\n","date":"10 July 2024","externalUrl":null,"permalink":"/posts/azure-key-vault/","section":"Posts","summary":"Access control # Azure Key Vault offers two main methods for controlling access:\nAccess Policies (legacy) Role-Based Access Control (RBAC) - recommended Access Policies # Applied at the Key Vault level Permissions granted affect all keys within the vault Less granular control compared to RBAC RBAC # Can be set at either the Key Vault or Object level Allows for more granular permissions (e.","title":"Azure Key Vault","type":"posts"},{"content":" Monitor Tools # Azure Activity Log is equivalent to AWS CloudTrail. Azure Advisor: Provides recommendations for cost savings and best practices. Traffic Analytics: showing statistics, summary, numbers of the network Application Insight # It is a powerful application performance management tool.\nIt offer two main methods of implementation:\nAgent-based (Codeless): No code changes required Supports multiple languages (e.g., .NET, Java, Node.js) Limited customization options SDK-based: Requires code changes Supports multiple languages (Java, Python, JavaScript, etc.) Offers more control and customization Implemented as middleware in the application Features:\nLogging: Sniff any logger.warning(f\u0026quot;foo bar\u0026quot;) and send to cloud Centralized log collection and analysis Metrics: Similar to Prometheus client library Custom metrics Tracing: Monitors requests from start to end Measures duration and identifies bottlenecks Dependency tracking Exception monitoring Real-time dashboards Integrate with Microsoft Sentinel (a platform to consolidate all security related data) VM Inslights # Monitors CPU, memory, disk, and network usage Container Inslights # Monitor containerized applications Works with AKS and ACI Provides container-specific metrics and logs Metrics / Insight # Azure Monitor is similar to AWS CloudWatch.\nTo setup the monitoring, we need to\nInstall Azure Monitor Agent for collect metrics and logs. Setup Data Collection Rule (DCR) to send metrics and logs from the agent to Log Analytics Workspaces. Components of a DCR:\nResources: Specify VMs from which to collect data. Data Sources: Include Performance Data, Syslog, and Event Logs gathered by the agent. Destination: Log Analytics Workspace. Notice that:\nOne DCR can collect data from multiple VMs One VM can contribute data to multiple DCRs This setup allows centralized management and analysis of metrics and logs from Azure VMs using Azure Monitor and Log Analytics Workspaces.\nLegacy Way will use Log Analytics:\nCreate a Log Analytics workspace. Install the Log Analytics agent. Under the Log Analytics workspace, configure performance counters. Monitoring Agents # Monitor Agent:\nCapable of sending logs and metrics to multiple Log Analytics Workspaces. Recommended as a replacement for other agents. Does not support custom logs or IIS logs. Windows/Linux Diagnostics Agent:\nFound in the Diagnostics tab. Can send logs and metrics to multiple destinations such as Log Analytics Workspaces, Storage Accounts, and Event Hubs. Provides more robust functionality compared to Monitor Agent. Both agents support metrics and logs and are designed for use with VMs.\nLog Analytics Agent:\nSupports sending logs only. Configuration is decentralized (needs to be configured on the VM itself). Can be used outside of Azure environments. Alerts # ITSM Integration # One of the alert destinations is send the alert to ITSM via IT Service Management Connector.\nBut this will be retired in 2025. Currently, it can only be created via API.\nAlert Rate Limits # SMS/Voice Call: Every 5 minutes. Email: Less than 100 per hour. Emails will only be sent to individual users, not to groups or service principals.\nAdministrative Operations Alerts # You can configure alerts for administrative operations logged in Azure Activity, which essentially covers any API call. Examples include:\nWriting a tag to resource and VM Creating a disk Attaching a disk to VM In the activity log, these operations are identified by \u0026quot;eventSource\u0026quot;: \u0026quot;Administrative\u0026quot;.\nSuppressed Notifications # When an alert processing rule suppresses notifications, the alert will still be listed in the portal, but no notification will be sent.\nAlert Components # Resource (Scope): The subscription or resource the alert pertains to. Condition (Signal): The source of data and the threshold that triggers the alert. Signal Source: Typically a Log Analytics Workspace. Action Group: Defines the notifications and actions to be taken when the alert is triggered. User Response States # If the user response is \u0026ldquo;New\u0026rdquo;, it can be changed to \u0026ldquo;Acknowledged\u0026rdquo; or \u0026ldquo;Closed.\u0026rdquo; If the user response is \u0026ldquo;Closed\u0026rdquo;, it cannot be changed. ","date":"10 July 2024","externalUrl":null,"permalink":"/posts/azure-monitoring/","section":"Posts","summary":"Monitor Tools # Azure Activity Log is equivalent to AWS CloudTrail. Azure Advisor: Provides recommendations for cost savings and best practices. Traffic Analytics: showing statistics, summary, numbers of the network Application Insight # It is a powerful application performance management tool.","title":"Azure Monitoring","type":"posts"},{"content":" Evolution # StorageV1: Original BlobStorage: Introduced for Blob-specific capabilities StorageV2: Added ZRS support Data Lake Gen2: Supports Hierarchical Namespace, POSIX ACL, HDFS protocol Limitations # StorageV1: No Blob features, no ZRS support ZRS: Not in StorageV1. GRS: Not in Premium storage, focused on low-latency Premium:: No Tier Access, focused on low-latency Live migration of storage redundancy: LRS to ZRS Only Storage Tiers # Hot / Cool / Cold (online tiers) Archive (offline tiers) Up to 15 hours to transition to online tiers. No Zone-related redudancy support, designed for Archive, not HA Default tier: Hot / Cool only File Share: Hot and Cool only Tier Durations: 30, 90, 180 days for cool, cold, archive Blob Features (except V1) # Tier Access: Enables cost savings by managing data storage in different tiers (hot, cool, cold, archive) Lifecycle Management: Facilitates moving objects between tiers and setting retention periods for data Access Policies: Allows setting up to 5 access policies to limit SAS Immutable Policies: Supports configuring up to 2 immutable policies to prevent data modification or deletion Object Replication: Provides options to replicate objects to specific region GRS does not allow choosing the replication region Scoped Encryption: Allows using different encryption keys for specific containers Encryption # Blob: support file-level encryption File Share: Data at rest encryption only Customer-provided key: Blob: Supported Data Lake: Not Support IAM Role Conditions Support # Containers Queues Storage Type Comparision # Page blob: Block-level backup, DB, OS disk Block blob: Large objects (e.g., video), up to 8TB Append blob: Logs File share: Legacy (SMB-like), up to 5TB/100TB with large file share feature Shared Access Signature (SAS) # A SAS is a secure way to share our Azure Storage resources without compromising the account keys.\nIt provides temporary, fine-grained access control to storage resources.\nResource Levels # It can be applied to different resource levess:\nService: blob, file, queue, table Container: e.g., blob container Object: e.g., specific blob Permissions # And can grant various permission, including:\nREAD: Read data WRITE: Write data DELETE: Delete objects CREATE: Create new objects LIST: List objects in a container ADD: Add messages to a queue Usage Examples # Operation SAS Permissions Azure CLI Command Download Blob Resource Type: Object Permissions: Read az storage blob download --account-name \u0026quot;$account\u0026quot; --container-name \u0026quot;$container\u0026quot; --sas-token \u0026quot;$token\u0026quot; --name \u0026quot;$file\u0026quot; List Blobs in Container Resource Type: Containe Permissions: List az storage blob list --account-name \u0026quot;$account\u0026quot; --container-name \u0026quot;$container\u0026quot; --sas-token \u0026quot;$token\u0026quot; List Containers Resource Type: Servic Permissions: List az storage container list --account-name \u0026quot;$account\u0026quot; --sas-token \u0026quot;$token\u0026quot; Access levels # It offers two main access levels:\nPublic: Allows anonymous read access for containers and blobs Private: Rquires authentication for all access Access Policies # As mentioned, SAS provides a way to grant limited access to objects in our storage account to others, without exposing the account keys.\nNotice that:\nSAS is signed using the account key. If compromised, a SAS cannot be directly revoked. To handle the compromised SAS, we can Rotate the account key (affects all SAS tokens signed with it). Use an access policy to further restrict the SAS. That\u0026rsquo;s why Access Policy is needed. It\nProvides an additional layer of control over SAS tokens. Can be used to modify or revoke permissions without changing the account key. Each container can have up to 5 access policies defined.\nImmutable Blob Storage Policy # Immutable storage can helps us to store business-critical data objects in a WORM (Write Once, Read Many) state.\nThere are two types of immutability policies:\nLegal Hold # Allows creating and reading blobs. Prevents modification or deletion of blobs. Useful for short-term scenarios or legal investigations. Can be removed when no longer needed. Time-Based Retention # Allows WORM within a specified period. After the retention period, objects can be deleted but not modified. Retention period can be extended but not shortened. Useful for regulatory compliance. Lifecycle Management # Lifecycle Management applies rules specifying blob types\nBlockBlob AppendBlob and their subtypes\nbase snapshot version { \u0026#34;rules\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;exampleRule\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Lifecycle\u0026#34;, \u0026#34;definition\u0026#34;: { \u0026#34;actions\u0026#34;: { \u0026#34;version\u0026#34;: { # this rule apply to version only \u0026#34;delete\u0026#34;: { \u0026#34;daysAfterCreationGreaterThan\u0026#34;: 30 } } }, \u0026#34;filters\u0026#34;: { \u0026#34;blobTypes\u0026#34;: [ \u0026#34;blockBlob\u0026#34; # rule apply to BlockBlob ] } } } ] } If multiple rules apply, the less expensive action is chosen:\ndelete \u0026gt; move to archive \u0026gt; move to cool. Objects moved to archive are not moved back to cool. Filters:\nprefixMatch: Matches file paths. blobIndexMatch: Matches index tags (a property of objects). blobTypes: Specifies block or append blob. Actions:\nFor baseBlob: enableAutoTierToHotFromCool: Enables switching files back to hot if accessed. tierToArchive: Moves files to archive. tierToCool: Moves files to cool. tierToCold: Moves files to cold. delete: Delete file. IAM with Conditions # To restrict view permissions to specific blobs, IAM can be configured with conditions based on blob index tags.\nExample:\nRole: Storage Blob Data Reader Action: Read (to view blobs) Condition Expression: Attribute source: Resource Attribute: Blob index tags Values in key Key: \u0026lt;Key\u0026gt; Operator: StringEquals Value: \u0026lt;Value\u0026gt; Encryption # Customer-managed keys:\nUse RSA Support key sizes of 2048, 3072, and 4096 bits Stored in Azure Key Vault or Azure Managed HSM Allow for key rotation and revocation managed by the customer Azure-managed keys:\nUse symmetric key encryption (AES-256) Managed entirely by Microsoft Automatically rotated by Microsoft Require no setup or management by the customer Network ACL # Configuration Example # Bypass: AzureServices Virtual Network Rules: None configured IP Rules: None configured Default Action: Allow This configuration indeed allows all access by default. The \u0026ldquo;AzureServices\u0026rdquo; bypass allows trusted Microsoft services to access the storage account even if network rules are in place.\nWhen no virtual network or IP rules are specified, and the default action is set to \u0026ldquo;Allow\u0026rdquo;, public IP access is permitted from any network.\nNetwork Modes # Enable Public Access:\nDefault Action: Allow Enable Access from Selected Virtual Networks and IP Addresses:\nFirewall (IP Rules): Allows us to specify individual IP addresses or CIDR ranges that can access the storage account. Virtual Network (Virtual Network Rules): Allows us to specify Azure Virtual Networks that can access the storage account. Exceptions (Bypass): Allows us to bypass the firewall for certain Azure services. Disable Public Access:\nDefault Action: Disabled This is the most restrictive setting, blocking all public access. Access is only possible through specified virtual networks or IP addresses. Miscellaneous # Azure Backup / Site Recovery When using Azure Backup for VMs or Azure Site Recovery with a storage account that has restricted public access, we need to enable the \u0026ldquo;Allow trusted Microsoft services to access this storage account\u0026rdquo; option. Event hub Capture: Event Hub capture does not support premium storage accounts. It works with Data Lake Storage Gen2 or standard blob storage accounts. The data format used for Event Hub capture is Avro. File Share When using SMB protocol with Azure File Share and authenticating with AD, Azure AD DS can be utilized. ","date":"10 July 2024","externalUrl":null,"permalink":"/posts/azure-storage-account/","section":"Posts","summary":"Evolution # StorageV1: Original BlobStorage: Introduced for Blob-specific capabilities StorageV2: Added ZRS support Data Lake Gen2: Supports Hierarchical Namespace, POSIX ACL, HDFS protocol Limitations # StorageV1: No Blob features, no ZRS support ZRS: Not in StorageV1.","title":"Azure Storage Account","type":"posts"},{"content":"","date":"10 July 2024","externalUrl":null,"permalink":"/","section":"Hugo's IT journal","summary":"","title":"Hugo's IT journal","type":"page"},{"content":"","date":"10 July 2024","externalUrl":null,"permalink":"/tags/key-vault/","section":"Tags","summary":"","title":"key vault","type":"tags"},{"content":"","date":"10 July 2024","externalUrl":null,"permalink":"/tags/monitoring/","section":"Tags","summary":"","title":"monitoring","type":"tags"},{"content":"","date":"10 July 2024","externalUrl":null,"permalink":"/posts/","section":"Posts","summary":"","title":"Posts","type":"posts"},{"content":"","date":"10 July 2024","externalUrl":null,"permalink":"/tags/storage-account/","section":"Tags","summary":"","title":"storage account","type":"tags"},{"content":"","date":"10 July 2024","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","date":"9 July 2024","externalUrl":null,"permalink":"/tags/acr/","section":"Tags","summary":"","title":"acr","type":"tags"},{"content":"","date":"9 July 2024","externalUrl":null,"permalink":"/tags/aks/","section":"Tags","summary":"","title":"aks","type":"tags"},{"content":"","date":"9 July 2024","externalUrl":null,"permalink":"/tags/app-service/","section":"Tags","summary":"","title":"app service","type":"tags"},{"content":"The region of the App and App Service Plan should be matched.\nApp Service Plan defines the underlying infrastructure and resources for hosting applications.\nThis includes the OS (Windows/Linux), Region, no of instances, instance type, and pricing tier.\nApp refers to the actual application or code that runs on the App Service Plan.\nMultiple apps can share a single App Service Plan, which helps optimize resource utilization and costs.\nService Supported Operating Systems Container Instance Linux, Windows Container App Linux App Service Linux, Windows AKS Windows, Linux Runtime Support # Both Windows and Linux\n.NET Core 3.0 and later PHP Linux only\nRuby Windows only\nASP.NET (traditional .NET Framework) HA # For HA or DR scenarios, we can deploy App Services across multiple regions:\nCreate identical App Services in separate regions. One region serves as the active site while the other acts as standby. Azure Front Door can be used to route traffic between these regions. Price Tiers # Standard and Premium Tiers: Scaling: Horizontal: Increase the no of VM instances Vertical: Upgrade to a higher instance type Slot Deployment: Available for managing multiple deployment slots Isolated Tier: Run app on a dedicated VNet, providing network isolation and improved security. Can integrate with App Service Environment (ASE) - which run app on a dedicated host Key Vault # To use Azure Key Vault in an App Service\nCreate Managed User Identity in App Service Config IAM in Key Vault Reference Key Vault secrets in App Service configuration Logging # App Service Logs: Web Server Logging Application Insights Profile: Performance Traces Backup # App service backups are stored in a Storage Account.\nWe can create _backup.filter to exclude specific folders from the backup.\n-*.avi -*.mp4 -/wwwroot/large_files +/wwwroot/important_folder ","date":"9 July 2024","externalUrl":null,"permalink":"/posts/azure-app-service/","section":"Posts","summary":"The region of the App and App Service Plan should be matched.\nApp Service Plan defines the underlying infrastructure and resources for hosting applications.\nThis includes the OS (Windows/Linux), Region, no of instances, instance type, and pricing tier.","title":"Azure App Service","type":"posts"},{"content":" AKS # Azure AD Authentication # Legacy Method:\nCreate an app in EntraID manually Assign directory read role to the app Link the app with AKS az aks update-credentials \\ --resource-group myResourceGroup \\ --name myAKSCluster \\ --reset-aad \\ --aad-client-app-id \u0026lt;client-id\u0026gt; \\ --aad-server-app-id \u0026lt;server-id\u0026gt; \\ --aad-server-app-secret \u0026lt;server-secret\u0026gt; \\ --aad-tenant-id \u0026lt;tenant-id\u0026gt; Current Method:\nAKS service creates the necessary Azure AD application automatically We just need to enable the feature when creating or updating the cluster az aks update --resource-group myResourceGroup --name myAKSCluster --enable-aad Networking # Node Networking # By default, AKS use Azure Managed Virtual Network and subnet for nodes.\nWe can use Bring Your Own Virtual Network (BYOVNET) to use our existing VNET for nodes.\n\u0026ldquo;Enabling public IP per node\u0026rdquo; doesn\u0026rsquo;t directly affect the cluster\u0026rsquo;s internet access.\nThe cluster can still access the internet even with this option disabled, unless we configure \u0026ldquo;Enable private cluster\u0026rdquo; to Yes.\nPod Networking # Azure CNI Kubenet Azure CNI:\neach pod is assigned an IP address that\u0026rsquo;s individually accessible within the vnet useful when we need direct communication with pods using their IP addresses. Both the VNET and AKS cluster must be in the same location kubenet:\npods will form a internal network which use within the cluster only The VNET can be in any location, but it needs to be routable to the AKS cluster Notice that Windows node pools and Virtual Node are only supported with Azure CNI networking.\nNetwork Policy # Calico: Can be used with both kubenet and Azure CNI pod networking Azure: Must be used with Azure CNI pod networking Virtual Node # Virtual nodes in AKS use Azure Container Instances (ACI) as the underlying infrastructure, similar to how AWS Fargate works for EKS\nVirtual nodes can be enabled as an add-on:\naz aks enable-addons --addons virtual-node --name myAKSCluster --resource-group myResourceGroup Limitations:\nNo support for DaemonSets or init containers No IPv6 support Only compatible with Azure CNI To deploy applications to virtual nodes, we need to use specific nodeSelector and tolerations in pod specification\nnodeSelector: kubernetes.io/role: agent beta.kubernetes.io/os: linux type: virtual-kubelet tolerations: - key: virtual-kubelet.io/provider operator: Exists - key: azure.com/aci effect: NoSchedule Windows Container # Windows Node Pool # az aks nodepool add \\ --resource-group \u0026lt;resource-group-name\u0026gt; \\ --cluster-name \u0026lt;cluster-name\u0026gt; \\ --os-type Windows \\ --name \u0026lt;windows-nodepool-name\u0026gt; \\ --node-count \u0026lt;number-of-nodes\u0026gt; \\ --kubernetes-version \u0026lt;kubernetes-version\u0026gt; \\ --node-taints \u0026#34;os=windows:NoSchedule\u0026#34; Manual-installed Virtual Node # Azure-managed virtual node add-on for AKS does not natively support Windows containers.\nHowever, there is a way to use Windows containers with virtual nodes in AKS, through a manual installation process.\nSee https://github.com/virtual-kubelet/azure-aci/blob/master/docs/windows-virtual-node.md\nACR # Geo-replication is an exclusive feature available in the premium SKU only.\nIntegrate with AKS # az aks update -g myResourceGroup -n myAKSCluster --enable-managed-identity az aks update --name myAKSCluster --resource-group myResourceGroup --attach-acr \u0026lt;acr-name\u0026gt; With this command, AKS will configures the AcrPull role for the managed identity, allowing the AKS cluster to pull images from the specified ACR\nContainer App # Smallest subnet is /23. (It requires minimum of 512 addresses) restartPolicy: onFailure: Ensures that the container will restart if it crashes. ipAddress.type: public: Makes the container publicly accessible. apiVersion: 2019-12-01 type: Microsoft.ContainerInstance/containerGroups name: my-container-group location: eastus properties: containers: - name: my-container properties: image: nginx:latest ports: - port: 80 resources: requests: cpu: 1.0 memoryInGB: 1.5 restartPolicy: OnFailure ipAddress: type: Public ports: - protocol: TCP port: 80{ \u0026#34;type\u0026#34;: \u0026#34;Microsoft.ContainerInstance/containerGroups\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;2021-09-01\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;mycontainergroup\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;eastus\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;containers\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;mycontainer\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;image\u0026#34;: \u0026#34;mcr.microsoft.com/azuredocs/aci-helloworld:latest\u0026#34;, \u0026#34;ports\u0026#34;: [ { \u0026#34;port\u0026#34;: 80 } ], \u0026#34;resources\u0026#34;: { \u0026#34;requests\u0026#34;: { \u0026#34;cpu\u0026#34;: 1, \u0026#34;memoryInGB\u0026#34;: 1.5 } } } } ], \u0026#34;restartPolicy\u0026#34;: \u0026#34;OnFailure\u0026#34;, \u0026#34;ipAddress\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;Public\u0026#34;, \u0026#34;ports\u0026#34;: [ { \u0026#34;protocol\u0026#34;: \u0026#34;tcp\u0026#34;, \u0026#34;port\u0026#34;: 80 } ] }, \u0026#34;osType\u0026#34;: \u0026#34;Linux\u0026#34; } } Container Instance # DNS name label # The DNS name label scope reuse is only available in the public networking.\nThere are three levels of DNS name label reuse:\nTenant Subscription Resource Group When we create a container instance, it generates a DNS name label for it.\nThis label, known as the DNS name label, can be reused if you recreate the container, meaning it will either retain the same DNS name label (reuse) or assign a new one.\nFor example,\nThe DNS name label 23098djd in hugo-23098djd.\u0026lt;region\u0026gt;.azurecontainer.io is generated by Azure.\nIf we delete and recreate the container with the same name (hugo in this case), Azure will reuse the same 23098djd DNS name label.\nContainer Groups # Container Groups in ACI are similar to pods in Kubernetes.\nThey are the top-level resource in ACI and can contain one or more containers that are scheduled on the same host machine.\nWindows:\nLimited to single container per group Linux:\nSupport multiple contianers within single group { \u0026#34;name\u0026#34;: \u0026#34;myContainerGroup\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Microsoft.ContainerInstance/containerGroups\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;2023-05-01\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;[resourceGroup().location]\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;containers\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;container1\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;image\u0026#34;: \u0026#34;mcr.microsoft.com/azuredocs/aci-helloworld:latest\u0026#34;, \u0026#34;ports\u0026#34;: [ { \u0026#34;port\u0026#34;: 80 } ], \u0026#34;resources\u0026#34;: { \u0026#34;requests\u0026#34;: { \u0026#34;cpu\u0026#34;: 1, \u0026#34;memoryInGB\u0026#34;: 1.5 } } } }, { \u0026#34;name\u0026#34;: \u0026#34;container2\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;image\u0026#34;: \u0026#34;mcr.microsoft.com/azuredocs/aci-tutorial-sidecar\u0026#34;, \u0026#34;resources\u0026#34;: { \u0026#34;requests\u0026#34;: { \u0026#34;cpu\u0026#34;: 1, \u0026#34;memoryInGB\u0026#34;: 1.5 } } } } ], \u0026#34;osType\u0026#34;: \u0026#34;Linux\u0026#34;, \u0026#34;ipAddress\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;Public\u0026#34;, \u0026#34;ports\u0026#34;: [ { \u0026#34;protocol\u0026#34;: \u0026#34;TCP\u0026#34;, \u0026#34;port\u0026#34;: 80 } ] } } } ","date":"9 July 2024","externalUrl":null,"permalink":"/posts/azure-container-services/","section":"Posts","summary":"AKS # Azure AD Authentication # Legacy Method:\nCreate an app in EntraID manually Assign directory read role to the app Link the app with AKS az aks update-credentials \\ --resource-group myResourceGroup \\ --name myAKSCluster \\ --reset-aad \\ --aad-client-app-id \u0026lt;client-id\u0026gt; \\ --aad-server-app-id \u0026lt;server-id\u0026gt; \\ --aad-server-app-secret \u0026lt;server-secret\u0026gt; \\ --aad-tenant-id \u0026lt;tenant-id\u0026gt; Current Method:","title":"Azure Container Services","type":"posts"},{"content":" Instance Types # D series: General Purpose; S for SSD M / E series: Memory optimized F series: Compute optimized H series: HPC L series: Disk optimized N series: GPU (NVIDIA) DNS Formats # internal: \u0026lt;vm-name\u0026gt;.internal.cloudapp.azure.com public: \u0026lt;vm-name\u0026gt;.\u0026lt;region\u0026gt;.cloudapp.azure.com SetupComplete.cmd # Runs once during the Windows out-of-box experience (OOBE) setup Used for one-time configuration tasks after Windows installation Similar to cloud-init in Linux environments Location: %WINDIR%\\Setup\\Scripts\\SetupComplete.cmd Runs with SYSTEM privileges GPO - Logon Scripts: Runs every time a user logs into the computer (User Context) GPO - Start Scripts: Runs every time the computer starts (SYSTEM privileges)\nWhen deciding between these options:\nIf the task needs to be performed only once after installation, use SetupComplete.cmd For recurring tasks or configurations that need to be enforced regularly, use GPO scripts SetupComplete.cmd is useful in imaging and deployment situation where we want to perform actions immediately after installation Desired State Configuration # It is similar to Chef for Configuration Management.\nWe can enable DSC at \u0026ldquo;Extension\u0026rdquo; section\nWhen setup DSC, VM need to be in running state\nFor example,\nIt can be used for configuration management (install Nginx) on Windows\nSave InstallNginx.ps1\nConfiguration InstallNginx { Node \u0026#34;localhost\u0026#34; { Package Nginx { Ensure = \u0026#34;Present\u0026#34; Name = \u0026#34;nginx\u0026#34; Source = \u0026#34;https://nginx.org/packages/windows/\u0026#34; } } } Update InstallNginx.ps1.zip to storage account\nAdd DSC Extension to run the script\nSet-AzVMDscExtension -ResourceGroupName \u0026#34;MyResourceGroup\u0026#34; ` -VMName \u0026#34;MyVM\u0026#34; ` -ArchiveBlobName \u0026#34;InstallNginx.ps1.zip\u0026#34; ` -ArchiveStorageAccountName \u0026#34;mystorageaccount\u0026#34; ` -ConfigurationName \u0026#34;InstallNginx\u0026#34; ` -Version \u0026#34;2.76\u0026#34; Although we can Azure Custom Script Extension to install Nginx as well, DSC is more suitable for maintaining a desired configuration state, while Custom Script Extension is better for one-time script execution.\nRedeploy # Any data stored on the temporary disk will be lost when the VM is redeployed, resized / restarted.\nOn Windows, the temporary disk is assigned to drive D:\\`` On Linux, it's located at /dev/sdb1`\nImport VHDx to Azure # Convert the VHDx to VHD before importing it Ensure the VHD is in fixed-size format, not dynamically expanding. Convert VDHX to VHD\nConvert-VHD -Path \u0026#34;C:\\Path\\To\\Your\\DiskFile.vhdx\u0026#34; -DestinationPath \u0026#34;C:\\Path\\To\\Output\\DiskFile.vhd\u0026#34; -VHDType Fixed Verify the conversion\nGet-VHD -Path \u0026#34;C:\\Path\\To\\Output\\DiskFile.vhd\u0026#34; Import to Azure\nAdd-AzVhd -ResourceGroupName \u0026#34;MyResourceGroup\u0026#34; -LocalFilePath \u0026#34;C:\\Path\\To\\Output\\DiskFile.vhd\u0026#34; -Destination \u0026#34;https://mystorageaccount.blob.core.windows.net/vhds/DiskFile.vhd\u0026#34; Disk Operations # Data Disks:\nCan be detached from a running VM without stopping it. OS disk\nRequires the VM to be stopped and deallocated to make changes. Host Groups # Is a collection of dedicated hosts All hosts in a group must be of the same size (e.g., DSv3, ESv3) Located within a single Availability Zone in a region Dedicated Hosts:\nPhysical hosts in Azure datacenters dedicated to us (rent the whole physical server!) ScaleSet # There are two mode for managing VMSS:\nOrchestration: VMs are managed automatically by Azure VM: we need to manually add or remove VM instances Scaling Mechanisms # It supports both manual and automatic scaling based on defined rules and metrics.\nThere are 2 key concepts about the scaling:\nDuration: The period over which the metric is observed. Cooldown: The period to wait after a scale-out or scale-in action before another scaling action can occur For Example, we have scaling rule:\nIf CPU \u0026gt; 70% for 5 minutes, scale up by 1 instance. Minimum instances: 1 Maximum instances: 3 Default instances: 1 Cooldown period: 5 minutes CPU \u0026gt; 70% lasted for 15 minutes\n5min: Scale up by 1 instance (total: 2) 5-10min: 5 min cooldown period 10min: Scale up by 1 instance (total: 3) 10-15min: 5 min cooldown period 15min: No scale up happened as Max instances is reach Host Groups # We can create VMSS using host group\nThe scale set can only exist in a single AZ. As all hosts in a host group are in the same AZ To achieve HA across multiple AZs, we need to create multiple scale sets Each scale set would use a host group in a different AZ We can use Azure Load Balancer or an Application Gateway to distribute traffic across these multiple scale sets Planned Upgrade # No more than 20% of the scale set is upgrading at any time. For a scale set with 10 VMs, a maximum of 2 VMs are upgraded at a time, leaving 8 VMs operational. Placement Group # A placement group must be in the same region (location) as the scale set it\u0026rsquo;s associated with.\nAvailability Set # Availability sets:\nProvide high availability within a single data center Use fault domains and update domains to distribute VMs across different physical hardware Protect against hardware failures and planned maintenance events Availability zones:\nProvide HA across multiple data centers within a region Each zone is a separate physical location with independent power, cooling, and networking Protect against data center-level failures. Fault and Update Domains # Fault Domain is for unplanned outage Update Domain is for planned outage Limitation:\nWhen Fault Domains is set to 1, Update Domains can only be 1 If Fault Domains is \u0026gt; 2, Update Domains can be up to 20 For example:\nFault Domain: 2 Update Domain: 10 Total VMs: 11 Fault Domain VMs 1 2 2 1 3 1 4 1 5 1 6 1 7 1 8 1 9 1 10 1 Max no of unavailable VMs during planned maintenance is 2 VMs (as 1: 2 VMs)\nFault Domain VMs Distribution 1 6 VMs 2 5 VMs Max no of unavailable VMs during unplanned maintenance is 6 VMs (as 1: 6 VMs)\nResizing Availability Set # Stop and deallocate all VMs in the availability set Resize the availability set Restart the VMs ","date":"9 July 2024","externalUrl":null,"permalink":"/posts/azure-vm/","section":"Posts","summary":"Instance Types # D series: General Purpose; S for SSD M / E series: Memory optimized F series: Compute optimized H series: HPC L series: Disk optimized N series: GPU (NVIDIA) DNS Formats # internal: \u0026lt;vm-name\u0026gt;.","title":"Azure VM","type":"posts"},{"content":"","date":"9 July 2024","externalUrl":null,"permalink":"/tags/container-app/","section":"Tags","summary":"","title":"container app","type":"tags"},{"content":"","date":"9 July 2024","externalUrl":null,"permalink":"/tags/container-instance/","section":"Tags","summary":"","title":"container instance","type":"tags"},{"content":"","date":"9 July 2024","externalUrl":null,"permalink":"/tags/vm/","section":"Tags","summary":"","title":"vm","type":"tags"},{"content":"","date":"6 July 2024","externalUrl":null,"permalink":"/tags/arm-template/","section":"Tags","summary":"","title":"arm template","type":"tags"},{"content":"We can view resources deployed via ARM templates at:\nResource Group \u0026gt; Deployments When using count, the copyIndex() function starts counting from 0.\n\u0026#34;resources\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;Microsoft.Storage/storageAccounts\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;[concat(\u0026#39;storage\u0026#39;, copyIndex())]\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;2022-01-01\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;[resourceGroup().location]\u0026#34;, \u0026#34;count\u0026#34;: 3 } ] This example will deploy 3 storage accounts named storage0, storage1, and storage2.\nWe can use resourceId() to reference the existing resource. The following example show how to reference a resource named VM1Nic1\n{ \u0026#34;name\u0026#34;: \u0026#34;VM1\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Microsoft.Compute/virtualMachines\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;2022-01-01\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;[parameters(\u0026#39;vmLocation\u0026#39;)]\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;networkProfile\u0026#34;: { \u0026#34;networkInterfaces\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;[resourceId(\u0026#39;Microsoft.Network/networkInterfaces\u0026#39;, \u0026#39;VM1Nic1\u0026#39;)]\u0026#34; } ] } } } ImageReference is used to choose the OS image when creating VM.\nTo find the values for publisher, offer, sku, and version, I usually simply pretend to create a VM in Azure Portal and look up these values during the setup process.\n{ \u0026#34;name\u0026#34;: \u0026#34;VM1\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Microsoft.Compute/virtualMachines\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;2022-01-01\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;[parameters(\u0026#39;vmLocation\u0026#39;)]\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;storageProfile\u0026#34;: { \u0026#34;imageReference\u0026#34;: { \u0026#34;publisher\u0026#34;: \u0026#34;MicrosoftWindowsServer\u0026#34;, \u0026#34;offer\u0026#34;: \u0026#34;WindowsServer\u0026#34;, \u0026#34;sku\u0026#34;: \u0026#34;2019-Datacenter\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;latest\u0026#34; } } } } Differences between variables and parameteers:\nParameters: Customizable. We can provide these values when we use the template or script. Variables: These values are calculated based on the parameters or other logic. This example show that we can use parameters to change which vmSize use to deploy. and the storage account name is concat from storage + resource group id\n{ \u0026#34;$schema\u0026#34;: \u0026#34;https://schema.management.azure.com/schemas/2019-04-01/deploymentTemplate.json#\u0026#34;, \u0026#34;contentVersion\u0026#34;: \u0026#34;1.0.0.0\u0026#34;, \u0026#34;parameters\u0026#34;: { \u0026#34;vmSize\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;Standard_DS1_v2\u0026#34;, \u0026#34;allowedValues\u0026#34;: [ \u0026#34;Standard_DS1_v2\u0026#34;, \u0026#34;Standard_DS2_v2\u0026#34; ], \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;The size of the virtual machine.\u0026#34; } } }, \u0026#34;variables\u0026#34;: { \u0026#34;storageAccountName\u0026#34;: \u0026#34;[concat(\u0026#39;storage\u0026#39;, uniqueString(resourceGroup().id))]\u0026#34; }, \u0026#34;resources\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;Microsoft.Compute/virtualMachines\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;2020-06-01\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;[concat(\u0026#39;vm\u0026#39;, uniqueString(resourceGroup().id))]\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;[resourceGroup().location]\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;hardwareProfile\u0026#34;: { \u0026#34;vmSize\u0026#34;: \u0026#34;[parameters(\u0026#39;vmSize\u0026#39;)]\u0026#34; }, \u0026#34;storageProfile\u0026#34;: { \u0026#34;osDisk\u0026#34;: { \u0026#34;createOption\u0026#34;: \u0026#34;FromImage\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;[variables(\u0026#39;storageAccountName\u0026#39;)]\u0026#34; } } } } ] } Powershell # We can deploy an ARM template at different levels:\nManagement Group: Use New-AzManagementGroupDeployment. Subscription: Use New-AzSubscriptionDeployment. Resource Group: Use New-AzResourceGroupDeployment. When deploying with New-AzResourceGroupDeployment, there are two modes:\nComplete: Deletes resources in the resource group that are not specified in the template. Incremental: Retains existing resources in the resource group that are not specified in the template. Example usage:\nNew-AzResourceGroupDeployment -TemplateUri xxx -TemplateParameterFile params.json -ResourceGroupName RG1 -Mode Complete Regarding New-AzSubscriptionDeployment, the -Location parameter only affects where the ARM template itself is stored, not the location of the resources being deployed.\n$templateFile = \u0026#34;path/to/your/template.json\u0026#34; $parameterFile = \u0026#34;path/to/your/parameters.json\u0026#34; New-AzSubscriptionDeployment ` -Name \u0026#34;ExampleDeployment\u0026#34; ` -Location \u0026#34;eastus\u0026#34; ` -TemplateFile $templateFile ` -TemplateParameterFile $parameterFile To deploy a resource within a subscription, we cannot do it directly. We need to wrap it within a deployment resource.\n{ \u0026#34;$schema\u0026#34;: \u0026#34;https://schema.management.azure.com/schemas/2019-04-01/deploymentTemplate.json#\u0026#34;, \u0026#34;contentVersion\u0026#34;: \u0026#34;1.0.0.0\u0026#34;, \u0026#34;resources\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;Microsoft.Resources/resourceGroups\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;2021-04-01\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;[parameters(\u0026#39;location\u0026#39;)]\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;[parameters(\u0026#39;resourceGroupName\u0026#39;)]\u0026#34;, \u0026#34;properties\u0026#34;: {} }, { \u0026#34;type\u0026#34;: \u0026#34;Microsoft.Resources/deployments\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;2021-04-01\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;storageAccountDeployment\u0026#34;, \u0026#34;resourceGroup\u0026#34;: \u0026#34;[parameters(\u0026#39;resourceGroupName\u0026#39;)]\u0026#34;, \u0026#34;dependsOn\u0026#34;: [ \u0026#34;[resourceId(\u0026#39;Microsoft.Resources/resourceGroups\u0026#39;, parameters(\u0026#39;resourceGroupName\u0026#39;))]\u0026#34; ], \u0026#34;properties\u0026#34;: { \u0026#34;mode\u0026#34;: \u0026#34;Incremental\u0026#34;, \u0026#34;template\u0026#34;: { \u0026#34;$schema\u0026#34;: \u0026#34;https://schema.management.azure.com/schemas/2019-04-01/deploymentTemplate.json#\u0026#34;, \u0026#34;contentVersion\u0026#34;: \u0026#34;1.0.0.0\u0026#34;, \u0026#34;resources\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;Microsoft.Storage/storageAccounts\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;2021-04-01\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;[parameters(\u0026#39;storageAccountName\u0026#39;)]\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;[parameters(\u0026#39;location\u0026#39;)]\u0026#34;, \u0026#34;sku\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Standard_LRS\u0026#34; }, \u0026#34;kind\u0026#34;: \u0026#34;StorageV2\u0026#34;, \u0026#34;properties\u0026#34;: {} } ] } } } ], \u0026#34;parameters\u0026#34;: { \u0026#34;location\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;eastus\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Location for the resources.\u0026#34; } }, \u0026#34;resourceGroupName\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Name of the resource group to create.\u0026#34; } }, \u0026#34;storageAccountName\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Name of the storage account to create.\u0026#34; } } } } Template Library # To create template:\nNavigate to an existing VM in the Azure portal Select \u0026ldquo;Export template\u0026rdquo; from the left menu Review the generated template Click \u0026ldquo;Add to library\u0026rdquo; to save it as a Template Spec To use the Template:\nGo to the desired Resource Group Click \u0026ldquo;New\u0026rdquo; or \u0026ldquo;Add\u0026rdquo; Search for and select \u0026ldquo;Template deployment\u0026rdquo; In \u0026ldquo;Template Source\u0026rdquo;, select \u0026ldquo;Template spec\u0026rdquo; Choose the saved template Fill in any required parameters Review and create OR\nNavigate to \u0026ldquo;Template specs\u0026rdquo; in the Azure portal Select the specific template that want to deploy Click \u0026ldquo;Deploy\u0026rdquo; Choose the target subscription and resource group Fill in any required parameters Review and deploy Virtual Machine Extensions # To domain join AD in a VM, we need to deploy Virtual Machine Extensions (Microsoft.Compute/virtualMachines/extensions) with the extension type JsonADDomainExtension in Template.\nIn the settings, provide\ndomain name username for domain joining OUpath In the protectedSettings, proivde\ndomain join account password { \u0026#34;$schema\u0026#34;: \u0026#34;https://schema.management.azure.com/schemas/2019-04-01/deploymentTemplate.json#\u0026#34;, \u0026#34;contentVersion\u0026#34;: \u0026#34;1.0.0.0\u0026#34;, \u0026#34;parameters\u0026#34;: { \u0026#34;vmName\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, \u0026#34;domainName\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, \u0026#34;domainUsername\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, \u0026#34;domainPassword\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;securestring\u0026#34; }, \u0026#34;ouPath\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;\u0026#34; } }, \u0026#34;resources\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;Microsoft.Compute/virtualMachines/extensions\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;2021-03-01\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;[concat(parameters(\u0026#39;vmName\u0026#39;), \u0026#39;/JsonADDomainExtension\u0026#39;)]\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;[resourceGroup().location]\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;publisher\u0026#34;: \u0026#34;Microsoft.Compute\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;JsonADDomainExtension\u0026#34;, \u0026#34;typeHandlerVersion\u0026#34;: \u0026#34;1.3\u0026#34;, \u0026#34;autoUpgradeMinorVersion\u0026#34;: true, \u0026#34;settings\u0026#34;: { \u0026#34;Name\u0026#34;: \u0026#34;[parameters(\u0026#39;domainName\u0026#39;)]\u0026#34;, \u0026#34;User\u0026#34;: \u0026#34;[parameters(\u0026#39;domainUsername\u0026#39;)]\u0026#34;, \u0026#34;Restart\u0026#34;: \u0026#34;true\u0026#34;, \u0026#34;Options\u0026#34;: \u0026#34;3\u0026#34;, \u0026#34;OUPath\u0026#34;: \u0026#34;[parameters(\u0026#39;ouPath\u0026#39;)]\u0026#34; }, \u0026#34;protectedSettings\u0026#34;: { \u0026#34;Password\u0026#34;: \u0026#34;[parameters(\u0026#39;domainPassword\u0026#39;)]\u0026#34; } } } ] } ","date":"6 July 2024","externalUrl":null,"permalink":"/posts/azure-arm-template/","section":"Posts","summary":"We can view resources deployed via ARM templates at:\nResource Group \u0026gt; Deployments When using count, the copyIndex() function starts counting from 0.\n\u0026#34;resources\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;Microsoft.Storage/storageAccounts\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;[concat(\u0026#39;storage\u0026#39;, copyIndex())]\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;2022-01-01\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;[resourceGroup().","title":"Azure ARM Template","type":"posts"},{"content":" Recovery Service Vault # The Vault needs to be in the same region as the VM.\nThe vault includes two primary services:\nBackup Service Recovery Service (for Disaster Recovery) Backup:\nTypically has longer Recovery Point Objective (RPO) and Recovery Time Objective (RTO) Backup frequency options: Hourly (every 4 hours) Daily Weekly Longer RPO generally means more data to back up, which can lead to a longer RTO Supports long retention periods, up to 10 years Recovery:\nPrimarily used for Failover and Disaster Recovery (DR) scenarios Usually has shorter RPO and RTO (0-12 snapshots in hours) Supports recovery between: Azure to Azure (different regions) On-premises to Azure Azure to on-premises Backup # There are three items that can be backed up:\nVM SQL on VM File Shares Steps to Create a Backup on VM: # Create a Recovery Services Vault Configure Storage Replication Type to ZRS Create a Backup Policy Define the schedule Set retention rules Configure Backup for VM Backup Report # We can configure backup report at Diagnostic setting.\nThere are two main options to store the data:\nStorage account Must be in the same region as Vault Helps minimize latency Log analytics Can be in a different region from Vault allows for centralized logging across multiple regions Backup Agent # When configuring the backup on a VM, a backup agent will be deployed to the VM.\nFor Windows VM,\nThe Recovery Services Agent (MARSA) is used. For Linux VM,\nThe Azure VM Backup extension is used. Cross Region Restore (CRR) # CRR is an opt-in feature that allows data recovery in a secondary region if the primary region is unavailable.\nIt needs to be enabled at the vault level.\nOnce enabled, all subsequent backups are automatically replicated to the secondary region.\nSupported Backup Types for CRR:\nMARS agent backups Azure VM Azure Files share Backup Policy # Backup policies cannot be shared between different Recovery Services Vaults. Each vault have its own backup policy. (can\u0026rsquo;t share between vault) Enhanced Policy Features:\nSupports multiple backups per day 30-day tier retention Supports trusted launch VMs Supports SSH v2 and Ultra Disk Standard Policy Features:\nSupports one backup per day 5-day tier retention Restore backup # Restore backup to existing vm, it will NOT overwrite the\ninstance size change admin password change And\ndata disk will become unattached data on tmp disk (D:\\ by default) will be lost data on OS disk will be restored To restore a file\nClick File Recovery (vs Restore VM) Select a restore point Download and run the script that mounts the disk from the selected recovery point as local drive Copy the desired file using file explorer Site Replication (DR) # To Replicate On-Premise Servers to Azure:\nPhysical Host:\nCreate Azure Recovery Services Vault Install Appliance and Register the Server Create Storage Account Configure Replication Policy Enable Replication Hyper-V Server:\nCreate Azure Recovery Services Vault Create Hyper-V Site Install \u0026ldquo;Site Recovery Provider\u0026rdquo; and Register the Server into the Hyper-V Site Configure Replication Policy (e.g., interval) Enable Replication When replicating a VM using Site Recovery:\nThe VM will replicate to the subnet with the same name as the source subnet. If no matching subnet name is found, it will replicate to the first subnet in the target network, sorted in alphabetical order. Delete Vault # Stop Backup Disable Soft Delete and Immutable Vault Delete Backup Data SQL VM File Share Remove Linkage of Resources and Storage Account Stop Replication Backup Vault # Recovery Service Vault supports VM, SQL on VM and File Share.\nWhile Backup Vault supports:\nDisk Blob PostgreSQL / MySQL AKS Steps to Create a Backup on Disk: # Create a Backup Vault Create a Backup Policy Define Schedule Set Retention Choose Policy Add Data Sources (Disk) At this stage, we can click \u0026ldquo;assign missing roles\u0026rdquo; to add necessary roles to the disk Required Roles # Disk Reader Snapshot Contributor Backup at App Service # Setting retention to 0 days means the backup is kept indefinitely.\nThe backup of a web app includes only the production slot. We cannot restore a test slot from backup.\nThe storage account used for backing up in App Service must be a Blob Storage.\n","date":"6 July 2024","externalUrl":null,"permalink":"/posts/azure-backup/","section":"Posts","summary":"Recovery Service Vault # The Vault needs to be in the same region as the VM.\nThe vault includes two primary services:\nBackup Service Recovery Service (for Disaster Recovery) Backup:","title":"Azure Backup","type":"posts"},{"content":" Resource Mover # We can move a resource between resource groups within the same subscription or to a different subscription.\nThe resource\u0026rsquo;s location will stay the same during the move.\nHowever, Azure policies might change since they depend on the subscription, resource group, or management group the resource is part of.\nSupported Resources:\nVM VNET, NSG, IP, NICS Managed Disks Availability Sets App Service Plan and App Storage Accounts Recovery Services Vaults SQL Cosmos DB Unsupported Resources:\nLoad Balancer AKS Domain Services Batch accounts Redis Data Factory ExpressRoute circuits Deletion # There are two common factors that prevent the deletion of a resource group:\nResource Lock Recovery Services Vault For Resource Lock,\nCheck for locks at both the Resource Group and Recovery Service Vault Remove any existing locks For Recovery Service Vault:\nBackup Items (including soft-deleted items): Stop the Backup at Backup Item. Diable soft delete. Delete backup data. Protected Resources: Stop protection on VMs Linked Storage Accounts: Unregister any linked storage accounts under Backup Infrastructure \u0026gt; Storage Account Private Endpoints: Remove any associated private endpoints. Replication Data: Stop replication and remove any replicated items. ","date":"6 July 2024","externalUrl":null,"permalink":"/posts/azure-resource-group/","section":"Posts","summary":"Resource Mover # We can move a resource between resource groups within the same subscription or to a different subscription.\nThe resource\u0026rsquo;s location will stay the same during the move.","title":"Azure Resource Group","type":"posts"},{"content":"","date":"6 July 2024","externalUrl":null,"permalink":"/tags/backup-service/","section":"Tags","summary":"","title":"backup service","type":"tags"},{"content":"","date":"6 July 2024","externalUrl":null,"permalink":"/tags/recovery-service/","section":"Tags","summary":"","title":"recovery service","type":"tags"},{"content":"","date":"6 July 2024","externalUrl":null,"permalink":"/tags/resource-group/","section":"Tags","summary":"","title":"resource group","type":"tags"},{"content":" Network Interface # A single NIC can have multiple public and private IPs. Network Security Group (NSG) # NSG is stateful and contains inbound and outbound rules.\nService Tags are named groups that refer to Azure services (e.g., AzurePortal, AzureLoadBalancer).\nAn NSG can be attached to:\nSubnets Network Interfaces Order of NSG rules check:\nInbound: Subnet \u0026gt; NIC Outbound: NIC \u0026gt; Subnet A single NSG can be attached to multiple subnets or network interfaces.\nBy default, NSG allows:\nAll traffic within the VNET. Traffic from the load balancer. Additional points:\nOnce traffic is denied, it is dropped. If NSG is attached to both subnet and NIC, traffic must be allowed in both NSGs. NSG must be in the same region as the VNET. To Check NSG for SSH Connectivity (e.g., Can VM1 SSH into VM2?)\nIdentify Source and Destination: VM1 is the source. VM2 is the destination. Check NSG Rules: For the source (VM1), look at the outbound rules. For the destination (VM2), look at the inbound rules. Route Table # Imagine we have VNETs A, B, and C that are peered together. VNET C acts as a network VNET to firewall, guard, and audit the network traffic. We want any egress public traffic from VNETs A and B to route through VNET C first.\nTo achieve this, we can add a route rule in the route tables of VNETs A and B with the address prefix 0.0.0.0/0, next hop type set to appliance, and the next hop address set to the appliance's address.\nIf we want to route all VPN traffic through VNET C, we can set a route table rule at the GatewaySubnet.\nSubnet # A subnet can have multiple address spaces, and additional address spaces can be added after the subnet has been created.\nAzure reserves the first four and last IP addresses (5 in total) in each subnet.\nThe smallest subnet size in a VNet is /29.\nNetwork Diagnose Tools # NSG issue # IP Flow Verify (VM) NSG Diagnostics (ScaleSet, NIC, Application Gateway) Connectivity issue # Connection Troubleshoot (VMs, Bastion, and Application Gateway) Connection monitor # Connection Monitor is a regional service available in each Azure region.\nIt continuously checks connections over time.\nTo utilize Connection Monitor:\nAzure VMs: Install the Network Watcher Agent. On-premises: Install the Azure Monitor Agent. Additionally, use the Azure Connected Machine agent to connect the VMs on Azure. This setup allows for proactive monitoring of connections, ensuring continuous visibility and troubleshooting capabilities across Azure environments and on-premises infrastructure.\nFlow Log # Enabling Flow Log requires\nStorage Account Register Insight Provider (At Subscription) Enabling Traffic Analytics requires\nFlow Log Log Analytics Workspace Private Link in Azure Monitor # \u0026ldquo;Monitor Private Link Scope\u0026rdquo; refers to a collection of private links.\nTo use a private connection in Azure Monitor, it is necessary to establish private link connections because Azure Monitor integrates with various services. The \u0026ldquo;Monitor Private Link Scope\u0026rdquo; is essential for this purpose.\nApp Service # Integrating with a VNET provides egress connectivity to the VNET.\nApp Service will be assigned a private IP, which it uses to access services within the VNET.\nHowever, this assignment is dynamic and can change during scaling operations.\nInbound traffic and internet traffic will still use the public IP.\nNSGs / Firewall can be utilized to control the App Service traffic flow into the VNET.\nAzure Load Balancer # The SKU of the Load Balancer must match that of the public IP:\nStandard public IP addresses must be used with Standard Load Balancers. Basic public IP addresses are compatible only with scale sets or availability sets. To create the load balancer, a frontend IP is required.\nTo set up the load balancing rule, a backend pool and a health probe need to be configured.\nThe load balancer and VMs need to be in the same VNET.\nNSG # Traffic originating from the LB always appears to come from 168.63.129.16.\nWhen VMs are behind the LB, NSG rules must explicitly specify the source as 168.63.129.16 to allow traffic.\nBy default, traffic from 168.63.129.16 (Serivce Tag: AzureLoadBalancer) is allowed, so NSG rules are typically used to deny traffic.\nNAT Rules # Load Balancer are used for distributing traffic. Interestingly, in Azure, they can also act as a NAT.\nInbound NAT Rule:\nAllows the outside network to reach the backend point or VMs behind the backend pool To connect to VM1 and VM2 via the Load Balancer on same port, we need 2 frontend IPs: \u0026lt;frontend_ip1\u0026gt;:8080 -\u0026gt; VM1:8080 \u0026lt;frontend_ip2\u0026gt;:8080 -\u0026gt; VM2:8080 Frontend IPs can be mapped to a backend pool using a single frontend IP, where the frontend IP starts with a base port that increments for each mapping: \u0026lt;frontend_ip1\u0026gt;:8081 -\u0026gt; VM1:8080 \u0026lt;frontend_ip1\u0026gt;:8082 -\u0026gt; VM2:8080 \u0026lt;frontend_ip1\u0026gt;:X -\u0026gt; VMX:8080 Outbound Rules:\nMultiple VMs in the backend pool can share a single frontend IP for outbound traffic. Floating IP # Disabled: Traffic routes through the load balancer to the VM\u0026rsquo;s IP. Enabled: Traffic routes through the load balancer to the VM\u0026rsquo;s Floating IP (FIP). In an on-premise Active-Passive HA setup, a VIP is typically used, to which clients always connect. Monitoring tools check server health and shift the VIP between active and passive instances. The VM will respond to ARP requests for the VIP when it holds it.\nIn the cloud, ARP and other broadcast messages are not permitted, necessitating the use of a load balancer.\nTo set up a Floating IP in Linux:\nBasically, create a loopback interface with the Floating IP. This setup is commonly used to achieve High Availability in SQL Server, where a Floating IP is needed for direct server return.\nHA Port # Configuration: Set port to 0 and protocol to all in the load balancing rule. This directs all ports and protocols to the VM directly\nDue to this, each floating IP can only have 1 load balancer rule and connect to 1 backend.\nThis feature is for:\nInternal Load Balancer only Standard tier only Use Case: Virtual Appliance (eg: Firewall):\nConfigure two floating IPs and HA ports on the virtual appliance to tag traffic from different services. HA port is also required to inspect traffic for any protocol and any port. Floating IP is necessary for HA and to distinguish the traffic from different service, such as: Route all Web Server subnet traffic to FIP1. Route all DB subnet traffic to FIP2. Requests are first forwarded to the virtual appliance (acting as a man in the middle) before being sent to the destination. Session Persistence # Session Persistence based on Client IP ensures that traffic from the same client is always served by the same VM.\nVM # A VM without a public IP can still access the internet.\nFor a VM with standard public IP:\nSecurity settings enforce blocking all traffic by default. You must attach a NSG to allow traffic into the VM. For a VM with basic public IP:\nThe VM is accessible publicly without needing an NSG attached. A VM can change its subnet or NIC but not its VNET. To change the VNET or move the VM to another region, we need to recreate the VM.\nBastion # Bastion must be deployed to a subnet named AzureBastionSubnet.\nThe subnet must be /26 or larger.\nBastion protects VMs only.\nBasic SKU:\nCreates 2 instances. Standard SKU:\nAllows specifying the number of instances (host scaling). Supports file upload/download. Supports native client (use Azure CLI for SSH/RDP connections). Public IP requirements:\nCan be regional or global (to be confirmed). Must be static. Requires Standard SKU. Without the native client, we can only use Bastion through the Azure portal.\nEach instance supports up to 20 RDP or 40 SSH connections.\nFor example, with 100 concurrent SSH connections, we need 3 instances, which means we need the Standard Bastion SKU and host scaling set to 3.\nResizing the subnet is unnecessary since we only need additional instances.\nExpressroute # The Basic SKU does not support the coexistence of VPN Gateway and ExpressRoute. The ErGw3Az SKU supports FastPath. VNET Peering # With VNET Peering, the VNET address spaces must not overlap.\nWhen peering is disconnected, we need to recreate the peering connections.\nWhen transit routing is enabled in a VNET, traffic can transit through it to reach other VNETs.\nFor example, if we have VNET1, VNET2, and VNET3:\nInitially, VNET1 cannot access VNET3. If transit routing is enabled in VNET2, VNET1 can access VNET3 through VNET2. One key difference between S2S VPN and VNET Peering is that VNET Peering does not require creating an extra GatewaySubnet to connect two VNETs.\nFirewall # Firewall Premium supports public IP with:\nStandard SKU Global \u0026amp; Regional? (to be confirmed) IPv4 Only Policy:\nParent policies are often used to enforce organization-wide security standards. Child policies allow for environment-specific or application-specific rules. Rules in child policies take precedence over rules in the parent policy. Parent and child policies need to be in the same region. The region of the firewall should be the same as the region of the VNET.\nIn App Service, to control outbound traffic with a firewall, we need to enable VNET integration and then create a user-defined route table to route all traffic to the firewall.\nService Endpoint # Service Endpoints use private IP addresses to access Azure services, simplifying access control with ACLs.\nVNET and Service Endpoint must be in the same region.\nIt is in the Subnet level.\nVirtualWAN # Virtual WAN (similar to AWS Transit Gateway) is a global resource that can span across regions, while Virtual Hubs are regional resources.\nSingle Virtual WAN can connect to multiple Virtual Hubs.\nBasic VirtualWAN has limited functionality compare to the standard tier:\nOnly support S2S VPN Does not support ExpressRoute connections Does not support P2P connections To create a S2S VPN between a branch and a VNET:\nCreate a Virtual WAN. Create a Virtual Hub. Create a VPN site in the Virtual Hub. Connect the VPN site to the Hub. Connect the VNET to the Hub. VPN # S2S VPN # The Basic SKU VPN Gateway does not support coexistence with ExpressRoute for S2S VPN.\nSteps to Create a S2S VPN\nCreate a gateway subnet. Create a VPN gateway. Create a local gateway (representing the on-premise site). Create a VPN connection. Each VPN gateway can support 2 connections for failover, requiring 2 local gateways (for 2 on-premise routers) and using up 2 public IP addresses for the gateway.\nIKEv2 supports up to 10 S2S connections. IKEv1 supports only 1 S2S connection. IKEv2 is necessary if redundancy is required.\nP2S VPN # When to Use # P2S VPN: Suitable for remote workers, as the IP addresses connecting to the VNET can vary. S2S VPN: Suitable for branch offices, connecting on-premise networks to the VNET with fixed IPs. Authentication # P2S VPN can authenticate using\ncertificate authentication Active Directory To setup Certificate Authentication:\nGenerate a CA and client certifcate. Upload the CA public key to the P2S VPN Setting. Connect to the VPN using client certifcate. Common issue # The on-premise network is connected to VNET A via a S2S VPN. Clients connect to VNET A using a P2S VPN. VNET A is peered with VNET B. P2S VPN clients may fail to access VNET B when the network topology changes, necessitating a reinstall or restart of the VPN client.\nIn this setup, the client cannot access VNET B, while the on-premise network can, indicating this issue.\nVPN Types # Route-based VPN: Used for large networks; uses a route table. Policy-based VPN: Used for small networks; uses ACLs, requiring specific source and destination addresses. Since usually P2S VPN clients do not have fixed IPs, it is difficult to use ACLs (as the source cannot be specified). Therefore, route-based VPN is usually used for P2S VPNs. Certainly! Here\u0026rsquo;s a rewritten version:\nDNS # DNS settings can be configured at the\nVNET NIC We can either use custom DNS or Azure DNS.\nTo use custom DNS, we can\nDeploy DNS server in VNET Config DNS server to forward Azure Domain to Azure DNS (168.63.129.16). This ensures that Azure related names can be resolved correctly. Config the DNS setting at VNET / NIC with the custom DNS server IP Custom Domains # Update NS records at your domain registrar. Verify domain ownership using a TXT record. Configure your custom domain with a CNAME or A record. Zone Migration # Azure offers PowerShell scripts to facilitate DNS zone migration, useful for transitioning from on-premises or other cloud providers.\nPrivate DNS Zones # Private DNS zones are necessary for connecting with Virtual Networks (VNETs). One DNS zone can be connected to multiple VNETs, and multiple resolution zones can be linked to a single VNET. Auto-registration automated the creation of DNS records for linked VNETs. Auto-registration is only supported within private DNS zones. Each VNET can designate one registration zone with auto-registration enabled. Custom DNS Server # Virtual Machines (VMs) can function as custom DNS servers. Each VNET can opt for Azure DNS or a custom DNS setup. Ensure connectivity to the VM when using it as a DNS server. Application Gateway # similar to AWS\u0026rsquo;s ALB Level 7 Application Load Balancer Regional Service Traffic Manager # DNS-based traffic routing service Global service, not tied to a specific region Can handle regional outages by redirecting traffic to healthy endpoints Premium version offers additional features for private networking and security Frontdoor # Similar to AWS CloudFront Global service, not tied to a specific region Rate limiting can be implemented using Frontdoor (Premium) and WAF Can terminate SSL certificates, similar to Application Gateway ","date":"3 July 2024","externalUrl":null,"permalink":"/posts/azure-network/","section":"Posts","summary":"Network Interface # A single NIC can have multiple public and private IPs. Network Security Group (NSG) # NSG is stateful and contains inbound and outbound rules.\nService Tags are named groups that refer to Azure services (e.","title":"Azure Networking","type":"posts"},{"content":"","date":"3 July 2024","externalUrl":null,"permalink":"/tags/networking/","section":"Tags","summary":"","title":"networking","type":"tags"},{"content":" IAM Roles # Access rights to Azure resources:\nOwner: Full access, including assigning roles and policies. User Access Administrator: Assign roles/policies. Contributor: Create and manage (read and write) resources. Reader: View resources (read-only). Co-admin (legacy):\nA co-admin can only be added at the subscription level. A co-admin cannot be assigned to a management group, resource group or resource. A co-admin has full access to all resources within the subscription. Logic Apps # Logic App Standard Developer: Develop and edit workflows/connections. Logic App Operator: Operate and manage workflows (enable/disable, resubmit, create connections). Storage Account # To view files in a storage account, we need the Reader role.\nThe Contributor role allows us to update storage account settings and create containers.\nHowever, the Reader and Contributor roles do not include DataActions, so they cannot perform operations on the files level. (such as upload / download / delete)\nTo read/write files or blobs, we need the following Data roles:\nStorage Blob Data: For blob. Storage File Data: For file share. Storage File Data SMB: For file share in SMB. Role Name Read Access Write Access Delete Access Modify Windows ACLs Override Existing Windows ACLs Storage File Data Privileged Contributor Yes Yes Yes Yes Yes Storage File Data Privileged Reader Yes No No No Yes Storage File Data SMB Share Contributor Yes Yes Yes No No Storage File Data SMB Share Elevated Contributor Yes Yes Yes Yes No Storage File Data SMB Share Reader Yes No No No No By default, File Share doesn\u0026rsquo;t work with Azure AD. You use an account key to connect to SMB.\nTo use Azure AD for authentication:\nTurn on Azure AD integration Choose \u0026ldquo;Enable permission for all authenticated users and groups\u0026rdquo; Pic a default role for the share When using SMB, two types of permissions apply:\nAzure RBAC Windows ACL The stricter permission wins. For example:\nAn SMB Share Contributor can read and write files But Windows ACL can override this An Elevated Contributor can change Windows ACL (except ownership) Some files might still be inaccessible even for a Contributor When backing disk to a storage account, we need to enable the \u0026ldquo;Allow trusted Microsoft services to access this storage account\u0026rdquo; option.\nOthers # Monitor Contributor/Network Contributor: Need to enable traffic analytics. Virtual Machine User Login: Login to VMs. Disk Snapshot Contributor: Manage disk snapshots. Website Contributor: Deploy content in web apps. DevTest Lab User: Use DevTest Labs. Resource Policy Contributor: Create and assign Azure policies. Entra Roles # Entra roles focus on managing Azure AD and don\u0026rsquo;t have direct rights to manage Azure resources.\nBut we can temporarily elevate a Global Administrator to manage the resources:\nSign in as a Global Administrator. Navigate to Azure Active Directory \u0026gt; Properties \u0026gt; Manage. Enable \u0026ldquo;Access management for Azure resources\u0026rdquo; Notice that:\nThis elevation should be temporary, only for making necessary changes to Azure Resources. It\u0026rsquo;s a backdoor option, meant for use only if access to resources is lost. This elevation applies only to the current signed-in user, not to all Global Administrators. Key Roles in Entra:\nGlobal Administrator: Has full control over the entire tenant, including the ability to manage all aspects of users and services.\nUser Administrator: Can create and manage users, but lacks the ability to control administrative accounts.\nSecurity Administrator: Manages security settings such as editing and viewing security policies, and handling alerts and recommendations.\nCloud Device Administrator: Manages devices within Azure AD, including enabling, disabling, and deleting devices, as well as viewing BitLocker keys. They cannot add new devices to the network.\nIntune Device Administrator: Manages devices specifically within Microsoft Intune, focusing on device and application management such as Mobile Device Management (MDM) and Mobile Application Management (MAM).\nCustom Roles # Custom Roles allow us to define specific permissions that aren\u0026rsquo;t covered by built-in roles.\nComponents of a custom role include:\nScopes Permissions: Actions NotActions DataActions NotDataActions A custom role can be assigned multiple scopes that define its applicability.\nThe scope and permissions of a custom role can be modified even after its creation.\nScopes # Subscription: /subscription/\u0026lt;subscription\u0026gt; Resource Group: /subscription/\u0026lt;subscription\u0026gt;/resourceGroups/\u0026lt;resource group\u0026gt; Note: Wildcards (*) are not allowed in the scope. Actions # Microsoft.Authorization/*: Management of access permissions (role assignment, access review, policy) for a resource group. Role Cloning # Cannot be Cloned:\nBuilt-in Azure AD roles (roles in EntraID) Can be Cloned:\nBuilt-in subscription roles (roles in IAM) Custom roles whether in EntraID / IAM Endpoint policy # Service Endpoints use private IP addresses to access Azure services, simplifying access control with ACLs.\nFor example,\nNormally, a VM accessing a storage account is seen from a public IP address. With a Service Endpoint and policy, the VM uses a private IP, allowing firewall rules to restrict access. While Service Endpoints can be used with various Azure services, endpoint policies currently offer extra control only for Storage Accounts.\nVNET and Service Endpoint Policy must be in the same region. Without a policy, a subnet can access any storage account. Policies can cover: All accounts in a subscription All accounts in a resource group A single account One service endpoint can have multiple policies. Azure Policy # Assignment # Policies can be assigned to:\nRoot Management Group Management Groups Subscriptions Resource Groups Resources We can specify that a policy applies to some scopes except for specific ones. However, the Root Management group cannot be selected for the exclusion.\nWhen creating a policy assignment using the Azure portal, system-assigned managed identity will be automatic created for us. However, we still need to assign appropriate permissions to this managed identity\nEvaluation # Policies are cumulative and follow the most restrictive rule.\nEach assignment is independently evaluated. The net result is considered to be cumulative most restrictive When multiple policy assignments affect a resource:\nEach assignment is independently evaluated. The net result of policy is considered to be cumulative and the most restrictive outcome is applied. Regarding deny and allow policies:\nIf a policy has a deny effect, it will block the specified action, even if other policies allow it. There is no explicit allow effect in Azure Policy. By default, all actions that are not explicitly denied are allowed. If the policy has a explict allows effect,\nonly those actions are permitted all other actions not explicitly allowed are implicitly denied If the policy has a explict denies effect,\nonly those actions are blocked. all other actions not explicitly denied are implicitly allowed Example:\nRoot tenant level allows all actions except creating a virtual network. Subscription level allows creating a virtual network. Result: You cannot create a virtual network because the root tenant\u0026rsquo;s restriction takes precedence.\nThere are two ways of triggering Azure Policy evaluations:\nAutomatic: On resource creation or update: Policies are evaluated immediately when a resource is created or updated. Periodic evaluation: Policies are evaluated approximately every 24 hours. Manual: On-demand evaluation: we can manually trigger policy evaluations (if the policy supports that) Effects # Modify:\nUse for tag properties Use case: Add / replace / remove tags duration update / creation Append:\nUse for non-tag properties Append specific properties to the resource Doesn\u0026rsquo;t modify existig properties, only add new one Use case: Add a specific IP address to the allowed list in a storage account\u0026rsquo;s firewall rules DeployIfNotExists:\nUse for deploying related resources or configurations Evaluates if the resource meet with specified conditions If the resource doesn\u0026rsquo;t exist or doesn\u0026rsquo;t meet conditions, it deploys a template Use case: Enable Transparent Data Encryption (TDE) on SQL databases if it\u0026rsquo;s not enabled Tag Management # Tags can be assigned to management groups, subscriptions, resource groups, and resources, but they do not inherit from the parent level.\nFor tag inheritance, we can use the built-in Azure policy. This policy ensures that when a resource is created or updated, it automatically inherits the tag.\nThere are two types of policies for tag inheritance:\nModify: Can trigger a remediation manual Append: Can trigger a remediation when the resource is updated or created. These policies ensure that tags are automatically assigned to resources as needed.\nResource lock # There are two types of locks:\nRead-only: Prevents modifications to a resource, such as moving it in or out, while allowing other operations.\nDelete: Prevents deletion of a resource.\nLocks can be applied to:\nSubscriptions Resource groups Resources However, locks cannot be applied to management groups.\nSSPR (Self-Service Password Reset) # Enable non-administrative users to reset their own passwords.\nAdministrators do not adhere to the SSPR policy.\nCapabilities of the Global Administrator include:\nEnabling/disabling SSPR Resetting passwords for administrative users Setting up authentication methods Configuring security questions Capabilities of the User Administrator include:\nResetting passwords for non-administrative users Viewing SSPR reports Azure Device # Azure AD supports registration and management of various devices, including:\nWindows 10 or 11 laptops iPhones and iPads Android phones and tablets MacOS devices There are two primary ways to add a device to Azure AD:\nDevice Registration: For personal or BYOD (Bring Your Own Device) use. Device Join: For company-owned devices. Azure devices enable:\nSingle Sign-On (SSO) to cloud and on-premise resources. Implementation of Conditional Access policies. (e.g.: allowing specific app access based on registered device criteria) Microsoft Intune # Microsoft Intune is primarily used for:\nMobile Device Management (MDM): Managing security and settings on mobile devices. Mobile Application Management (MAM): Controlling access to and security of mobile applications. ","date":"29 June 2024","externalUrl":null,"permalink":"/posts/azure-acl/","section":"Posts","summary":"IAM Roles # Access rights to Azure resources:\nOwner: Full access, including assigning roles and policies. User Access Administrator: Assign roles/policies. Contributor: Create and manage (read and write) resources. Reader: View resources (read-only).","title":"Azure ACL","type":"posts"},{"content":"","date":"29 June 2024","externalUrl":null,"permalink":"/tags/role/","section":"Tags","summary":"","title":"role","type":"tags"},{"content":"","date":"26 June 2024","externalUrl":null,"permalink":"/tags/azcopy/","section":"Tags","summary":"","title":"azcopy","type":"tags"},{"content":" Disk Type # SSD # Standard SSD: General Purpose Premium SSD: Support high IOPS Ultra Disk: High-performance option (Overskill for most scenarios) HDD # Standard HDD Disk Caching # Azure provides three caching options for disks:\nWrite-only: Caches data only for write operations. Read-only: Caches data only for read operations. Read and write: Caches data for both read and write operations. For SQL Server workloads:\nLog files: No caching is recommended. Data files: It is recommended to use read-only caching. For scenarios where preventing data loss is critical, read-only caching is the safest option.\nDisk Encryption # Azure offers three primary methods for disk encryption:\nServer-Side Encryption (SSE)\nDefault encryption method managed by Azure Storage. Disk is decrypted when exported to VHD. More advanced configurations can be applied via Disk Encryption Set. Azure Disk Encryption (ADE)\nUses DM-Crypt for Linux or BitLocker for Windows. Can be enabled via VM extension or in \u0026ldquo;Additional Settings\u0026rdquo; under \u0026ldquo;Disks\u0026rdquo;. Disk remains encrypted when exported to VHD. Decryption requires the Key Vault key. Encryption at Host\nRequires support in the subscription plan. Encrypts data from the VM to Azure Storage. Disk Encryption Set # Disk Encryption Set (DES) offers three types of encryption:\nEncryption at rest with Customer-Managed Key (CMK) Confidential Encryption with CMK Double Encryption with CMK + Platform-Managed Key (PMK) ","date":"26 June 2024","externalUrl":null,"permalink":"/posts/azure-disk/","section":"Posts","summary":"Disk Type # SSD # Standard SSD: General Purpose Premium SSD: Support high IOPS Ultra Disk: High-performance option (Overskill for most scenarios) HDD # Standard HDD Disk Caching # Azure provides three caching options for disks:","title":"Azure Disk","type":"posts"},{"content":"","date":"26 June 2024","externalUrl":null,"permalink":"/tags/azure-file-explorer/","section":"Tags","summary":"","title":"azure file explorer","type":"tags"},{"content":" Naming Conventions # Prefix:\nAzure: Older prefix for Azure PowerShell cmdlets. Rm: Resource Manager (older prefix). Az: Newer prefix for Azure PowerShell cmdlets. Actions:\nGet-: Returns an object. e.g. Get-AzVM Retrieves details of a virtual machine. Set-: Updates the property of an object. e.g. Set-AzVMSize Changes the size of a virtual machine. Update-: Updates a resource with the object. e.g. Update-AzVM Updates a virtual machine with new configuration settings. New-: Creates a resource. e.g. New-AzVM Creates a new virtual machine. Azure AD # Add AD User # New-AzureADUser\n$userPrincipalName = \u0026#34;newuser@yourtenant.onmicrosoft.com\u0026#34; $displayName = \u0026#34;New User\u0026#34; $passwordProfile = New-Object -TypeName Microsoft.Open.AzureAD.Model.PasswordProfile $passwordProfile.Password = \u0026#34;P@ssw0rd1234\u0026#34; New-AzureADUser -DisplayName $displayName -PasswordProfile $passwordProfile -UserPrincipalName $userPrincipalName -AccountEnabled $true -MailNickname \u0026#34;newuser\u0026#34; New-MgUser\n$userPrincipalName = \u0026#34;newuser@yourtenant.onmicrosoft.com\u0026#34; $displayName = \u0026#34;New User\u0026#34; $password = \u0026#34;P@ssw0rd1234\u0026#34; New-MgUser -UserPrincipalName $userPrincipalName -DisplayName $displayName -PasswordProfile @{ Password = $password } -MailNickname \u0026#34;newuser\u0026#34; -AccountEnabled $true Invite AD Guest User # New-AzureADMSInvitation\n$invitedUserEmailAddress = \u0026#34;guestuser@example.com\u0026#34; $inviteRedirectUrl = \u0026#34;https://myapplication.com/welcome\u0026#34; New-AzureADMSInvitation -InvitedUserEmailAddress $invitedUserEmailAddress -InviteRedirectUrl $inviteRedirectUrl -SendInvitationMessage $true New-MgInvitation\n$invitedUserEmailAddress = \u0026#34;guestuser@example.com\u0026#34; $inviteRedirectUrl = \u0026#34;https://myapplication.com/welcome\u0026#34; New-MgInvitation -InvitedUserEmailAddress $invitedUserEmailAddress -InviteRedirectUrl $inviteRedirectUrl -SendInvitationMessage $true Get Role Definition # Get-AzRoleDefinition: Get Azure role definition. AD Sync # Start-ADSyncSyncCycle -PolicyType Initial: Full sync of AD Connect (slow). Start-ADSyncSyncCycle -PolicyType Delta: Delta sync of AD Connect (quick). VM # Create VM # New-AzureRmVM New-AzVM New-AzureVM Upload Virtual Disk to Storage Account # Add-AzVhd -ResourceGroupName \u0026#34;ResourceGroupName\u0026#34; -Destination \u0026#34;https://storageaccount.blob.core.windows.net/containername/diskname.vhd\u0026#34; -LocalFilePath \u0026#34;C:\\Path\\To\\My\\Local.vhd\u0026#34; Create VM from VHD # Create Image from VHD. Create VM with VM Config. Commands:\nNew-AzImageConfig Set-AzImageOsDisk Add-AzImageDataDisk New-AzImage New-AzureVMConfig New-AzureVM $resourceGroupName = \u0026#34;MyResourceGroup\u0026#34; $location = \u0026#34;EastUS\u0026#34; $imageName = \u0026#34;MyCustomImage\u0026#34; $vmName = \u0026#34;MyNewVM\u0026#34; $vmSize = \u0026#34;Standard_DS1_v2\u0026#34; $osDiskUri = \u0026#34;https://mystorageaccount.blob.core.windows.net/vhds/myosdisk.vhd\u0026#34; $dataDiskUri = \u0026#34;https://mystorageaccount.blob.core.windows.net/vhds/mydatadisk.vhd\u0026#34; $storageAccount = \u0026#34;mystorageaccount\u0026#34; $containerName = \u0026#34;vhds\u0026#34; $osDiskName = \u0026#34;$vmName-osdisk.vhd\u0026#34; $imageConfig = New-AzImageConfig -Location $location $imageConfig = Set-AzImageOsDisk -Image $imageConfig -OsType Windows -OsState Generalized -BlobUri $osDiskUri $imageConfig = Add-AzImageDataDisk -Image $imageConfig -Lun 0 -BlobUri $dataDiskUri $image = New-AzImage -ResourceGroupName $resourceGroupName -ImageName $imageName -Image $imageConfig $vmConfig = New-AzureVMConfig -VMName $vmName -VMSize $vmSize $vmConfig = Set-AzVMOSDisk -VM $vmConfig -Name $osDiskName -CreateOption FromImage -SourceImageUri $image.StorageProfile.OsDisk.Image.Uri -StorageAccountName $storageAccount -ContainerName $containerName New-AzVM -ResourceGroupName $resourceGroupName -Location $location -VM $vmConfig Update VM # Get the VM object. Update the config (IP/subnet). Call Update command to apply changes. Commands:\nGet-AzureVM Set-AzureStaticVNetIP Set-AzureSubnet Update-AzureVM $resourceGroupName = \u0026#34;MyResourceGroup\u0026#34; $vmName = \u0026#34;MyExistingVM\u0026#34; $newStaticIP = \u0026#34;10.0.0.10\u0026#34; $newSubnetName = \u0026#34;MyNewSubnet\u0026#34; $vm = Get-AzureVM -ResourceGroupName $resourceGroupName -Name $vmName $vm = Set-AzureStaticVNetIP -VM $vm -IPAddress $newStaticIP $vm = Set-AzureSubnet -VM $vm -SubnetName $newSubnetName Update-AzureVM -ResourceGroupName $resourceGroupName -VM $vm AKS # Set AKS Config # Set-AzAksCluster Build and Push Image # az acr build --registry \u0026lt;registry-name\u0026gt; --image \u0026lt;image-name\u0026gt;:\u0026lt;tag\u0026gt; \u0026lt;local-context-path\u0026gt; Create AKS # az aks create Update Node Pool Config # az aks nodepool update ARM Template # Deploy ARM Template # At Management Group: New-AzManagementGroupDeployment At Subscription: New-AzSubscriptionDeployment At Resource Group: New-AzResourceGroupDeployment $templateFile = \u0026#34;path/to/template.json\u0026#34; $parametersFile = \u0026#34;path/to/parameters.json\u0026#34; $managementGroupName = \u0026#34;MyManagementGroup\u0026#34; New-AzManagementGroupDeployment -ManagementGroupId $managementGroupName -TemplateFile $templateFile -TemplateParameterFile $parametersFile $subscriptionId = \u0026#34;MySubscriptionId\u0026#34; New-AzSubscriptionDeployment -SubscriptionId $subscriptionId -TemplateFile $templateFile -TemplateParameterFile $parametersFile $resourceGroupName = \u0026#34;MyResourceGroup\u0026#34; New-AzResourceGroupDeployment -ResourceGroupName $resourceGroupName -TemplateFile $templateFile -TemplateParameterFile $parametersFile azcopy # Create Container # azcopy make \u0026#34;https://\u0026lt;storage-account-name\u0026gt;.blob.core.windows.net/\u0026lt;container-name\u0026gt;\u0026#34; Copy Folder Recursively # azcopy copy \u0026#34;\u0026lt;local-folder-path\u0026gt;\u0026#34; \u0026#34;https://\u0026lt;storage-account-name\u0026gt;.blob.core.windows.net/\u0026lt;container-name\u0026gt;\u0026#34; --recursive Sync # like rsync, transfer the delta only\nazcopy sync \u0026#34;\u0026lt;local-folder-path\u0026gt;\u0026#34; \u0026#34;https://\u0026lt;storage-account-name\u0026gt;.blob.core.windows.net/\u0026lt;container-name\u0026gt;\u0026#34; Others # Accept Marketplace Terms # $publisher = \u0026#34;publisher-name\u0026#34; $offer = \u0026#34;offer-name\u0026#34; $plan = \u0026#34;plan-name\u0026#34; Set-AzMarketplaceTerms -Publisher $publisher -Product $offer -Name $plan -Terms $marketplaceTerms -Accept ","date":"26 June 2024","externalUrl":null,"permalink":"/posts/azure-cmd/","section":"Posts","summary":"Naming Conventions # Prefix:\nAzure: Older prefix for Azure PowerShell cmdlets. Rm: Resource Manager (older prefix). Az: Newer prefix for Azure PowerShell cmdlets. Actions:\nGet-: Returns an object. e.g. Get-AzVM Retrieves details of a virtual machine.","title":"Azure Powershell","type":"posts"},{"content":" Summary # Service File Storage Blob Storage AzCopy Yes Yes Import/Export Yes Yes File Sync Yes No Auth\\Tool AD Account Key SAS SMB Yes Yes No AzCopy Yes Yes Yes CLI Yes Yes Yes File Sync # Similar to Google Drive, File Sync allows us to synchronize folders on Windows with a file share.\nSteps:\nCreate a storage sync service. Install the File Sync agent on Windows. Register the server. Create a sync group and cloud endpoint. Add server endpoints. Each sync group can: Have one cloud endpoint (file share). Be registered with multiple servers. Have one server endpoint per server. For example, a server endpoint is a local file path like D:\\data. You cannot add both \\\\Win0001:\\C$\\data1 and \\\\Win0001:\\C$\\data2 to the same sync group, but different servers are allowed. Each sync group can only have one cloud endpoint. Only servers that are registered can join the sync group. Uploading files to the file share can cause conflicts: newer files replace older ones, which will be renamed. Changes made to files on the file share are detected by Azure File Sync once every 24 hours by default. We can manually trigger a sync to update changes sooner. Changes made locally are updated in real-time. Import/export # Azure Import/Export is akin to AWS\u0026rsquo;s Snow Family, facilitating the transfer of vast data from on-premises to the cloud by shipping data on disks to Azure.\nSetup Steps:\nPrepare Data and Drives: Before copying data to disks, prepare:\nDataset CSV: Lists files and their corresponding storage account mappings. Driveset CSV: Details about the disks. Copy Data to Disk and Create Journal File: Execute the following command to initiate data copying and create a journal file:\n.\\WAImportExport.exe PrepImport /j:JournalTest.jrn /id:session#1 /InitialDriveSet:driveset.csv /DataSet:dataset.csv /logdir:C:\\logs This command prepares for the import job and logs the process.\nUpload Journal File and Create Import Job: After data copying completes, a journal file is generated. Create import job with necessary details. Upload this file to it.\nShip Disks to Azure: Physically transport the disks containing the data to Azure’s designated facility.\nUpdate Import/Export Information: Inform Azure that the disks have been shipped. This step finalizes the data transfer process.\nOther Data Migration Options:\nImport/Export: Suitable for up to 1TB of data. Data Box Disk: Supports up to 35TB of data. Data Box: Handles up to 80TB of data. Data Box Heavy: Capable of managing up to 800TB of data. These options provide flexibility depending on the data volume and transfer requirements.\nazcopy # AzCopy is a command-line tool designed for copying data to and from Azure Storage, including both file and blob storage.\nIt supports:\nAuthentication:\nAD SAS Platforms:\nWindows Linux macOS No Android / iPhone Azure File Explorer # Capability:\nCannot create storage accounts. Can change blob types during file uploads, choosing from append, block, or page. Can create file shares, containers, queues, and tables. Can also add data to tables and queues. ","date":"26 June 2024","externalUrl":null,"permalink":"/posts/azure-storage-tools/","section":"Posts","summary":"Summary # Service File Storage Blob Storage AzCopy Yes Yes Import/Export Yes Yes File Sync Yes No Auth\\Tool AD Account Key SAS SMB Yes Yes No AzCopy Yes Yes Yes CLI Yes Yes Yes File Sync # Similar to Google Drive, File Sync allows us to synchronize folders on Windows with a file share.","title":"Azure Storage Tools","type":"posts"},{"content":"","date":"26 June 2024","externalUrl":null,"permalink":"/tags/cli/","section":"Tags","summary":"","title":"cli","type":"tags"},{"content":"","date":"26 June 2024","externalUrl":null,"permalink":"/tags/disk/","section":"Tags","summary":"","title":"disk","type":"tags"},{"content":"","date":"26 June 2024","externalUrl":null,"permalink":"/tags/file-sync/","section":"Tags","summary":"","title":"file sync","type":"tags"},{"content":"","date":"19 December 2023","externalUrl":null,"permalink":"/tags/ansible/","section":"Tags","summary":"","title":"ansible","type":"tags"},{"content":" adminrc # Via Horizon # Via Command Line # While there isn\u0026rsquo;t a direct way to generate the adminrc file from the command line, you can use the OpenStack CLI to fetch the necessary authentication parameters and convert them into environment variables.\nCheck Available Options:\nopenstack -h Parameter CLI Argument Environment Variable API endpoint --os-auth-url http://controller:5000 export OS_AUTH_URL=http://controller:5000 API Version --os-identity-api-version 3 export OS_IDENTITY_API_VERSION=3 User --os-username admin export OS_USERNAME=admin Password --os-password admin export OS_PASSWORD=admin Project --os-project-name admin export OS_PROJECT_NAME=admin Domain --os-domain-name Default export OS_DOMAIN_NAME=Default Use the converted environment variables to create the adminrc file. eg:\nvim adminrc\nexport OS_AUTH_URL=http://controller:5000 export OS_IDENTITY_API_VERSION=3 export OS_USERNAME=admin export OS_PASSWORD=admin export OS_PROJECT_NAME=admin export OS_DOMAIN_NAME=Default Once the adminrc file is ready, we can\nsource adminrc ","date":"19 December 2023","externalUrl":null,"permalink":"/posts/openstack-adminrc/","section":"Posts","summary":"adminrc # Via Horizon # Via Command Line # While there isn\u0026rsquo;t a direct way to generate the adminrc file from the command line, you can use the OpenStack CLI to fetch the necessary authentication parameters and convert them into environment variables.","title":"Basic Operations in Openstack","type":"posts"},{"content":"","date":"19 December 2023","externalUrl":null,"permalink":"/tags/cinder/","section":"Tags","summary":"","title":"cinder","type":"tags"},{"content":"","date":"19 December 2023","externalUrl":null,"permalink":"/tags/esxi/","section":"Tags","summary":"","title":"ESXi","type":"tags"},{"content":"I\u0026rsquo;m attempting to configure CentOS Stream 9 on ESXi using a cloud image.\nHowever, the cloud image is not available in VMDK format, and the ISO file is quite large at 9GB.\nI\u0026rsquo;d prefer not to install the VM from the ISO.\nInstead, I plan to convert the QCOW2 image to VMDK format and then import it into ESXi. To begin, I need to download the QCOW2 cloud image: https://cloud.centos.org/centos/9-stream/x86_64/images/\nI followed the steps outlined in this guide (https://blog.ktz.me/migrate-qcow2-images-from-kvm-to-vmware/) to convert a VMDK from a QCOW2 image:\nUse the qemu-img tool to convert the QCOW2 image to a VMDK format: qemu-img convert -f qcow2 -O vmdk quassel.qcow2 quasselog.vmdk Copy the resulting VMDK file to ESXi.\nIn ESXi, create a VM with no disk attached.\nPerform an additional conversion of the VMDK file to a ESXi-compatible format using vmkfstools:\nvmkfstools -i quasselog.vmdk -d thin quassel.vmdk After this conversion, I got two files. Copy both of them to the VM\u0026rsquo;s datastore.\nEdit the VM\u0026rsquo;s settings, add a new disk, and select the option to use an existing disk. Choose the converted VMDK file to attach it to the VM.\nThe cloud image does not automatically assign a default linux account / password, leaving us with no means to access the system.\nThere are two methods available for logging into the VM:\nUse cloud-init to configure the network and user Set a default root password within the QCOW2 image Method1:\nStart by creating a user-data file:\nvim user-data\n#cloud-config users: - name: centos plain_text_passwd: centos groups: sudo sudo: ALL=(ALL) NOPASSWD:ALL shell: /bin/bash lock_passwd: false See more on cloud-init User-Data Examples.\nNext, convert the user-data file to base64 encoding:\nbase64 user-data In ESXi, modify the VM\u0026rsquo;s settings:\nNavigate to Configuration Parameters. Add the following parameters: guestinfo.userdata.encoding: base64 guestinfo.userdata: \u0026lt;base64 encoded user-data\u0026gt; Include the network configuration\nnetwork: version: 2 ethernets: eth1: dhcp4: false dhcp6: false addresses: - 192.168.0.100/24 gateway4: 192.168.0.100 nameservers: addresses: - 8.8.8.8 See more on cloud-init Network Config Format v2 Examples.\nFinally, add this data to the guestinfo.metadata to the Configuration Parameters.\nSee more on cloud-init VMware Data Source.\nMethod2:\nvirt-customize -a centos.qcow2 --root-password password:\u0026lt;password\u0026gt; See more on Use libguestfs to manage virtual machine disk images\n","date":"19 December 2023","externalUrl":null,"permalink":"/posts/import-qcow2-cloud-image-to-esxi/","section":"Posts","summary":"I\u0026rsquo;m attempting to configure CentOS Stream 9 on ESXi using a cloud image.\nHowever, the cloud image is not available in VMDK format, and the ISO file is quite large at 9GB.","title":"Import qcow2 cloud image to ESXi","type":"posts"},{"content":"","date":"19 December 2023","externalUrl":null,"permalink":"/tags/kolla-ansible/","section":"Tags","summary":"","title":"kolla-ansible","type":"tags"},{"content":"","date":"19 December 2023","externalUrl":null,"permalink":"/tags/kvm/","section":"Tags","summary":"","title":"kvm","type":"tags"},{"content":"","date":"19 December 2023","externalUrl":null,"permalink":"/tags/linux/","section":"Tags","summary":"","title":"linux","type":"tags"},{"content":"I\u0026rsquo;ve successfully set up OpenStack using Devstack and Kolla-Ansible, and you can find my configuration here.\nThis article is part of a series, with other sections located on different pages:\nKolla Ansible adminrc OVN \u0026amp; Provider Network Cinder Swift Image Volume \u0026amp; Snapshot Usage \u0026amp; Quota On this page, I\u0026rsquo;ll be detailing my personal journey through the world of OpenStack.\nInitial Steps:\nMy OpenStack adventure began with an older version (Pika). I accessed it through a VirtualBox image from a Udemy course. This phase helped me understand the basics of OpenStack.\nI learned about VM creation and storage management using Horizon, similar to AWS\u0026rsquo;s web console.\nLimitations and Exploring More:\nI soon noticed limitations in the Pika version, especially in networking. It was still using the Linux Bridge instead of OVN.\nMy goal was to understand how different OpenStack components like Compute, Storage and Network work together. I realized that focusing on an outdated version wouldn\u0026rsquo;t benefit my learning journey.\nEvolving to a More Sophisticated Setup:\nTo deepen my understanding, I moved on to the latest stable release of OpenStack (2023.1), using Devstack. Initially, the installation was challenging due to some unclear steps in the script.\nHowever, Kolla-Ansible changed my experience. It made the installation process smoother and is recommended for production environments.\nAfter that I did my COA Exam.\nDelving into High Availability:\nI then shifted my focus to understand High Availability (HA) in OpenStack.\nTo do this, I set up a home lab with a refurbished HPE DL360p Gen8 server.\nThis allowed me to experiment and apply my knowledge in a practical setting.\nEngaging with the OpenStack Community:\nJoining the OpenStack community has been a significant part of my journey.\nI\u0026rsquo;ve gained a lot of insights by observing how community members address OpenStack-related issues and queries.\nThis engagement has greatly enhanced my understanding in OpenStack.\n","date":"19 December 2023","externalUrl":null,"permalink":"/posts/openstack/","section":"Posts","summary":"I\u0026rsquo;ve successfully set up OpenStack using Devstack and Kolla-Ansible, and you can find my configuration here.\nThis article is part of a series, with other sections located on different pages:","title":"My journal on Openstack","type":"posts"},{"content":"","date":"19 December 2023","externalUrl":null,"permalink":"/tags/openstack/","section":"Tags","summary":"","title":"openstack","type":"tags"},{"content":" LVM2 Backend # To configure the LVM2 backend in cinder.conf, follow these steps:\nCreate an LVM volume group named \u0026ldquo;cinder-volumes.\u0026rdquo;\nDefine a configuration section like the one shown below in cinder.conf:\n[lvm-1] volume_group = cinder-volumes volume_driver = cinder.volume.drivers.lvm.LVMVolumeDriver volume_backend_name = lvm-1 target_helper = tgtadm target_protocol = iscsi Restart the Cinder service.\nSet the volume_backend_name property of the desired volume type to match the volume_backend_name defined in the cinder conf:\nopenstack volume type set --property volume_backend_name=lvm-1 \u0026lt;VOLUME_TYPE_NAME\u0026gt; If you need additional LVM backends, you can create more volume groups based on your requirements.\nFor more details, you can refer to the documentation at: https://docs.openstack.org/kolla-ansible/latest/reference/storage/cinder-guide.html\nLUKS Encryption # Enable Barbican in Kolla Configuration:\nvim /etc/kolla/global.yml\nenable_barbican: \u0026#34;yes\u0026#34; barbican_crypto_plugin: \u0026#34;simple_crypto\u0026#34; barbican_library_path: \u0026#34;/usr/lib/libCryptoki2_64.so\u0026#34; Configure Cinder with Barbican:\nvim /etc/kolla/config/cinder.conf\n[key_manager] backend = barbican Restart cinder-api, cinder-volume and cinder-backup.\nvim /etc/kolla.config/nova.conf\n[key_manager] backend = barbican Deploy the Configuration:\nRun the Kolla-Ansible deploy command:\nkolla-ansible -i ./all-in-one deploy Use the OpenStack CLI to create a LUKS encrypted volume type:\nopenstack volume type create --encryption-provider luks --encryption-cipher aes-xts-plain64 --encryption-key-size 256 --encryption-control-location front-end LUKS Now, create a volume using the LUKS type:\nopenstack volume create --size 1 --type LUKS enc_vol https://docs.openstack.org/cinder/latest/configuration/block-storage/volume-encryption.html#\n","date":"19 December 2023","externalUrl":null,"permalink":"/posts/openstack-cinder/","section":"Posts","summary":"LVM2 Backend # To configure the LVM2 backend in cinder.conf, follow these steps:\nCreate an LVM volume group named \u0026ldquo;cinder-volumes.\u0026rdquo;\nDefine a configuration section like the one shown below in cinder.","title":"Openstack - Cinder","type":"posts"},{"content":" Conversion between Image, Volume and Snapshot # Each of these can serve as a source for creating a new VM.\nFrom/To Image Volume Snapshot Image Y Volume Y Snapshot Y Public image vs shared image # Public Images Shared Images Visibility Accessible to all projects. Visible only to the owning project by default. Use Case For common base images or standard configurations. For controlled access to specific images. Access Control All users can list, view, and use these images. Admins can share with certain projects; unlisted for others. Add / remove project to shared image\nopenstack image set \u0026lt;image\u0026gt; --shared openstack image add project \u0026lt;image\u0026gt; \u0026lt;project\u0026gt; openstack image remove project \u0026lt;image\u0026gt; \u0026lt;project\u0026gt; Backup and Restore VM # Only images can be imported/exported to/from the OpenStack environment.\nVolumes and snapshots remain within the OpenStack system.\nTo export an image, the openstack server image save command is used.\nTake Snapshot # openstack server image create Restore from Image # openstack server create --image \u0026lt;image\u0026gt; Download image # openstack image save \u0026lt;image\u0026gt; --file \u0026lt;output_file\u0026gt; Volume without cinder # Cinder serves as OpenStack\u0026rsquo;s volume service (akin to AWS EBS).\nIt\u0026rsquo;s worth noting that OpenStack can still be utilized even in the absence of Cinder (similar to AWS\u0026rsquo;s ephemeral storage).\nWhen Cinder is not present, two key distinctions become apparent:\nThe \u0026ldquo;Volumes\u0026rdquo; tab will not appear in the Horizon dashboard. The \u0026ldquo;\u0026ndash;boot-from-volume\u0026rdquo; option becomes inaccessible when using the \u0026ldquo;openstack server create\u0026rdquo; command. 0 byte image \u0026amp; snapshot # If you select \u0026ldquo;Yes\u0026rdquo; for \u0026ldquo;Create New Volume\u0026rdquo; (\u0026ndash;boot-from-volume in cli) during VM creation, the image won\u0026rsquo;t be copied into the disk.\nInstead, it will only store metadata to establish a linkage to the image.\nTherefore, if you intend to export / snapshot a Volume-booted VM, you will end up with a 0-byte image.\nThis displays the metadata associated with the snapshot:\n[ { \u0026#34;volume_id\u0026#34;: null, \u0026#34;guest_format\u0026#34;: null, \u0026#34;encrypted\u0026#34;: null, \u0026#34;volume_type\u0026#34;: null, \u0026#34;snapshot_id\u0026#34;: \u0026#34;d23dd96f-e96e-450a-a832-747587886fb2\u0026#34;, \u0026#34;encryption_secret_uuid\u0026#34;: null, \u0026#34;disk_bus\u0026#34;: \u0026#34;virtio\u0026#34;, \u0026#34;device_name\u0026#34;: \u0026#34;/dev/vda\u0026#34;, \u0026#34;image_id\u0026#34;: null, \u0026#34;encryption_format\u0026#34;: null, \u0026#34;delete_on_termination\u0026#34;: false, \u0026#34;device_type\u0026#34;: \u0026#34;disk\u0026#34;, \u0026#34;destination_type\u0026#34;: \u0026#34;volume\u0026#34;, \u0026#34;source_type\u0026#34;: \u0026#34;snapshot\u0026#34;, \u0026#34;boot_index\u0026#34;: 0, \u0026#34;no_device\u0026#34;: null, \u0026#34;encryption_options\u0026#34;: null, \u0026#34;volume_size\u0026#34;: 1, \u0026#34;tag\u0026#34;: null } ] ","date":"19 December 2023","externalUrl":null,"permalink":"/posts/openstack-image-volume-snapshot/","section":"Posts","summary":"Conversion between Image, Volume and Snapshot # Each of these can serve as a source for creating a new VM.\nFrom/To Image Volume Snapshot Image Y Volume Y Snapshot Y Public image vs shared image # Public Images Shared Images Visibility Accessible to all projects.","title":"Openstack - Image, Volume and Snapshot","type":"posts"},{"content":" Concept # https://ubuntu.com/blog/data-centre-networking-what-is-ovn\nOVN manages the Northbound and Southbound DB, playing a crucial role in network orchestration:\nNorthbound DB\nFunctions as a public API service, facilitating interactions with external services, such as the OpenStack API. Southbound DB\nManages OVN\u0026rsquo;s internal data, allowing for communication among OVN controllers. Northbound to Southbound Translation (northd):\nOperates as an intermediary, converting instructions from the Northbound DB to a format understandable by the Southbound DB. Cli:\novn-nbctl: Manages the definitions of routers, bridges, and networks within the cluster. ovn-sbctl: Manages the information related to servers. Compute Node Network Integration:\nEach compute node runs an OVN controller that connects to the centralized Southbound OVN DB located on the network node. This OVN controller coordinates networking operations by communicating with a local instance of OVS server. The local database can be managed using the ovs-vsctl command.\nTwo crucial parameters in the networking context are:\novn-bridge-mappings: This parameter establishes mappings between OVN logical networks and OVS bridges.\nenable-chassis-as-gw: It configures the chassis to act as gateways when the need arises.\nTroubleshooting # Misconfig in provider network # When setting up a provider network in Horizon, the network config must be specified as a flat and external type. The physical network designation corresponds to the identifier used within the OVS DB.\nFor instance, consider we have a provider network public and a corresponding physical network named providernet.\nIn such a scenario, the details of the network and its associated ports can be observed within the Northbound DB.\novn-nbctl list logical_switch public _uuid : d3cb813b-c44b-4782-87bb-aa6593d40357 acls : [] copp : [] dns_records : [] external_ids : {\u0026#34;neutron:availability_zone_hints\u0026#34;=\u0026#34;\u0026#34;, \u0026#34;neutron:mtu\u0026#34;=\u0026#34;1500\u0026#34;, \u0026#34;neutron:network_name\u0026#34;=public, \u0026#34;neutron:revision_number\u0026#34;=\u0026#34;1\u0026#34;} forwarding_groups : [] load_balancer : [] load_balancer_group : [] name : neutron-f5f8f47b-4093-42ae-a5e9-431ef06e5eeb other_config : {mcast_flood_unregistered=\u0026#34;false\u0026#34;, mcast_snoop=\u0026#34;false\u0026#34;, vlan-passthru=\u0026#34;false\u0026#34;} ports : [06684ced-8c82-400c-9bc8-5f1ba35b3687, 439f0fe3-fb00-4679-b7ca-0a850e4b6cbb, d38818c3-0ba5-4d82-8a88-b2fcfdc29a70] qos_rules : [] d38818c3-0ba5-4d82-8a88-b2fcfdc29a70 is the port of the provider network\novn-nbctl list logical_switch_port d38818c3-0ba5-4d82-8a88-b2fcfdc29a70 _uuid : d38818c3-0ba5-4d82-8a88-b2fcfdc29a70 addresses : [unknown] dhcpv4_options : [] dhcpv6_options : [] dynamic_addresses : [] enabled : [] external_ids : {} ha_chassis_group : [] name : provnet-840f094f-44df-4746-bdfe-8007ec2dd64f options : {mcast_flood=\u0026#34;false\u0026#34;, mcast_flood_reports=\u0026#34;true\u0026#34;, network_name=providernet} parent_name : [] port_security : [] tag : [] tag_request : [] type : localnet up : false In the OVS DB, the ovn-bridge-mappings config links the physical network named providernet to the bridge device br-ex.\nIt is essential that the information in the OVS DB matches with the data in the Northbound DB.\novs-vsctl list open _uuid : bc26d1c9-8134-4550-8099-9ebc34cd3509 bridges : [598efdc0-15f6-4dcb-924f-80c6bad3c1d3, ade90deb-b74e-4eaf-97f0-d56309aadac4] cur_cfg : 4 datapath_types : [netdev, system] datapaths : {system=d4540bf0-d630-448c-9fee-98d051f247e5} db_version : \u0026#34;8.3.0\u0026#34; dpdk_initialized : false dpdk_version : none external_ids : {hostname=ubuntu-jammy, ovn-bridge=br-int, ovn-bridge-mappings=\u0026#34;providernet:br-ex\u0026#34;, ovn-cms-options=enable-chassis-as-gw, ovn-encap-ip=\u0026#34;10.0.114.11\u0026#34;, ovn-encap-type=geneve, ovn-remote=\u0026#34;tcp:10.0.114.11:6642\u0026#34;, rundir=\u0026#34;/var/run/openvswitch\u0026#34;, system-id=\u0026#34;b13926a9-19ed-46c8-a877-3df39d32782b\u0026#34;} iface_types : [bareudp, erspan, geneve, gre, gtpu, internal, ip6erspan, ip6gre, lisp, patch, stt, system, tap, vxlan] manager_options : [0966f00e-7387-432a-a534-e3d1831b809e] next_cfg : 4 other_config : {vlan-limit=\u0026#34;0\u0026#34;} ovs_version : \u0026#34;2.17.7\u0026#34; ssl : [] statistics : {} system_type : ubuntu system_version : \u0026#34;22.04\u0026#34; If the mappings are not correctly configured, the ovs-vsctl show command will display only two bridges: br-int and br-ex.\nbr-ex will be associated with the network device enp0s9, but without the proper linkage, there will be no connection between br-int and br-ex.\nThis means that the bridges will operate independently and not pass traffic between each other.\novs-vsctl show bc26d1c9-8134-4550-8099-9ebc34cd3509 Manager \u0026#34;ptcp:6640:127.0.0.1\u0026#34; is_connected: true Bridge br-int fail_mode: secure datapath_type: system Port br-int Interface br-int type: internal Bridge br-ex Port enp0s9 Interface enp0s9 Port br-ex Interface br-ex type: internal ovs_version: \u0026#34;2.17.7\u0026#34; An automatic creation of a patch-XXX port occurs when a corresponding mapping is found. This implies that the provider network must be operational.\novs-vsctl show bc26d1c9-8134-4550-8099-9ebc34cd3509 Manager \u0026#34;ptcp:6640:127.0.0.1\u0026#34; is_connected: true Bridge br-int fail_mode: secure datapath_type: system Port br-int Interface br-int type: internal Port patch-br-int-to-provnet-9c84ee84-3b4d-4dc8-8317-1f0939603413 Interface patch-br-int-to-provnet-9c84ee84-3b4d-4dc8-8317-1f0939603413 type: patch options: {peer=patch-provnet-9c84ee84-3b4d-4dc8-8317-1f0939603413-to-br-int} Bridge br-ex Port patch-provnet-9c84ee84-3b4d-4dc8-8317-1f0939603413-to-br-int Interface patch-provnet-9c84ee84-3b4d-4dc8-8317-1f0939603413-to-br-int type: patch options: {peer=patch-br-int-to-provnet-9c84ee84-3b4d-4dc8-8317-1f0939603413} Port enp0s9 Interface enp0s9 Port br-ex Interface br-ex type: internal ovs_version: \u0026#34;2.17.7\u0026#34; Incorrect setup will result in the VM lacking internet connectivity.\nWhile it\u0026rsquo;s still possible to attach a floating IP to the VM, it won\u0026rsquo;t work as expected.\nThe VM won\u0026rsquo;t be able to ping essential destinations like 8.8.8.8, the gateway, or any other machines within the provider network.\nBroken gateway # Once the bridge mapping is corrected, all ports will appear as active in OpenStack admin.\nHowever, internet access will cease when the compute node is offline. In this scenario, the VM resides on the controller node:\nController: Up (VM is located here) Compute: Down ovn-nbctl show ... router 680f21a0-2a4c-49e1-962e-869c33bdca08 (neutron-5351b89f-04d1-4498-9a03-6d5db3b016a2) (aka router1) port lrp-ca9fc58e-5a6e-431e-b7d9-b310413be9bf mac: \u0026#34;fa:16:3e:56:31:9b\u0026#34; networks: [\u0026#34;10.0.0.1/22\u0026#34;] port lrp-b08ccb1f-5481-430e-9bff-8a4623924186 mac: \u0026#34;fa:16:3e:81:6d:4d\u0026#34; networks: [\u0026#34;203.0.114.124/24\u0026#34;] gateway chassis: [f83ca915-8312-42dd-9f7c-3b7f6e128a16 b612c66f-6b89-4e08-9c8a-2b8da2037a60] nat 0f48c87b-9173-46f4-a6a2-8d90c8e8a779 external ip: \u0026#34;203.0.114.124\u0026#34; logical ip: \u0026#34;10.0.0.0/22\u0026#34; type: \u0026#34;snat\u0026#34; ovn-sbctl show Chassis \u0026#34;b612c66f-6b89-4e08-9c8a-2b8da2037a60\u0026#34; hostname: ubuntu-jammy Encap geneve ip: \u0026#34;10.0.114.11\u0026#34; options: {csum=\u0026#34;true\u0026#34;} Chassis \u0026#34;f83ca915-8312-42dd-9f7c-3b7f6e128a16\u0026#34; hostname: compute Encap geneve ip: \u0026#34;10.0.114.12\u0026#34; options: {csum=\u0026#34;true\u0026#34;} The gateway chassis contains two entries:\ncontroller compute (f83ca915-8312-42dd-9f7c-3b7f6e128a16) Since the compute was offline, it was the root cause of the network connectivity problem.\nTo address this issue initially, I removed the compute from the gateway chassis, which resolved the network connectivity problem.\novn-nbctl lrp-del-gateway-chassis lrp-b08ccb1f-5481-430e-9bff-8a4623924186 f83ca915-8312-42dd-9f7c-3b7f6e128a16 However, the problem recurred shortly after the config was corrected. This prompted an investigation into why the compute node kept being designated as a gateway chassis.\nUpon closer examination, it became evident that the issue originated within the compute node itself.\nRunning the command ovs-vsctl list open revealed a config entry labeled ovn-cms-options=enable-chassis-as-gw.\nWhen the ovn-cms-options=enable-chassis-as-gw config is present, the compute node is automatically assigned as a gateway chassis for the router.\nThe root cause of this recurrent behavior was found in the DevStack script, where the default setting for ENABLE_CHASSIS_AS_GW is configured to be true, leading to the compute node repeatedly assuming the role of a gateway chassis.\nUpdated as of 2023-10-05:\nThe assignment of the gateway chassis should exclusively occur on dedicated network nodes.\nWithin OVN, there exists a concept known as Distributed Floating IP, which operates as follows:\nFor VMs lacking a Floating IP, external traffic is routed through the network Node.\nWhen a Floating IP is attached, traffic is directed through the local bridge for direct internet access.\nThe L3 High Availability (L3HA) support feature is implemented to initiate a failover in the event of node failure.\nIn contrast, in an OVS setup, traffic consistently passes through the local bridge.\nReference:\nOVN: https://docs.openstack.org/networking-ovn/latest/admin/routing.html\nOVS: https://docs.openstack.org/neutron/2023.1/admin/deploy-ovs-provider.html#deploy-ovs-provider\n","date":"19 December 2023","externalUrl":null,"permalink":"/posts/openstack-ovn-provider-network/","section":"Posts","summary":"Concept # https://ubuntu.com/blog/data-centre-networking-what-is-ovn\nOVN manages the Northbound and Southbound DB, playing a crucial role in network orchestration:\nNorthbound DB\nFunctions as a public API service, facilitating interactions with external services, such as the OpenStack API.","title":"Openstack - OVN - Provider Network","type":"posts"},{"content":" Create swift ring # Swift is built on three primary components:\nObject\nAccount\nContainer\nFor each of these components, ring files are required:\n.builder\n.ring.gz\nRing files store information about the disks in the Swift storage system. When a new disk is added or an existing disk undergoes changes, it\u0026rsquo;s essential to regenerate and rebalance the relevant ring file.\nFor instance, when creating an object ring:\nswift-ring.builder object-1.builder create 10 3 1 10: This represents the partition power, implying that the ring will be divided into 2^10 partitions.\n3: Indicates the replica count. With a value of 3, the data will be replicated into three copies.\n1: Refers to min_part_hours. It ensures that when a disk goes out of service or a new disk is added, only one data piece will move to a new partition in an hour. This mechanism ensures a minimum of two copies are always available during any operation.\nThe matrix formed from the partition and replica information indicates where the second and third copies of each data piece will be stored.\nThe system detects the failure and identifies the data that was on the failed disk.\nTemporary partitions are created on other disks or nodes to take over the role of the failed disk.\nThe data that was on the failed disk is replicated to these temporary partitions to restore the desired level of redundancy.\nAdding disks to the Ring:\nswift-ring-builder object-1.builder add \u0026lt;disk\u0026gt; \u0026lt;weight\u0026gt; swift-ring-builder object-1.builder add r1z1-127.0.0.1:6001/d1 1 Here, parameters like z for zone, r for region, and other parameters related to host and disk provide constraints on where copies of the data are stored.\nd1 is the disk label, 1 is the weight\nSwift intelligently places replicas as far apart as possible to ensure data durability. For instance:\nIf there are multiple zones, regions, or hosts, Swift tries to keep replicas in different zones, regions, or hosts.\nIf there\u0026rsquo;s only one zone, region, and host, Swift will ensure that replicas are on different disks within that host.\nhttps://docs.openstack.org/kolla-ansible/latest/reference/storage/swift-guide.html\nCreate storage policy # Storage policies start from an index of 0. The first ring corresponds to [storage-policy-0], the second ring to [storage-policy-1], and so on.\nEach policy can be used to specify a different ring setting, which in turn can be used during container or object creation.\nExample:\n[storage-policy:0] name = gold default = yes [storage-policy:1] name = silver Add the above policy definitions to the /etc/kolla/config/swift.conf file.\nRedeploy the openstack\nkolla-ansible -i ./all-in-one deploy To create container with storage policy:\nopenstack container create \u0026lt;container\u0026gt; --storage-policy \u0026lt;storage-policy\u0026gt; Set Object Expiration Date # swift post \u0026lt;container\u0026gt; \u0026lt;object\u0026gt; -H \u0026#34;X-Delete-After:\u0026lt;seconds\u0026gt;\u0026#34; swift post \u0026lt;container\u0026gt; \u0026lt;object\u0026gt; -H \u0026#34;X-Delete-At:\u0026lt;timestamp\u0026gt;\u0026#34; Timestamp can be obtained from\ndate +%s -d \u0026#34;tomorrow 00:00\u0026#34; Remove Object Expiration Date # swift post \u0026lt;container\u0026gt; \u0026lt;object\u0026gt; -H \u0026#34;X-Remove-Delete-At:\u0026#34; ACL # Swift post -r \u0026#34;\u0026lt;read acl\u0026gt;\u0026#34; -w \u0026#34;\u0026lt;write acl\u0026gt; Permission Pattern read only bucket r.*, .rlistings project \u0026lt;project\u0026gt;:* user *:\u0026lt;user\u0026gt; role \u0026lt;role\u0026gt; Download Object # openstack object save \u0026lt;container\u0026gt; \u0026lt;object\u0026gt; --filename \u0026lt;file\u0026gt; ","date":"19 December 2023","externalUrl":null,"permalink":"/posts/openstack-swift/","section":"Posts","summary":"Create swift ring # Swift is built on three primary components:\nObject\nAccount\nContainer\nFor each of these components, ring files are required:\n.builder\n.ring.gz\nRing files store information about the disks in the Swift storage system.","title":"Openstack - Swift","type":"posts"},{"content":" List Compute # openstack host list openstack hypervisor list openstack compute server list List Services # openstack server list openstack endpoint list Compute usage # eg: RAM, CPU, DISK\nopenstack usage list openstack hypervisor stats show Quota usage # mysql use nova; select * from quotas_usage; mysql use neutron; select * from quotas_usage; Set Quota # Per Project\nopenstack quota show \u0026lt;project\u0026gt; openstack quota set \u0026lt;project\u0026gt; --\u0026lt;quota-key\u0026gt; \u0026lt;quota-value\u0026gt; Global default\nopenstack quota set default --class ","date":"19 December 2023","externalUrl":null,"permalink":"/posts/openstack-usage-quota/","section":"Posts","summary":"List Compute # openstack host list openstack hypervisor list openstack compute server list List Services # openstack server list openstack endpoint list Compute usage # eg: RAM, CPU, DISK","title":"Openstack - Usage \u0026 Quota","type":"posts"},{"content":" install kolla-ansible # all we need is this doc: Kolla-Ansible Quickstart\nAdditional Config # Swift Object Storage # To enable Swift with storage policies and additional disks for the VMs, follow the guide below and create the necessary ring files:\nSwift Guide Cinder Block Storage # For Cinder, label your disks accordingly:\nCinder Guide Additional Features # Volume backup LUKS encryption with Barbican in Nova and Cinder (Note: It\u0026rsquo;s tricky because Barbican is not configured on Swift and Cinder by default). To override the default configurations, place your custom config files into /etc/kolla/config. These will be merged with the existing ones, hence the config folder in this repository.\nTLS and Troubleshooting # I attempted to enable TLS; however, it caused issues with the volume backup service\u0026rsquo;s connectivity to the database.\nThis may be due to enabling TLS at pos- setup. Therefore, it is currently disabled. It\u0026rsquo;s advised to start with a fresh installation if you wish to enable TLS.\n","date":"19 December 2023","externalUrl":null,"permalink":"/posts/openstack-kolla-ansible/","section":"Posts","summary":"install kolla-ansible # all we need is this doc: Kolla-Ansible Quickstart\nAdditional Config # Swift Object Storage # To enable Swift with storage policies and additional disks for the VMs, follow the guide below and create the necessary ring files:","title":"Openstack with kolla ansible","type":"posts"},{"content":"","date":"19 December 2023","externalUrl":null,"permalink":"/tags/ovn/","section":"Tags","summary":"","title":"ovn","type":"tags"},{"content":"","date":"19 December 2023","externalUrl":null,"permalink":"/tags/proxmox/","section":"Tags","summary":"","title":"proxmox","type":"tags"},{"content":" Networking # NAT config # For my setup, I have one physical Ethernet connection on eno1 device, which by default is assigned to vmbr0.\nI configured a NAT bridge vmbr1 by following the instructions from the Proxmox Network config Guide.\nThe config for vmbr1 is as follows:\nauto vmbr1 iface vmbr1 inet static address 203.0.113.0/24 bridge-ports none bridge-stp off bridge-fd 0 post-up echo 1 \u0026gt; /proc/sys/net/ipv4/ip_forward post-up iptables -t nat -A POSTROUTING -s \u0026#39;203.0.113.0/24\u0026#39; -o vmbr0 -j MASQUERADE post-down iptables -t nat -D POSTROUTING -s \u0026#39;203.0.113.0/24\u0026#39; -o vmbr0 -j MASQUERADE After config, restart the networking service:\nsystemctl restart networking VLAN config # I wanted to set up VLAN for testing purposes and found this useful thread on the Proxmox Forum.\nThe config is similar to the NAT setup but includes VLAN-specific settings:\nauto vmbr1 iface vmbr1 inet static address 203.0.113.0/24 bridge-ports none bridge-stp off bridge-fd 0 post-up echo 1 \u0026gt; /proc/sys/net/ipv4/ip_forward - post-up iptables -t nat -A POSTROUTING -s \u0026#39;203.0.113.0/24\u0026#39; -o vmbr0 -j MASQUERADE + post-up iptables -t nat -A POSTROUTING -s \u0026#39;203.0.0.0/16\u0026#39; -o vmbr0 -j MASQUERADE - post-down iptables -t nat -D POSTROUTING -s \u0026#39;203.0.113.0/24\u0026#39; -o vmbr0 -j MASQUERADE + post-down iptables -t nat -D POSTROUTING -s \u0026#39;203.0.0.0/16\u0026#39; -o vmbr0 -j MASQUERADE +auto vmbr1.114 +iface vmbr1.114 inet static + address 203.0.114.1 + netmask 255.255.255.0 Managing ISO and VM Images # Uploading ISO # To upload an ISO file to Proxmox, follow the instructions detailed in the OVHcloud Support: Go to local \u0026gt; Upload \u0026gt; Select File.\nImporting ovf # For setting up GNS3 on Proxmox, I downloaded the GNS3 VM from GNS3 VM Download.\nSince GNS3 VM is available only in VMware ESXi (OVF) format, use the following command to import it to Proxmox, as explained in Zach Grace\u0026rsquo;s Proxmox Cheat Sheet:\nqm importovf \u0026lt;vmid\u0026gt; \u0026lt;ovf file\u0026gt; \u0026lt;storage\u0026gt; Enabling Nested Virtualization # Nested virtualization can be enabled in Proxmox by referring to the Proxmox Nested Virtualization Guide:\nqm set \u0026lt;vmid\u0026gt; --cpu host Creating VM with Cloud Image and Cloud-Init # To create a VM using a cloud image and cloud-init, use the steps provided in the Proxmox Cloud-Init Support:\nCreate as Template\nwget https://cloud-images.ubuntu.com/bionic/current/bionic-server-cloudimg-amd64.img qm create 9000 --memory 2048 --net0 virtio,bridge=vmbr0 --scsihw virtio-scsi-pci qm set 9000 --scsi0 local-lvm:0,import-from=/path/to/bionic-server-cloudimg-amd64.img qm set 9000 --ide2 local-lvm:cloudinit qm set 9000 --boot order=scsi0 qm set 9000 --serial0 socket --vga serial0 qm template 9000 Create VM\nqm clone 9000 123 --name ubuntu2 qm set 123 --sshkey ~/.ssh/id_rsa.pub qm set 123 --ipconfig0 ip=10.0.10.123/24,gw=10.0.10.1 Updating Proxmox Without Subscription # The default repository requires a paid subscription for Proxmox updates.\nHowever, it\u0026rsquo;s possible to bypass this by manually changing the repository settings, as outlined in the VirtualizationHowTo Guide.\nvim /etc/apt/sources.list.d/pve-enterprise.list\n-deb https://enterprise.proxmox.com/debian/pve bookworm enterprise +deb http://download.proxmox.com/debian/pve bookworm pve-no-subscription vim /etc/apt/sources.list.d/ceph.list\n-deb https://enterprise.proxmox.com/debian/ceph-quincy bookworm enterprise +deb http://download.proxmox.com/debian/ceph-quincy bookworm no-subscription ","date":"19 December 2023","externalUrl":null,"permalink":"/posts/proxmox-notes/","section":"Posts","summary":"Networking # NAT config # For my setup, I have one physical Ethernet connection on eno1 device, which by default is assigned to vmbr0.\nI configured a NAT bridge vmbr1 by following the instructions from the Proxmox Network config Guide.","title":"Proxmox Notes","type":"posts"},{"content":"","date":"19 December 2023","externalUrl":null,"permalink":"/tags/quota/","section":"Tags","summary":"","title":"quota","type":"tags"},{"content":"","date":"19 December 2023","externalUrl":null,"permalink":"/tags/swift/","section":"Tags","summary":"","title":"swift","type":"tags"},{"content":"","date":"16 December 2023","externalUrl":null,"permalink":"/tags/harvester/","section":"Tags","summary":"","title":"harvester","type":"tags"},{"content":"","date":"16 December 2023","externalUrl":null,"permalink":"/tags/hypervisor/","section":"Tags","summary":"","title":"hypervisor","type":"tags"},{"content":"","date":"16 December 2023","externalUrl":null,"permalink":"/tags/kubernetes/","section":"Tags","summary":"","title":"kubernetes","type":"tags"},{"content":"","date":"16 December 2023","externalUrl":null,"permalink":"/tags/longhorn/","section":"Tags","summary":"","title":"longhorn","type":"tags"},{"content":"I further explored the Harvester system and made some updates based on my discoveries.\nStorage # Expanding the Disk # Initially, we had one disk called /dev/sda, and Longhorn storage used the last partition, which was /dev/sda6.\nTo expand the volume, I followed a guide called AWS EC2 User Guide on Recognizing Expanded Volume in Linux. This process involves two main steps:\nExpanding the partition. Resizing the file system. However, since the growpart utility was not available, I used parted instead. Here are the steps I took:\nI deleted the 6th partition (/dev/sda6). Then, I recreated the partition. Finally, I saved the changes. Adding Additional Disks into Harvester # I followed the instructions in the Harvester Documentation to add another disk to Harvester. But after adding the new disk (/dev/sdb), it didn\u0026rsquo;t show up in the web interface.\nI fixed this problem by setting the auto-disk-provision-paths to /dev/sdb in the settings, as explained in the Harvester Advanced Documentation.\nI\u0026rsquo;m going to look into this issue further to understand it better.\nAdjusting Replication in Longhorn Storage # To change the replication settings in Longhorn storage, I did the following:\ncreated a new storage class and set the number of replicas to 1. used this new storage class when creating images and volumes. After making these changes, I could see in the Longhorn Dashboard that there was only one replica.\nIt\u0026rsquo;s worth noting that when setting up a PV on RKE2, the system usually uses the default storage class. To make things consistent, I made this new storage class the default one.\nEnable Longhorn UI Dashboard # To enable the Longhorn UI Dashboard:\nInitially, the Longhorn UI dashboard was not enabled by default. To turn it on, follow these steps: Start by enabling the support bundle as explained in the documentation. Then, you can access the UI from the \u0026ldquo;support\u0026rdquo; section. For detailed guidance, please consult the Harvester Documentation on Generating a Support Bundle.\nOptimize reserved CPU, memory, and storage: # You can make adjustments to the reserved CPU, memory, and storage settings within the Longhorn UI.\nHigh Availability # HA in VMs \u0026amp; Volumes # Testing Live Migration:\nDuring live migration, the VM moves to a different host while the storage remains on the original host. Scenario with Single Replica Volume:\nI created a VM with a volume having only one replica. Initially, both the storage and VM were on the same node. Then, I disabled the storage engine (disable scheduling and started eviction via Longhorn UI), causing the storage to relocate to a different host while the VM continued to operate normally. This demonstrates the independence of VM and storage HA and their ability to function on separate nodes.\nImpact of Storage Node Shutdown:\nWhen I shut down the host containing the sole storage, the VM became inoperative. This indicates that if all available storage becomes inaccessible, the VM will stop functioning.\nTesting with Multiple Replicas:\nIn cases where the volume has multiple replicas (e.g., 3) and the host with the storage is powered down, the VM remains operational. A new replica is created to maintain the required number of replicas. This showcases the HA feature within the volume.\nVM Host Shutdown Scenario:\nShutting down the host where the VM resides changes the VM status to \u0026ldquo;Not Ready.\u0026rdquo; Subsequently, the VM is relocated to another node and returns to normal operation. This scenario illustrates the HA capabilities of VMs.\nHA in Host Management # Master Node Promotion Limitations:\nI noticed that there is no direct method to manually promote a master node. Master node promotion only occurs when an existing master node is removed from Harvester. Additionally, there is no provision for adding extra master nodes to the system. This configuration restricts my control over which nodes can be promoted or excluded from promotion. Rancher Integration # Deploying RKE2 with Rancher:\nI have successfully deployed RKE2 using Rancher. Cluster scaling operations function smoothly. Upgrading RKE2 has been successful without major issues. Testing backup and restoration processes for the etcd database yielded positive results. The integration of PV in an RKE2 cluster (downstream) with Longhorn storage within Harvester (upstream) has been successful. Although there have been occasional bugs, the overall process has been satisfactory.\nNetworking # Setup vlan # Harvester config:\nConfiguring VLAN on Harvester is relatively straightforward. It involves creating a new VM network and assigning a VLAN ID. Proxmox config (for Hosting Harvester):\nTo establish VLANs on the bridge interface within Proxmox, additional settings are necessary. I utilized vmbr1 as the bridge interface for all Harvester hosts. For setting up NAT on Proxmox, you can refer to the guide provided at this link: Network config - sysadmin network masquerading.\nFor discussions related to VLAN config on the Proxmox bridge, you can find valuable insights and assistance in this forum thread: VLAN config in Cluster PVE 8.0.3.\nvi /etc/network/interfaces\nauto vmbr1 #private sub network iface vmbr1 inet static address 203.0.113.1/24 bridge-ports none bridge-stp off + bridge-vlan-aware yes + bridge-vids 114 bridge-fd 0 post-up echo 1 \u0026gt; /proc/sys/net/ipv4/ip_forward - post-up iptables -t nat -A POSTROUTING -s \u0026#39;203.0.0.0/24\u0026#39; -o vmbr0 -j MASQUERADE + post-up iptables -t nat -A POSTROUTING -s \u0026#39;203.0.0.0/16\u0026#39; -o vmbr0 -j MASQUERADE post-up iptables -t raw -I PREROUTING -i fwbr+ -j CT --zone 1 - post-down iptables -t nat -D POSTROUTING -s \u0026#39;203.0.0.0/24\u0026#39; -o vmbr0 -j MASQUERADE + post-down iptables -t nat -D POSTROUTING -s \u0026#39;203.0.0.0/16\u0026#39; -o vmbr0 -j MASQUERADE post-down iptables -t raw -D PREROUTING -i fwbr+ -j CT --zone 1 +auto vmbr1.114 +iface vmbr1.114 inet static + address 203.0.114.1/24 DHCP Server config (Ubuntu):\nAdd VLAN settings to the Netplan config in /etc/netplan/50-cloud-init.yaml. Define VLAN 114 on interface ens19 with an IP range of 203.0.114.182/24. + vlans: + ens19.114: + id: 114 + link: ens19 + addresses: [203.0.114.182/24] Dnsmasq config:\n-interface=ens19,lo +interface=ens19,ens19.114,lo ... +dhcp-range=ens19.114,203.0.114.2,203.0.114.254 +dhcp-option=ens19.114,3,203.0.114.1 +dhcp-host=BC:24:11:9B:39:4A,test,203.0.114.185 Setup bond # Post-Installation Network Bonding:\nI experimented with configuring network bonding on Harvester after its installation. For detailed guidance, I referred to the Harvester Documentation on Updating Configuration. Bonding Setup During Installation:\nThe Harvester documentation also provides instructions for setting up bonding during the installation process. This can be found in the Harvester Configuration Section. install: mode: create management_interface: interfaces: - name: ens18 + - name: ens19 method: dhcp bond_options: mode: active-backup miimon: 100 In this setup, the bond config utilizes the MAC address of the first listed interface (in this case, ens18). Consequently, the DHCP config should align with the MAC address of the first interface to ensure proper functioning. ","date":"16 December 2023","externalUrl":null,"permalink":"/posts/more-on-harvester/","section":"Posts","summary":"I further explored the Harvester system and made some updates based on my discoveries.\nStorage # Expanding the Disk # Initially, we had one disk called /dev/sda, and Longhorn storage used the last partition, which was /dev/sda6.","title":"More on harvester","type":"posts"},{"content":"","date":"16 December 2023","externalUrl":null,"permalink":"/tags/pxeboot/","section":"Tags","summary":"","title":"pxeboot","type":"tags"},{"content":"I recently experimented with PXE boot and auto-installation for various OS in my lab setup. Here is the process and details sharing with you.\nNetboot install process # When the VM is started, it enters the UEFI. The VM sends a DHCP request to the broadcast address. The DHCP server responds to this request by leasing an IP address to the VM. Along with the IP address, the DHCP server provides the location of the boot image (grubx64.efi for UEFI boot). The VM then downloads the boot image from the PXE server and start the GRUB2 bootloader. GRUB2 loads the grub.cfg file, which contains the config for the boot menu. The boot menu appears, presenting the user with different OS options to install. Once the user selects the desired OS, the installation process begins. Workflow # Configure the dnsmasq, tftp, and Apache servers. Set up DHCP for a test VM. Prepare the boot loader. Customize the test VM to boot using grub2 and load the grub2.cfg file, optionally including a dummy menu entry. Prepare the ISO, extract the kernel, initrd, repo and retrieve the boot command from the official documentation. If feasible, conduct a manual installation using qemu with a CD-ROM to obtain the auto-installation script. Further develop the auto-installation script within the qemu environment. Update the auto-installation script Update the grub2.cfg file - crafting the appropriate menu entry based on the qemu command. Commence the netboot installation in a real environment. Lessions Learnd # Separating netboot and auto install in testing # When conducting tests, it\u0026rsquo;s advisable to separate the PXE boot and auto-installation processes.\nThe primary objective of PXE booting is to ensure that the VM acquires the correct IP address and that the boot loader loads correctly.\nTo effectively test the auto-installation, it\u0026rsquo;s best to do so independently.\nYou can carry out the auto-installation testing using QEMU/KVM.\nBy directly initiating the VM with specific kernel, initrd, and boot options, you can bypass the PXE boot and boot loader stages, proceeding directly into the installation process.\nThis approach not only saves time by eliminating the need to wait for the BIOS and boot loader but also isolates any potential network issues related to PXE booting.\nDebugging with Serial Display # Initially, I opted to connect a serial port to the VM for debugging purposes.\nHowever, it\u0026rsquo;s important to note that many installers and kernels have limited support for serial output, often resulting in a blank screen.\nAs a result, I transitioned to using X11 forwarding for GUI display on my local PC and stopped relying on the serial port unless it was explicitly recommended.\nUsing CD-ROM as a Troubleshooting Fallback # In cases where issues arise with QEMU, attaching a CD-ROM to the VM can help identify potential misconfigurations. CD-ROM installation should be considered as a last resort.\nFor instance, I encountered difficulties when attempting to initiate XCP-ng installation with UEFI, even when using a CD-ROM. This may indicate possible hardware compatibility issues.\nEnabling Logging # Enabling logs for DHCP, TFTP, and Apache is a crucial step for troubleshooting.\nThese logs provide in-depth insights into the booting and installation processes.\nWhen examining the TFTP logs, you can monitor requests for the boot loader (grubx64.efi), boot menu (grub.cfg), and kernel/ramdisk, aiding in the identification of required config and pinpointing any misconfigurations.\nSimultaneously, the Apache server\u0026rsquo;s access logs prove invaluable for comprehending the installation process.\nYou can observe the progress of the installation, such as the loading of the ISO or the retrieval of packages from the server.\nThis level of visibility is essential for ensuring that the installation proceeds as anticipated and for diagnosing any potential irregularities.\nObtain the auto-install script from manual installation (if feasible) # For instance, in the case of Ubuntu:\nWhen installation ubuntu using server installer, an auto-install file that allows you to replicate the installation process is generated and can be found at /var/log/installer/autoinstall-user-data.\nhttps://ubuntu.com/server/docs/install/autoinstall\nFor CentOS:\nAfter the installation is successfully completed, all the selections made during the installation process are saved in /root/anaconda-ks.cfg.\nhttps://docs.centos.org/en-US/centos/install-guide/Kickstart2/#sect-kickstart-file-create\nUniversal Boot loader # There are multiple ways to obtain the boot loader image. You can either install the required packages or copy them from an existing server.\nHowever, it\u0026rsquo;s important to note that different Linux distributions may compile these components differently, potentially missing necessary features and using varying config paths.\nAs an illustration, let\u0026rsquo;s consider how Ubuntu and CentOS handle their Grub configurations:\nIn the case of Ubuntu, Grub is loaded from a subdirectory named \u0026ldquo;grub\u0026rdquo; within the root directory (\u0026quot;/grub/grub.cfg\u0026quot;).\nhttps://ubuntu.com/server/docs/install/netboot-amd64\nCreate /srv/tftp/grub/grub.cfg that contains\nMeanwhile, CentOS loads Grub directly from the root directory (\u0026quot;/grub.cfg\u0026quot;).\nhttps://docs.centos.org/en-US/centos/install-guide/pxe-server/#sect-network-boot-setup-uefi\nAdd a configuration file named grub.cfg to the tftpboot/ directory. A sample configuration file at /var/lib/tftpboot/grub.cfg might look like:\nFollowing the official guides from each distribution can sometimes be confusing and may not work seamlessly when dealing with netboot across different distributions using a single bootloader.\nOne valuable feature I\u0026rsquo;ve encountered in the legacy boot loader pxelinux.0 is its ability to load config files based on MAC addresses. It automatically searches for the MAC address and, if not found, gracefully defaults to default. This functionality proves exceptionally useful when booting various machines with distinct menu configs.\nMore information on this feature can be found here: PXELINUX config.\nAnother essential feature is multiboot, which is necessary for XCP-ng installations in UEFI environments. (Yes, UEFI is ok actually, just not work on my lab setup)\nSo, I\u0026rsquo;m faced with two challenges:\nI need to determine the config paths (grub2.cfg). I want to incorporate custom features into the grub2 image. Regarding the first challenge, I\u0026rsquo;ve found a simple solution by examining the TFTP server\u0026rsquo;s access log to identify the config path. This helps me locate the necessary config files for my setup.\nFor the second challenge, I\u0026rsquo;ve discovered that Debian\u0026rsquo;s version of Grub can load configs based on MAC addresses, and I\u0026rsquo;ve learned that grub2-mkimage is the tool to create custom Grub images.\nHowever, it\u0026rsquo;s important to note that we have to specify the required modules ourselves, and the module list can typically be obtained from the source code of the Grub package in the specific distribution.\nYou can check this page to find out the commands to build the grub image.\nLab Setup # The Ubuntu guide located at https://ubuntu.com/server/docs/install/netboot-amd64 has already covered 80% of the necessary steps for setting up various components, including PXE, the Apache server, and obtaining essential components such as the kernel, ramdisk, Grub2 EFI, and pxelinux.0.\nHost System: Proxmox\nOS to be netboot:\nUbuntu 22.04 Rocky 9.2 XCP-ng 8.2 Harvester v1.2.1 Servers # PXE Server: 203.0.113.182 (ubuntu22.04) Test VM: 203.0.113.100 PXE Server and Test VM are in the same subnet: 203.0.113.0/24\nPXE Server # Installed with dnsmasq for TFTP and DHCP services Apache2 server for hosting installation files Test VM # Attached with an empty disk Boot order set to disk first, then network Boot mode: UEFI Services # Dnsmasq # Dnsmasq is primarily used for DHCP - it assigns IP addresses to servers and maps these IP addresses to the MAC addresses of the VMs.\nBut it also serves as the tftp server\nthe conf of Dnsmasq be like:\ninterface=ens19,lo bind-interfaces enable-tftp tftp-root=/srv/tftp log-queries log-dhcp dhcp-leasefile=/var/dnsmasq/leasefile dhcp-boot=pxelinux.0 dhcp-match=set:efi-x86_64,option:client-arch,7 dhcp-boot=tag:efi-x86_64,grubx64.efi dhcp-range=ens19,203.0.113.2,203.0.113.254 dhcp-option=ens19,3,203.0.113.1 dhcp-host=BC:24:11:3A:C4:49,testvm,203.0.113.100 Breakdowns:\nlog-queries log-dhcp I enabled logging to keep track of the dhcp / tftp requests for troubleshooting.\ndhcp-boot=pxelinux.0 dhcp-match=set:efi-x86_64,option:client-arch,7 dhcp-boot=tag:efi-x86_64,grubx64.efi This is a crucial setting. It directs the server to boot using grubx64.efi (GRUB2 EFI image) if it is in UEFI mode. In case of legacy mode, it uses pxelinux.0.\ndhcp-option=ens19,3,203.0.113.1 This option is set to inform the VM that the gateway IP is 203.0.113.1.\ndhcp-range=ens19,203.0.113.2,203.0.113.254 dhcp-host=BC:24:11:3A:C4:49,testvm,203.0.113.100 This specifies that any DHCP requests coming from ens19 will have an IP lease range between 203.0.113.2 and 203.0.113.254.\nFurthermore, dhcp-host= is used to assign a specific IP (reverse 203.0.113.100) to a particular server, identified by its MAC address.\nTFTP # TFTP server is used for serving the boot loader, its config (boot menu), kernel and ramdisk.\nI opted for UEFI boot and chose GRUB2 as the boot loader.\nThe GRUB config file is located at EFI/BOOT/grub.cfg, and grubx64.efi is used as the GRUB EFI image.\nFor my XCP-ng installation, I had to switch to legacy boot due to some issues with UEFI.\nIn this scenario, mboot.c32, menu.c32, and pxelinux.0 were utilized for the legacy boot process. The menu configuration for this is found in pxelinux.cfg/default.\nBoth the RAM disk and the kernel are extracted directly from the ISOs (mount -o loop xxx.iso /mnt) of the OS.\nTFTP Server Folder Structure:\n/srv └── tftp ├── EFI │ └── BOOT │ └── grub.cfg ├── grubx64.efi ├── harvester-1.2.1 │ ├── initrd │ └── vmlinuz ├── mboot.c32 ├── menu.c32 ├── pxelinux.0 ├── pxelinux.cfg │ └── default ├── rocky-9.3 │ ├── initrd │ └── vmlinuz ├── ubuntu-22.04 │ ├── initrd │ └── vmlinuz └── xcp-ng-8.3.0 ├── install.img ├── vmlinuz └── xen grub.cfg\n... menuentry \u0026#39;Ubuntu 22.04\u0026#39; { linuxefi /ubuntu-22.04/vmlinuz console=tty1 root=/dev/ram0 ramdisk_size=3000000 ip=dhcp url=http://192.168.0.182/iso/ubuntu-22.04.3-live-server-amd64.iso autoinstall ds=nocloud-net\\;s=http://192.168.0.182/ubuntu/cloud-init/ cloud-config-url=/dev/null initrdefi /ubuntu-22.04/initrd } menuentry \u0026#39;Rocky 9.3\u0026#39; { linuxefi /rocky-9.3/vmlinuz console=tty1 ip=dhcp inst.ks=http://192.168.0.182/rocky/ks.cfg inst.repo=http://192.168.0.182/rocky/repo initrdefi /rocky-9.3/initrd } menuentry \u0026#39;xcp-ng 8.3.0\u0026#39; { multiboot2 /xcp-ng-8.3.0/xen dom0_mem=2048M,max:2048M watchdog dom0_max_vcpus=4 com1=115200,8n1 console=com1,vga module2 /xcp-ng-8.3.0/vmlinuz console=hvc0 console=ttyS0 answerfile=http://192.168.0.182/xcp-ng/answerfile.xml install module2 /xcp-ng-8.3.0/install.img } menuentry \u0026#39;harvester 1.2.1 (Create)\u0026#39; { linuxefi /harvester-1.2.1/vmlinuz ip=dhcp net.ifnames=1 rd.cos.disable rd.noverifyssl console=tty1 root=live:http://192.168.0.182/harvester/rootfs.squashfs harvester.install.automatic=true harvester.install.config_url=http://192.168.0.182/harvester/config-create.yaml initrdefi /harvester-1.2.1/initrd } menuentry \u0026#39;harvester 1.2.1 (Join)\u0026#39; { linuxefi /harvester-1.2.1/vmlinuz ip=dhcp net.ifnames=1 rd.cos.disable rd.noverifyssl console=tty1 root=live:http://192.168.0.182/harvester/rootfs.squashfs harvester.install.automatic=true harvester.install.config_url=http://192.168.0.182/harvester/config-join.yaml initrdefi /harvester-1.2.1/initrd } ... There are primarily two types of boot menu config to work with:\nGrub2 EFI: This config is intended for systems that use the UEFI standard for booting, representing a modern approach to the boot process (grub2.cfg).\nLegacy Boot: The legacy boot config is designed for older systems that lack UEFI support. It\u0026rsquo;s crucial to have this config as a fallback to ensure compatibility with older hardware (pxelinux.cfg/default).\nBoth config serve the same purpose, which is to boot the kernel with a ramdisk, and they share similarities in their command structures.\nFor specific boot config and commands for each operating system, you can refer to their respective netboot documentation:\nxcp-ng. Ubuntu. Rocky, given its similarity to CentOS. Harvester. ChatGPT can be a valuable tool in converting these configurations from one format to another, such as from grub.cfg to pxelinux.cfg/default.\nFurthermore, these commands can also be adapted for use with QEMU. This is especially useful for developing and testing auto-install config.\nFor instance, you can simulate the installation process in a testing environment, as I did with Harvester, before deploying it in a live setting.\nqemu-system-x86_64 \\ -name harvester \\ -enable-kvm \\ -m 16384 \\ -cpu host \\ -smp 16,sockets=2 \\ -hda /vms/harvester.img \\ -nic user,hostfwd=tcp:127.0.0.1:1025-:22,dhcpstart=10.0.2.101 \\ -bios /usr/share/ovmf/OVMF.fd \\ -kernel harvester-v1.2.1-vmlinuz-amd64 \\ -initrd harvester-v1.2.1-initrd-amd64 \\ -append \u0026#39;ip=dhcp net.ifnames=1 rd.cos.disable rd.noverifyssl console=ttyS0 root=live:http://192.168.0.182/harvester/harvester-v1.2.1-rootfs-amd64.squashfs harvester.install.automatic=true harvester.install.config_url=http://192.168.0.182/harvester/config-create.yaml\u0026#39; \\ -no-reboot Apache # Apache functions as an HTTP server responsible for delivering essential files required for OS installations.\nWhen initiating an OS boot from the kernel, there is typically an option available to define auto-installation parameters, including:\ninstallation config (eg: keyboard, timezone, network, disk partition, post install) installation source (eg: cdrom or repo) Config:\nHarvester uses yaml Rocky, CentOS, and RedHat use (kickstart)[https://docs.centos.org/en-US/8-docs/advanced-install/assembly_kickstart-installation-basics/] Ubuntu uses autoinstall config, which is a module of cloud-init XCP-ng uses answerfile Source:\nHarvester and Ubuntu are ISO-based, meaning the entire ISO is loaded into RAM.\nThis requires the VM to have sufficient RAM to hold the entire content of the CD.\nXCP-ng and Rocky installations use repository, which involves extracting contents from the ISO and copying them to Apache.\nWhen copying files from the ISO to the Apache server, it\u0026rsquo;s crucial not to overlook hidden files (dot files). These files can be essential for a valid repo.\nApache Server Folder Structure:\n/var/www/html . ├── harvester │ ├── config-create.yaml │ └── config-join.yaml ├── iso │ ├── harvester-v1.2.1-amd64.iso │ ├── openSUSE-Leap-15.0-NET-x86_64-Current.iso │ ├── Rocky-9.3-x86_64-minimal.iso │ ├── ubuntu-22.04.3-live-server-amd64.iso │ ├── xcp-ng-8.3.0-beta1.iso │ └── xcp-ng-8.3.0-beta1-netinstall.iso ├── README.md ├── rocky │ ├── ks.cfg │ └── repo │ ├── BaseOS │ ├── EFI │ ├── images │ ├── isolinux │ ├── LICENSE │ ├── media.repo │ ├── minimal │ └── rocky.img ├── ubuntu │ └── cloud-init │ ├── meta-data │ └── user-data └── xcp-ng ├── answerfile.xml ├── noresync.sh ├── post-install-script └── repo ├── boot ├── EFI ├── EULA ├── install.img ├── LICENSES ├── Packages ├── repodata └── RPM-GPG-KEY-xcpng ","date":"16 December 2023","externalUrl":null,"permalink":"/posts/pxeboot/","section":"Posts","summary":"I recently experimented with PXE boot and auto-installation for various OS in my lab setup. Here is the process and details sharing with you.\nNetboot install process # When the VM is started, it enters the UEFI.","title":"pxeboot","type":"posts"},{"content":"","date":"16 December 2023","externalUrl":null,"permalink":"/tags/rancher/","section":"Tags","summary":"","title":"rancher","type":"tags"},{"content":"","date":"5 December 2023","externalUrl":null,"permalink":"/tags/firewall/","section":"Tags","summary":"","title":"firewall","type":"tags"},{"content":"","date":"5 December 2023","externalUrl":null,"permalink":"/tags/firewalld/","section":"Tags","summary":"","title":"firewalld","type":"tags"},{"content":"","date":"5 December 2023","externalUrl":null,"permalink":"/tags/security/","section":"Tags","summary":"","title":"security","type":"tags"},{"content":"Topology:\nsubnet100: 192.168.100.0/24 web01: 192.168.100.111 web02: 192.168.100.112 subnet200: 192.168.200.0/24 In firewalld, there is a trusted zone:\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;utf-8\u0026#34;?\u0026gt; \u0026lt;zone target=\u0026#34;ACCEPT\u0026#34;\u0026gt; \u0026lt;short\u0026gt;Trusted\u0026lt;/short\u0026gt; \u0026lt;description\u0026gt;Trusted\u0026lt;/description\u0026gt; \u0026lt;service name=\u0026#34;ssh\u0026#34;/\u0026gt; \u0026lt;source address=\u0026#34;192.168.100.0/24\u0026#34;/\u0026gt; \u0026lt;forward/\u0026gt; \u0026lt;/zone\u0026gt; I initially expected that only ssh would be open to subnet100. However, I discovered that all ports were open to subnet100.\nThis was due to the target=\u0026quot;ACCEPT\u0026quot; setting, which accepts all traffic that does not match any rule in the zone.\nTo rectify this, I removed target=\u0026quot;ACCEPT\u0026quot;. Now, any traffic that does not match a rule will be rejected.\nAs a result, only the ssh service can be accessed by subnet100.\nNext, let\u0026rsquo;s examine the following setup:\npublic: http subnet100: ssh, mysql, galera \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;utf-8\u0026#34;?\u0026gt; \u0026lt;zone\u0026gt; \u0026lt;short\u0026gt;Public\u0026lt;/short\u0026gt; \u0026lt;description\u0026gt;Public\u0026lt;/description\u0026gt; \u0026lt;interface name=\u0026#34;eth0\u0026#34;/\u0026gt; \u0026lt;service name=\u0026#34;http\u0026#34;/\u0026gt; \u0026lt;forward/\u0026gt; \u0026lt;/zone\u0026gt; \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;utf-8\u0026#34;?\u0026gt; \u0026lt;zone\u0026gt; \u0026lt;short\u0026gt;Trusted\u0026lt;/short\u0026gt; \u0026lt;description\u0026gt;Trusted\u0026lt;/description\u0026gt; \u0026lt;source address=\u0026#34;192.168.100.0/24\u0026#34;/\u0026gt; \u0026lt;service name=\u0026#34;ssh\u0026#34;/\u0026gt; \u0026lt;service name=\u0026#34;mysql\u0026#34;/\u0026gt; \u0026lt;service name=\u0026#34;galera\u0026#34;/\u0026gt; \u0026lt;/zone\u0026gt; I anticipated that only traffic from subnet100 would be able to access ssh, mysql and galera.\nhttp is accessible publicly, including subnet100.\nHowever, this was not the case - web01 could not access http://web02.\nThis issue relates to the concept of zones in firewalld.\nfirewall-cmd --get-active-zone public interfaces: eth0 trusted sources: 192.168.100.0/24 firewall-cmd --get-default-zone public This show there are two zones: public and trusted in firewall.\npublic is set as the default zone.\nThe key concept here is that only active zones enforce their rules. An active zone must have either an interface or a source defined in its config.\nIf the traffic doesn\u0026rsquo;t match any zone, it defaults to the public zone.\nSince web02 fall into the trusted zone, which initially did not have an http rule, their http traffic was dropped.\nTo resolve this, http can be added to the trusted zone config:\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;utf-8\u0026#34;?\u0026gt; \u0026lt;zone\u0026gt; \u0026lt;short\u0026gt;Trusted\u0026lt;/short\u0026gt; \u0026lt;description\u0026gt;Trusted\u0026lt;/description\u0026gt; \u0026lt;source address=\u0026#34;192.168.100.0/24\u0026#34;/\u0026gt; \u0026lt;service name=\u0026#34;ssh\u0026#34;/\u0026gt; + \u0026lt;service name=\u0026#34;http\u0026#34;/\u0026gt; \u0026lt;service name=\u0026#34;mysql\u0026#34;/\u0026gt; \u0026lt;service name=\u0026#34;galera\u0026#34;/\u0026gt; \u0026lt;/zone\u0026gt; To further refine the network rules, if we want to limit http access exclusively to web02, we can employ a rich rule in firewalld.\nThis approach is contingent on web02 being part of the trusted zone.\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;utf-8\u0026#34;?\u0026gt; \u0026lt;zone\u0026gt; \u0026lt;short\u0026gt;Trusted\u0026lt;/short\u0026gt; \u0026lt;description\u0026gt;Trusted\u0026lt;/description\u0026gt; \u0026lt;source address=\u0026#34;192.168.100.0/24\u0026#34;/\u0026gt; \u0026lt;service name=\u0026#34;ssh\u0026#34;/\u0026gt; \u0026lt;service name=\u0026#34;mysql\u0026#34;/\u0026gt; \u0026lt;service name=\u0026#34;galera\u0026#34;/\u0026gt; + \u0026lt;rule family=\u0026#34;ipv4\u0026#34;\u0026gt; + \u0026lt;source address=\u0026#34;192.168.100.112\u0026#34;/\u0026gt; + \u0026lt;service name=\u0026#34;http\u0026#34;/\u0026gt; + \u0026lt;accept/\u0026gt; \u0026lt;/rule\u0026gt; \u0026lt;/zone\u0026gt; In other scenario where we want\nsubnet200: http subnet100, subnet200: ssh mysql and galera \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;utf-8\u0026#34;?\u0026gt; \u0026lt;zone\u0026gt; \u0026lt;short\u0026gt;Trusted\u0026lt;/short\u0026gt; \u0026lt;description\u0026gt;Trusted\u0026lt;/description\u0026gt; \u0026lt;source address=\u0026#34;192.168.100.0/24\u0026#34;/\u0026gt; \u0026lt;service name=\u0026#34;ssh\u0026#34;/\u0026gt; \u0026lt;service name=\u0026#34;mysql\u0026#34;/\u0026gt; \u0026lt;service name=\u0026#34;galera\u0026#34;/\u0026gt; + \u0026lt;rule family=\u0026#34;ipv4\u0026#34;\u0026gt; + \u0026lt;source address=\u0026#34;192.168.200.0/24\u0026#34;/\u0026gt; + \u0026lt;service name=\u0026#34;http\u0026#34;/\u0026gt; + \u0026lt;accept/\u0026gt; \u0026lt;/rule\u0026gt; \u0026lt;/zone\u0026gt; But this config will not work effectively.\nIn this config, the rule specified for subnet200 within the trusted zone will not be effective, as the zone itself does not apply to subnet200.\nfirewall-cmd --get-active-zone public interfaces: eth0 trusted sources: 192.168.100.0/24 It\u0026rsquo;s still 192.168.100.0/24.\nTo address the issue of allowing specific services for different subnets, the firewalld config can be modified by adding subnet200 as a source in the trusted zone.\nThis change allows both subnet100 and subnet200 to be active within the trusted zone.\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;utf-8\u0026#34;?\u0026gt; \u0026lt;zone\u0026gt; \u0026lt;short\u0026gt;Trusted\u0026lt;/short\u0026gt; \u0026lt;description\u0026gt;Trusted\u0026lt;/description\u0026gt; \u0026lt;source address=\u0026#34;192.168.100.0/24\u0026#34;/\u0026gt; + \u0026lt;source address=\u0026#34;192.168.200.0/24\u0026#34;/\u0026gt; \u0026lt;service name=\u0026#34;http\u0026#34;/\u0026gt; \u0026lt;service name=\u0026#34;ssh\u0026#34;/\u0026gt; \u0026lt;service name=\u0026#34;mysql\u0026#34;/\u0026gt; \u0026lt;service name=\u0026#34;galera\u0026#34;/\u0026gt; \u0026lt;rule family=\u0026#34;ipv4\u0026#34;\u0026gt; \u0026lt;source address=\u0026#34;192.168.200.0/24\u0026#34;/\u0026gt; \u0026lt;service name=\u0026#34;http\u0026#34;/\u0026gt; \u0026lt;accept/\u0026gt; \u0026lt;/rule\u0026gt; \u0026lt;/zone\u0026gt; However, this config permits both subnets to access ssh, mysql and galera, which may not be desirable.\nTo control access more granularly, the config can use rich rules to specify which services are accessible by which subnet.\nAlternatively, creating a new zone can also serve this purpose.\nIn this example,\nsubnet200: ssh subnet100: mysql galera subnet100, subnet200: http \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;utf-8\u0026#34;?\u0026gt; \u0026lt;zone\u0026gt; \u0026lt;short\u0026gt;Trusted\u0026lt;/short\u0026gt; \u0026lt;description\u0026gt;Trusted\u0026lt;/description\u0026gt; \u0026lt;source address=\u0026#34;192.168.100.0/24\u0026#34;/\u0026gt; + \u0026lt;source address=\u0026#34;192.168.200.0/24\u0026#34;/\u0026gt; \u0026lt;service name=\u0026#34;http\u0026#34;/\u0026gt; + \u0026lt;rule family=\u0026#34;ipv4\u0026#34;\u0026gt; + \u0026lt;source address=\u0026#34;192.168.200.0/24\u0026#34;/\u0026gt; + \u0026lt;service name=\u0026#34;ssh\u0026#34;/\u0026gt; + \u0026lt;accept/\u0026gt; + \u0026lt;/rule\u0026gt; + \u0026lt;rule family=\u0026#34;ipv4\u0026#34;\u0026gt; + \u0026lt;source address=\u0026#34;192.168.100.0/24\u0026#34;/\u0026gt; + \u0026lt;service name=\u0026#34;mysql\u0026#34;/\u0026gt; + \u0026lt;accept/\u0026gt; + \u0026lt;/rule\u0026gt; + \u0026lt;rule family=\u0026#34;ipv4\u0026#34;\u0026gt; + \u0026lt;source address=\u0026#34;192.168.100.0/24\u0026#34;/\u0026gt; + \u0026lt;service name=\u0026#34;galera\u0026#34;/\u0026gt; + \u0026lt;accept/\u0026gt; + \u0026lt;/rule\u0026gt; \u0026lt;/zone\u0026gt; This config effectively segregates the services each subnet can access, ensuring a more secure and controlled network environment.\n","date":"5 December 2023","externalUrl":null,"permalink":"/posts/firewall-zone/","section":"Posts","summary":"Topology:\nsubnet100: 192.168.100.0/24 web01: 192.168.100.111 web02: 192.168.100.112 subnet200: 192.168.200.0/24 In firewalld, there is a trusted zone:\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;utf-8\u0026#34;?\u0026gt; \u0026lt;zone target=\u0026#34;ACCEPT\u0026#34;\u0026gt; \u0026lt;short\u0026gt;Trusted\u0026lt;/short\u0026gt; \u0026lt;description\u0026gt;Trusted\u0026lt;/description\u0026gt; \u0026lt;service name=\u0026#34;ssh\u0026#34;/\u0026gt; \u0026lt;source address=\u0026#34;192.168.100.0/24\u0026#34;/\u0026gt; \u0026lt;forward/\u0026gt; \u0026lt;/zone\u0026gt; I initially expected that only ssh would be open to subnet100.","title":"Understanding firewalld zone","type":"posts"},{"content":"Purpose of this note: The existing online tutorials provide limited information about KVM.\nMany tutorials reference the use of virt-manager, which is akin to a GUI tool for creating virtual machines.\nHowever, my interest lies in a deeper understanding of KVM, which these tutorials do not adequately cover.\nI would like to begin with\na high-level overview break down KVM into different aspects and then combine all the details into a lab Setup Specification # Regarding the upcoming commands/guide, I am using Ubuntu 22.04 as my host operating system, and the guest OS is openSUSE 15.6.\nObjectives # Focus primarily on QEMU, though insights into libvirt are also valuable, as its underlying configurations can aid in verifying settings. Gain an understanding of the differences and uses of serial vs. monitor in QEMU, as well as the console. Learn about VNC and SPICE in QEMU. Comprehend networking aspects within KVM. Methodology # Creating VMs with KVM can be approached in two ways:\nLibvirt - Higher Level:\nUtilizes the libvirtd daemon, allowing VM and network definitions (in XML) to persist and enabling autostart on boot. The default bridge network default comes with bridge device named virbr0. QEMU - Lower Level:\nOperates solely through the command line. For networking, it involves using a helper tool to add a tap network to a bridge. Resources for reference # I\u0026rsquo;ve found two GitHub projects that have been tremendously helpful in my study from scratch. The labs I\u0026rsquo;ve worked on are essentially based on these projects:\ncreate-vm - Using libvirt vm-provision - Leveraging QEMU In addition to these projects, I\u0026rsquo;ve also come across three command builders for QEMU and virt-install that, despite not being updated for a while, are incredibly valuable for studying:\nCommand Line Libvirt Command Line QEMU QEMU Command Generator For in-depth information and documentation, you can refer to the following:\nThe man page of qemu-system-x86_64 The wiki page of QEMU provides a wealth of details and examples. On the Arch Linux wiki, you\u0026rsquo;ll find a plethora of resources. Additionally, if you\u0026rsquo;re looking for guidance on using virt-install, RedHat has a comprehensive guide available:\nRedHat\u0026rsquo;s Virt-Install Guide For installation guides, you can follow these resources:\nUbuntu: Virtualization with Libvirt CentOS: Creating Guest VMs on Linux KVM Prepartion # Preparation for create VM with KVM:\nInstall related software packages Cloud image Disk image Cloud-init ISO First, ensure qemu and virt-install are installed.\nThen, obtain a cloud image to serve as the base disk.\nCreate a copy of this image (backed by the cloud image) to form the disk image for our VM.\nConfigure the user and network settings using cloud-init, which is packed into an ISO and attached to the VM as a CD-ROM.\nQEMU Shortcuts # In QEMU, there are many ways to achieve the same thing with different parameters and numerous shortcuts. This can be quite confusing as different tutorials, websites, and discussions often use varying methods.\nFor clarity, I prefer to convert the parameters to their shortcut forms:\n-hda = -drive -cdrom = -drive -drive = -blockdev -device -audio = -audiodev -device -nic = -netdev -device For example:\n-drive file=linux.img,format=qcow2,if=virtio is equivalent to:\n-hda linux.img And:\n-drive file=cloud-init.img,media=cdrom,if=virtio is the same as:\n-hda cloud-init.img Also:\n-chardev socket,id=char0,path=u1-serial,server=on,wait=off -serial chardev:char0 can be shortened to:\n-serial unix:u1-serial,server,nowait Furthermore:\n-netdev bridge,id=hn0 -device virtio-net-pci,netdev=hn0,id=nic1 -netdev tap,helper=/usr/local/libexec/qemu-bridge-helper,id=hn0 -device virtio-net-pci,netdev=hn0,id=nic1 are equivalent to:\n-nic bridge,model=virtio-net-pci Networking # The common network types for VM access include:\nUser Bridge/Tap From my perspective, tap and bridge networks function similarly. When using a bridge, the qemu bridge-helper script creates a tap device and adds it to the bridge. In the case of a tap, up and down scripts achieve a similar outcome.\nThe user network type is akin to NAT. To SSH into the VM, setting up port forwarding is necessary.\nTo create a network using libvirt, configure it with a DHCP server using dnsmasq. The following commands are used:\nCreates the network temporarily\nvirsh create \u0026lt;network-config\u0026gt; Creates the network permanently and ensures the network starts automatically\nvirsh define \u0026lt;network-config\u0026gt; virsh net-autostart \u0026lt;network\u0026gt; Starts the defined network\nvirsh start \u0026lt;network\u0026gt; However, with qemu alone, no network is created automatically. The user must manually set up everything,\nTo set up a network on QEMU, you need to:\nCreate the network device. Set up a DHCP server (if needed), typically by creating a bridge. There are various methods for this:\nTemporary setup using iproute2, which is easy but not persistent. Persistent setups differ across operating systems and are not well documented. For instance, Ubuntu uses netplan, older versions or Debian use /etc/network/interfaces, and CentOS/RedHat uses /etc/sysconfig/network/ifcfg-\u0026lt;dev\u0026gt; A hack way is using libvirt for network creation. I\u0026rsquo;ve gathered some useful configurations from the web:\nUbuntu (netplan)\nbunch of example Centos / Redhat (/etc/sysconfig/network/)\nbasic bridge bond + bridge + vlan Opensuse - wicked (/etc/sysconfig/network/) - I can\u0026rsquo;t found a web page related to this\nbunch of examples Debian / Oldder Ubuntu (/etc/network/interfaces)\nbasic bridge vlan bond For DHCP, if an existing one is available, that\u0026rsquo;s good, but if you need one for testing, you can use libvirt to create a bridge network and DHCP server.\nFor those not using libvirt, here\u0026rsquo;s a snippet to create a temporary DHCP server with\ndnsmasq --interface=br0 --bind-interfaces --dhcp-range=172.20.0.2,172.20.255.254 Ref: QEMU Host-only Networking guide on the Arch Linux wiki.\nCloud Init # Since the cloud image doesn\u0026rsquo;t have a user password configuration, logging into the system via console isn\u0026rsquo;t possible.\nTherefore, SSH is necessary to access the system.\nThis requires the VM to have network connectivity and the user to have the SSH key or password set on the system. This setup can be achieved using cloud-init.\nTo facilitate this, at least three files are required:\nmeta-data: meta-data user-data: user-data network-config: network-config These files can either be packed into an ISO and attached to the VM, or served to the VM through a simple HTTP server.\nI note that for openSUSE, cloud-init\u0026rsquo;s network-config version 2 doesn\u0026rsquo;t work, so version 1 must be used.\nDisplay - VNC and Spice # Virt-viewer can be utilized for both VNC and Spice connections.\nEnsure that X11 forwarding is enabled on ssh server in the /etc/ssh/sshd_config file. On client, this can be activated with\nssh -X Since I\u0026rsquo;m using macOS, I have installed XQuartz as X11 Client.\nFor the lab purposes, I\u0026rsquo;m using the root account, so I use\nsudo -E bash to maintain the $DISPLAY variables for X11.\nIt\u0026rsquo;s crucial to set XQuartz\u0026rsquo;s X11 preference settings correctly.\nSpecifically, in the output settings \u0026gt; colors, select \u0026lsquo;from display\u0026rsquo; and avoid options like 256color.\nPreviously, I had set it to 256color to speed up the x2go redning, but this caused issues when connecting to Spice (the screen is blank).\nMost of the commands for Spice are mentioned in the official user manual, which can be found at Spice User Manual.\nFor instance, to start Spice, use the command:\n-vga qxl -spice port=3001 -soundhw hda -device virtio-serial -chardev spicevmc,id=vdagent,debug=0,name=vdagent -device virtserialport,chardev=vdagent,name=com.redhat.spice.0 Then, connect to the Spice server using:\nremote-viewer spice://127.0.0.1:3001 Other features like password authentication, TLS, and SASL authentication add additional security to the connection, which is good to know.\nIn the future, I would also like to explore USB and audio redirection/pasteboard functionalities.\nSerial and Console # Serial refers to the VM console, which I frequently use. Console refers to the QEMU console, which I rarely use for myself. There are different ways to connect to serial/console, and I\u0026rsquo;ll list those I have experience with:\nsocket telnet pty stdio Commands for connecting to serial/console:\n-serial unix:u1-serial,server,nowait -serial stdio,server,nowait -serial pty,server,nowait -serial telnet::7000,server,nowait To connect to serial from socket:\nsocat - UNIX-CONNECT:u1-serial To connect to serial from telnet:\ntelnet host port To connect to serial from pty:\nscreen /dev/pts/2 However, this can be messy for me. The style is often broken as it doesn\u0026rsquo;t resolve colors and newlines correctly.\nSetting export TERM=screen-256color improves the color issue but doesn\u0026rsquo;t resolve the newline problem.\nstdio:\nUseful for running QEMU in the foreground (without --daemonize) for quick testing. Labs # Preparation # Software Packages:\nFor QEMU:\napt install -y qemu-kvm For libvirt:\napt install -y qemu-kvm libvirt-daemon-system virtinst To create cloud-init ISO:\napt install -y genisoimage Cloud Image:\nwget https://download.opensuse.org/repositories/Cloud:/Images:/Leap_15.5/images/openSUSE-Leap-15.5.x86_64-1.0.0-NoCloud-Build1.143.qcow2 Disk Image:\nqemu-img create -b openSUSE-Leap-15.5.x86_64-1.0.0-NoCloud-Build1.143.qcow2 -f qcow2 -F qcow2 \u0026#34;linux.img\u0026#34; \u0026#34;20G\u0026#34; Cloud-init ISO:\nmkdir /lab/cloud-init -p cat \u0026lt;\u0026lt; EOF | tee /lab/cloud-init/network-config network: version: 1 config: - type: physical name: eth0 subnets: - type: dhcp - type: physical name: eth1 subnets: - type: dhcp EOF cat \u0026lt;\u0026lt; EOF | tee /lab/cloud-init/user-data #cloud-config users: - name: \u0026lt;user\u0026gt; sudo: [\u0026#34;ALL=(ALL) NOPASSWD:ALL\u0026#34;] groups: sudo shell: /bin/bash homedir: /home/hugotse ssh_authorized_keys: - \u0026#34;\u0026lt;SSH_PUBLIC_KEY\u0026gt;\u0026#34; EOF cat \u0026lt;\u0026lt; EOF | tee /lab/cloud-init/meta-data instance-id: u1 local-hostname: u1 EOF genisoimage -output \u0026#34;/lab/cloud-init.img\u0026#34; -volid cidata -rational-rock -joliet /lab/cloud-init/* Libvirt # To create VM with libvirt:\nCreate a bridge network:\ncat \u0026lt;\u0026lt; EOF | tee /lab/bridge.xml \u0026lt;network connections=\u0026#39;2\u0026#39;\u0026gt; \u0026lt;name\u0026gt;br0\u0026lt;/name\u0026gt; \u0026lt;forward mode=\u0026#39;nat\u0026#39;/\u0026gt; \u0026lt;bridge name=\u0026#39;br0\u0026#39; stp=\u0026#39;on\u0026#39; delay=\u0026#39;0\u0026#39;/\u0026gt; \u0026lt;mac address=\u0026#39;52:54:00:8a:e9:7c\u0026#39;/\u0026gt; \u0026lt;ip address=\u0026#39;192.168.123.1\u0026#39; netmask=\u0026#39;255.255.255.0\u0026#39;\u0026gt; \u0026lt;dhcp\u0026gt; \u0026lt;range start=\u0026#39;192.168.123.2\u0026#39; end=\u0026#39;192.168.123.254\u0026#39;/\u0026gt; \u0026lt;/dhcp\u0026gt; \u0026lt;/ip\u0026gt; \u0026lt;/network\u0026gt; EOF chmod u+s /usr/lib/qemu/qemu-bridge-helper mkdir /etc/qemu/ cat \u0026lt;\u0026lt; EOF | tee /etc/qemu/bridge.conf allow br0 EFO virsh net-define bridge.xml virsh net-start br0 virsh net-autostart br0 virt-install \\ --name=\u0026#34;u1\u0026#34; \\ --import \\ --disk \u0026#34;path=linux.img,format=qcow2\u0026#34; \\ --disk \u0026#34;path=cloud-init.img,device=cdrom\u0026#34; \\ --ram=\u0026#34;2048\u0026#34; \\ --vcpus=\u0026#34;2\u0026#34; \\ --osinfo opensuse15.0 \\ --wait 0 \\ --noautoconsole \\ --autostart \\ --network bridge=br0 Connect to console\nvirsh console u1 Connect to the display\nvirt-viewer -u u1 Stop \u0026amp; Stop the VM\nvirsh start u1 virsh shutdown u1 Remove the VM\nvirsh destroy u1 virsh undefine u1 QEMU # To create a VM with QEMU, combine everything together:\nqemu-system-x86_64 \\ -name u1 \\ # Enable KVM -enable-kvm \\ # Add disk image -hda linux.img \\ # Add cloud init -cdrom cloud-init.img \\ # Config Memory and CPU -m 2048 \\ -smp 2 \\ # First network - user network with port forward SSH to TCP 1025 -nic user,hostfwd=tcp:127.0.0.1:1025-:22 \\ # Second network - bridge network on br0 device -nic bridge,br=br0,model=virtio-net-pci \\ # Spice -vga qxl \\ -spice port=3001,password=123 \\ -soundhw hda \\ -device virtio-serial \\ -chardev spicevmc,id=vdagent,debug=0,name=vdagent \\ -device virtserialport,chardev=vdagent,name=com.redhat.spice.0 \\ # Serial -serial stdio ","date":"4 December 2023","externalUrl":null,"permalink":"/posts/kvm-qemu-libvirt/","section":"Posts","summary":"Purpose of this note: The existing online tutorials provide limited information about KVM.\nMany tutorials reference the use of virt-manager, which is akin to a GUI tool for creating virtual machines.","title":"KVM - QEMU, libvirt","type":"posts"},{"content":"","date":"4 December 2023","externalUrl":null,"permalink":"/tags/libvirt/","section":"Tags","summary":"","title":"libvirt","type":"tags"},{"content":"","date":"4 December 2023","externalUrl":null,"permalink":"/tags/qemu/","section":"Tags","summary":"","title":"qemu","type":"tags"},{"content":" Drawbacks and Challenges # High Resource Requirements in Harvester # When initiating Harvester, the system immediately demands significant resources, utilizing at least 10GB of RAM and 4 CPUs. This level of resource usage is notably high, especially when compared to other hypervisors (eg: xen, vmware).\nThe official hardware requirements are even higher:\nCPU: min: 8-core; prefer: 16 cores or more Memory: min 32 GB; prefer: 64 GB or more Disk Capacity: min 140 GB; prefer: 500 GB or more This high demand for resources can largely be attributed to Harvester\u0026rsquo;s underlying architecture, which is based on a Kubernetes cluster.\nOperational Limitations in VM Management # Tasks like\nmodifying network settings recovering VM snapshots require stopping the VM first, which can be a significant limitation in certain scenarios.\nLarge File Sizes for Exported Templates # The size of exported templates in Harvester is directly proportional to the VM size. This can result in unexpectedly large file sizes for templates.\nFor example, when starting with an original cloud image of 1GB and creating a VM with a 10GB disk, the resulting template size was also 10GB, significantly larger than the original image size.\nSpace Issue in default paritition setup # One notable storage issue in Harvester is the often insufficient default size of the /usr/local partition, particularly when it comes to system upgrades.\nTo address this, a workaround has been provided through a script available on GitHub.\nThis script is designed to purge unused images, thereby freeing up valuable space in the /usr/local partition. This solution is detailed in Harvester Issue #2668.\nRancher is lagging behind the k8s Releases # Rancher serves as a management platform for various clusters and operates independently of Harvester.\nFor installation, Rancher offers options to use either Helm or Docker.\nWhile the Docker installation method is effective, there have been challenges with installations on the latest k8s versions.\nRancher does not support the most recent k8s versions. For instance, as detailed in the Rancher v2.7.9, the latest supported Kubernetes version is v1.26.9, whereas the Kubernetes has already advanced to version v1.28.\nCapabilities # In terms of technology, Harvester incorporates several advanced features:\nRKE2 Integration: Harvester employs RKE2, which is similar to k3s but includes additional CIS hardening features for improved security. Multus for Networking: The use of Multus in Harvester enhances its networking capabilities, allowing for the configuration of multiple networks within the system. KubeVirt for VM Management: Harvester integrates KubeVirt, enabling the creation of VMs using KVM within containers. Network Management # The default network in Harvester is the Management Network, also known as the pod network. Harvester supports the creation of custom networks, enabling users to set up cluster networks (binding to the NIC) and VM networks (configuring subnets and VLANs) for tailored network configurations. The platform includes features for assigning custom MAC addresses and managing multiple networks. |--machine (ens19) |--harvester (ens19) |----vm (management network; vm network `private` that bond to ens19) My network setup involves configuring a virtual machine (VM) with two network interface cards (NICs). The first NIC is connected to the Management Network, and the second NIC is linked to a secondary network card on Harvester (eth19). This configuration allows an external machine to connect to the VM on the same network.\nI have also tested a setup with secondary network only. In this arrangement, there is connectivity between the VM and a machine on the same network, along with internet access.\nThis represents the cluster network setup. Multiple NICs can be chosen. This illustrates the VM network setup with an untagged network. show the VM network setup with type VLAN, but the VLAN ID is set to 1 For 2Nic Test,\nnetwork-config\nnetwork: version: 1 config: - type: physical name: eth0 subnets: - type: dhcp - type: physical name: eth1 subnets: - type: static address: 203.0.113.3/24 For 1Nic Test\nnetwork-config\nnetwork: version: 1 config: - type: physical name: eth0 subnets: - type: static address: 203.0.113.3/24 Virtual Machine Management # Creating VMs using cloud image and cloud-init. Accessing VMs is possible both through the Harvester host and from external. Adding extra disk to a VM, can be done while the VM is running. Adding or changing network connections, the VM needs to be stopped before making these changes. Taking snapshots of VMs. Managing multiple network interfaces, including setting up second NICs and assigning mac address. Supports backup options to external storage locations like S3 and NFS. The restore functionality provides flexibility to create new VMs or replace existing ones from backups and snapshots. ","date":"4 December 2023","externalUrl":null,"permalink":"/posts/review-of-harvester/","section":"Posts","summary":"Drawbacks and Challenges # High Resource Requirements in Harvester # When initiating Harvester, the system immediately demands significant resources, utilizing at least 10GB of RAM and 4 CPUs. This level of resource usage is notably high, especially when compared to other hypervisors (eg: xen, vmware).","title":"Review of Harvester","type":"posts"},{"content":"","date":"11 September 2023","externalUrl":null,"permalink":"/tags/aws/","section":"Tags","summary":"","title":"aws","type":"tags"},{"content":"","date":"11 September 2023","externalUrl":null,"permalink":"/tags/efs/","section":"Tags","summary":"","title":"efs","type":"tags"},{"content":"I recently had trouble connecting an ECS container to an EFS access point. Here\u0026rsquo;s an breakdown of what happened and how I fixed it:\nThe Problem:\nWhen I tried to link an ECS container with an EFS access point, I got this error: \u0026quot;'b'mount.nfs4: access denied by server while mounting 127.0.0.1:/'\u0026quot;\nFirst Thoughts:\nI thought the issues could be:\nEFS\u0026rsquo;s security group blocking our connection. Not having the right permissions in the task\u0026rsquo;s IAM role. What I Tried:\nTo figure out what was wrong, I did two things:\nI tried connecting the EFS without the access point directly from EC2. I changed the volume mount settings to leave out the access point. Both worked perfectly, so my first thoughts weren\u0026rsquo;t the problem. After some digging, I found a helpful article. It explained that if you don\u0026rsquo;t set up the right ownership and permission settings for the root directory of an access point, EFS won\u0026rsquo;t create it. This was causing the \u0026ldquo;access denied\u0026rdquo; error.\nThe Solution:\nWhen setting up the access point, I made sure to fill in the owner id, owner group id, and default file permissions in the Root directory creation permission section. This fixed the error.\n","date":"11 September 2023","externalUrl":null,"permalink":"/posts/aws-efs-access-point-error/","section":"Posts","summary":"I recently had trouble connecting an ECS container to an EFS access point. Here\u0026rsquo;s an breakdown of what happened and how I fixed it:\nThe Problem:\nWhen I tried to link an ECS container with an EFS access point, I got this error: \u0026quot;'b'mount.","title":"Fixing Access Issues Between ECS Containers and EFS Access Points","type":"posts"},{"content":" Service-wise Summary # Athena # Compression\nAthena uses different compression methods based on the need: gzip: Used when focusing on reducing the size of data. lzo or snappy: Used when focusing on speed. Workgroup\nAccess to the workgroup is controlled by IAM policies. Workgroups can be temporarily enabled or disabled. You can define policies for things like query result location or encryption. By turning on \u0026lsquo;Override client-side settings\u0026rsquo;, you can enforce the workgroup\u0026rsquo;s settings on users. Data usage control can be applied in two ways: Per query: The query stops when the amount of scanned data exceeds a limit. Workgroup-wide: Triggers an alert when the amount of scanned data exceeds a limit, but doesn\u0026rsquo;t enforce the limit. Access rights are based on a user\u0026rsquo;s permissions, not on service or EC2 roles. Query Timeout\nDML/DDL queries have a timeout, which can be increased in the service quota. Resolving HIVE_METASTORE_ERROR\nError: \u0026ldquo;Expected \u0026lsquo;xxxx\u0026rsquo; but \u0026lsquo;/\u0026rsquo; found.\u0026rdquo; Reason: This occurs when there\u0026rsquo;s an unsupported character in the partition or column. Fix: Remove the unsupported character. Error: \u0026ldquo;Storage descriptor is not populated.\u0026rdquo; Reason: This occurs when there\u0026rsquo;s a broken partition. Fix: Use \u0026lsquo;msck repair table\u0026rsquo; to repair the table, or copy the data into a separate folder and run the crawler on that folder. Error: \u0026ldquo;Payload size exceed.\u0026rdquo; Reason: This occurs when running a Lambda function in a query, as Lambda has a size limit on returned data. Fix: Upload the result to S3, then attach the presigned URL in the response. EMR # Block Public Access\nEMR has a setting similar to S3\u0026rsquo;s \u0026lsquo;Block public access\u0026rsquo;. This setting functions before EMR creation and removes public access in the security group, except for allowed port ranges. Consistent View (Legacy)\nPreviously, AWS used DynamoDB to track read-after-write consistency in S3 objects (using EMRFS). EBS Encryption and Custom Software\nEBS encryption and custom software installation in EMR can be accomplished using a custom Amazon Linux AMI. Custom software can also be installed using bootstrap actions, similar to the way user-data is used in EC2. Integration with DynamoDB\nEMR has integration with DynamoDB in Hive. Handling HTTP 503 Slow Down AWSZonS3Exception\nThis error is caused by an excessive number of reads to S3. It can be fixed by increasing the EMRFS retry limit or adding more prefixes, as the rate limit applies per prefix. HBase\nHBase is a column-based NoSQL database, ideal for OLTP operations. Features include: Data storage in S3. Support for read-replicas (although two masters cannot point to the same HBase path, read-replicas can be recreated). The ability to take snapshots to S3. High Availability\nHigh availability is currently supported only in a single AZ (support for multiple AZs is coming soon). For high availability, it\u0026rsquo;s recommended to create multiple masters and use EMRFS. Presto\nPresto can work with data from multiple sources. S3-Dist-CP\nS3-Dist-CP is a tool that helps in distributing data copying between EMRFS and HDFS. Sqoop\nSqoop is used to export and import data between relational databases (like RDS) and HDFS. EMR Managed Scaling\nEMR Managed Scaling reacts faster than traditional auto scaling. It can work with either instance groups or fleets. It is automatically configured, so no manual setup is needed. Glue # Problem with maximum concurrent runs.\nOnce the concurrent limit is reached and a job has already been completed, attempting to rerun the same job will result in a ConcurrentRunsExceededException being triggered. Possible hidden state indicating the job is still running. AWS Glue and Apache Hudi Integration.\nCrawler\nCan use S3 locations or data catalog tables as sources. When S3 location is chosen, it automatically creates or updates the schema and table. When a data catalog table is chosen, it does not create a new table. Choices are available for handling schema updates or deletions. ETL Job\nIn ETL job, you have the option to update the schema. Debugging OOM Exceptions:\nDriver\nProblem: The driver creates a task for each file in an S3 prefix. When there are too many files in one location, it can lead to an OOM error due to the large number of tasks. Solution: Use \u0026lsquo;groupFiles: inPartition\u0026rsquo; to load multiple files in one task, which reduces the number of tasks and prevents OOM. Executor\nProblem: When selecting data from MySQL, the default action is to load all rows. This can cause an OOM error if there are too many rows. Solution: Use a dynamic frame to limit the number of rows selected at once, which prevents OOM. Excluding S3 Storage Class in ETL Job\nThe S3 storage class can be excluded by setting it in the data catalog property or in the dynamic frame. FindMatches\nUses machine learning to find records from different tables that refer to the same object (such as a customer or product). PostAction\nCan be used to deduplicate records before ingesting data into Redshift. Kinesis Stream # Resharding\nThis involves splitting and merging shards. For instance, during a split, shard1 can be divided into shard10 and shard11. Here, shard1 becomes the parent, while shard10 and shard11 become the children. After resharding, the parent shard (shard1 in this case) becomes read-only but remains available. For ordered data reading, the parent should be consumed first, followed by the children. This process needs to be managed in the SDK. Enhanced Fanout\nKinesis limits throughput by shard (1MB for write, 2MB for read). Enhanced fanout can increase read throughput. This process uses a Lambda function to read from the stream and Lambda replicas to distribute the data to more destinations. Kinesis Stream as a Database\nThe data is stored until it expires. The expiry period can be set from 1 day to 1 year. The data is not removed when read, meaning it can be consumed multiple times. Stateless Nature\nConsumers need to remember their read position to prevent re-reading data. This is automatically handled in the KCL, which creates a DDB table to manage consumer memberships and track their states. If using the SDK, developers must implement the checkpoint by tracking the number of records read or by time to achieve the same effect. Using KCL\nA DDB table is created to maintain the consumer state. If the iterator expires unexpectedly, it could be due to the DDB not having enough write capacity. Handling Duplicate Records\nSequence numbers can be used to identify duplicates. Retries are common in streams and can occur due to network issues or unexpected consumer exits, which can lead to the checkpoint not being saved. Kinesis Firehose # Data can be delivered to storage (S3) or databases (DDB, Redshift, RDS, OpenSearch). The minimum batch time limit is 1 minute, which may not be ideal for real-time processing. A Lambda function can be used to transform data before it\u0026rsquo;s delivered or to send alerts. Kinesis Analytics # A CSV file in S3 can be attached as reference data to join with the stream. The data can be output to Lambda. RANDOM_CUT_FOREST can be used for anomaly detection. Data can be pre-processed with a Lambda function. OpenSearch # Issues leading to High JVM Pressure\nHaving too many divisions of an index, also known as shards. Uneven distribution of shards. Running summaries on text fields or wildcards. Combining large data tables. An overload of requests. UltraWarm Feature\nA cost-effective, read-only feature. The index state management tool can automate moving data to UltraWarm. To fine-tune performance, adjust the \u0026lsquo;max_num_segments\u0026rsquo; value in \u0026lsquo;force_merge\u0026rsquo;. A larger \u0026lsquo;max_num_segments\u0026rsquo; makes the process faster, but can cause delays after indexing. Quicksight # Setting Up Row-Level Security on a Dataset\nAttach a CSV file or a user-based query rule to the dataset. The file or query includes \u0026ldquo;user\u0026rdquo; or \u0026ldquo;group\u0026rdquo; and filtering conditions (similar to SQL\u0026rsquo;s WHERE clause: WHERE variable=value). The default setting is to restrict access; if there\u0026rsquo;s no entry, there\u0026rsquo;s no visibility. Tag-based rules can be used in API calls to create embedded URLs for guest users. Implementing Column Level Security\nChoose the columns that specific users or groups can view. To retrieve data from Athena, establish a connection to Athena and register the S3 bucket in Quicksight. Features of the Paid Version\nIsolated namespaces that readers can access. Authentication via Active Directory. Redshift # DBlink\nAn add-on to PostgreSQL that enables data queries directly from Redshift. It operates in read-only mode. Audit Log\nCan be activated as needed. Column Level Access Control\nAccess can be provided through the GRANT SELECT(COL1, COL2,\u0026hellip;) command. Query Monitoring Rule A rule includes:\nA rule name. One or more conditions. An action to be taken (stop, log, hop, change priority). Short Query Acceleration\nIn the workload management settings, this feature can prioritize faster queries. Spectrum\nThis feature is not compatible with Glacier data. S3 # S3 Select\nFor Glacier, it functions only with instant retrieval. Intelligent Tiering and Reduced Redundancy Storage are not compatible. Data output can be formatted as CSV or JSON. Data input can be formatted as CSV or JSON. Compression is supported, but only gzip or bzip formats are allowed. For columnar formats like Parquet, only gzip or snappy formats are allowed. S3 Object Lock can help prevent Amazon S3 objects from being deleted or overwritten for a fixed amount of time or indefinitely. Lake Formation # To maintain compatibility, \u0026ldquo;super\u0026rdquo; permission is given to IAMAllowedPrincipals. This means that if the IAM policy permits the data catalog, it will bypass the Lake Formation\u0026rsquo;s permission control. Here are the steps to use Lake Formation:\nTake away the IAMAllowedPrincipals\u0026rsquo; rights in Data Lake permissions, under Administrative roles and tasks for Database Creators. In the data catalog settings, deselect \u0026lsquo;Use only IAM access control for new databases\u0026rsquo; and \u0026lsquo;Use only IAM access control for new tables in new databases\u0026rsquo;. Register the S3 in the Data Lake location. Assign permissions in Data Lake permissions. If \u0026lsquo;select\u0026rsquo; permission is given, it will also allow the S3 permission. Topic-wise Summary # Encryption # Quicksight: Employs AWS managed keys or KMS-CMK for data encryption.\nRedshift: Utilizes CloudHSM, on-premise HSM, or KMS for data encryption.\nEMR:\nFor EMRFS: Server side: Uses either AWS managed keys or customer-managed KMS keys. Client side: Employs customer-managed KMS keys or custom KMS keys. For instance storage/EBS: Prefers EBS when possible. If not, it falls back to LUKS encryption. Access Control # Concepts\nRow-Level Security:\nConceals rows by filtering data based on a certain value (e.g., using the SQL command: SELECT * FROM table WHERE foo=bar). Column-Level Security:\nHides specific columns (these columns will not be included in a SELECT statement). Athena:\nAccess is managed through user/group permissions. EMR:\nAccess is managed via an EC2 role. For EMRFS, an IAM role is defined in the security configuration (this role is assumed by the EC2 role). Redshift:\nAccess is managed via a service role. Usage permissions are granted to IAM roles (similar to MySQL). Quicksight:\nImplements both row-level and column-level security on datasets. Authentication methods: IAM/SAML2/MFA are available for free. Active Directory is available with the Enterprise edition. Lake Formation:\nImplements both row-level and column-level security on the data catalog. Best Practices:\nRedshift:\nTo use data in Lake Formation, grant SELECT permission to the Redshift role. To allow a user to use a table in Redshift, use GRANT USAGE in Redshift. EMRFS:\nS3 access should be granted through an IAM role, not an EC2 role. The EC2 role assumes the IAM role to retrieve data from S3. DMS # Metrics for monitoring database migration. CDCLatencySource and CDCLatencyTarget indicate latency between the source and target in replication. ","date":"31 July 2023","externalUrl":null,"permalink":"/posts/exam-aws-data/","section":"Posts","summary":"Service-wise Summary # Athena # Compression\nAthena uses different compression methods based on the need: gzip: Used when focusing on reducing the size of data. lzo or snappy: Used when focusing on speed.","title":"AWS Certified Data Analytics - Specialty","type":"posts"},{"content":"","date":"31 July 2023","externalUrl":null,"permalink":"/tags/data/","section":"Tags","summary":"","title":"data","type":"tags"},{"content":"Scan and Query:\nAmazon DynamoDB has two operations for retrieving data: Scan and Query. Scan retrieves all items in a table, making it inefficient for larger tables. Query is designed to retrieve specific items based on their partition key values, allowing filtering of results. Indexing:\nThe primary key consists of the partition key and the sort key (optional), which uniquely identify each item in a table. While the primary key is essential, it may not always be sufficient for accessing data in the way needed. Global Secondary Index (GSI) is a separate index with its partition key and sort key that can be used for querying on non-primary key attributes. GSI can significantly reduce the number of items that need to be scanned and improve query speed. Primary key has to be unique, but GSI can be duplicated. Select and Filter:\nSelect and Filter expressions can also be used to specify attributes and conditions for retrieving data. Query is generally recommended for specific and efficient data retrieval, especially when used with a GSI. The choice of which operation to use depends on the specific use case and how data needs to be accessed and queried. ","date":"8 May 2023","externalUrl":null,"permalink":"/posts/brief-overview-of-scan-query-select-and-filter-of-dyanmodb/","section":"Posts","summary":"Scan and Query:\nAmazon DynamoDB has two operations for retrieving data: Scan and Query. Scan retrieves all items in a table, making it inefficient for larger tables. Query is designed to retrieve specific items based on their partition key values, allowing filtering of results.","title":"Brief Overview of Scan, Query, Select, and Filter of DyanmoDB","type":"posts"},{"content":"","date":"8 May 2023","externalUrl":null,"permalink":"/tags/cloud/","section":"Tags","summary":"","title":"cloud","type":"tags"},{"content":"","date":"8 May 2023","externalUrl":null,"permalink":"/tags/db/","section":"Tags","summary":"","title":"db","type":"tags"},{"content":"","date":"8 May 2023","externalUrl":null,"permalink":"/tags/ddb/","section":"Tags","summary":"","title":"ddb","type":"tags"},{"content":"","date":"8 May 2023","externalUrl":null,"permalink":"/tags/grpc/","section":"Tags","summary":"","title":"grpc","type":"tags"},{"content":" Workflow of RPC server # Receive a RPC request Parse request into message, context Service interceptor RPCMethodHandler Give RPC response [metadata (optional), message protobuf object / raise exception] Metadata has to be sent explicitly.\nSteps to work on RPC client-server # Server # Create RPC server object Create servicer object Add middleware to server Add servicer to RPC Start the server Client # Create channel object Add middleware to channel Send RPC request with stub object Ping-pong between client-server: # In bidirectional streaming RPC, the call is initiated by the client invoking the method, and the server receives the client metadata, method name, and deadline The server can choose to send back its initial metadata or wait for the client to start streaming messages Client and server-side stream processing is application-specific Since the two streams are independent, the client and server can read and write messages in any order For example, a server can wait until it has received all of a client’s messages before writing its messages, or the server and client can play “ping-pong” This is similar to a WebSocket, providing a live chat between client and server from the stub (channel) Example flow:\nClient yields request_stream1 Server receives request_stream1 Server yields result_stream1 Client yields stream2 Server receives stream2 Server yields result_stream2 The client keeps yielding stream, and the server iterates through the stream The server retrieves and stores each stream until iterator.next() == null The server responds with a stream message and ends the stream When sending two stream objects:\nIt is still considered a single request Multiple messages are sent through the channel (stub) Each stream message is sent to the same target server (behind ALB) The channel can last a long time unless the client or server closes it, passes the deadline, or reaches the idle_timeout When streaming, it follows a consumer and producer pattern In Client streaming RPC, the client acts as a producer and server as a consumer In Bidirectional streaming RPC, both client and server act as producers and consumers simultaneously Reference: https://www.oreilly.com/library/view/grpc-up-and/9781492058328/ch04.html\nMy Lab to Experience gRPC # For experiencing gRPC, I created a simple client and server to manage the records of a message \u0026ldquo;video.\u0026rdquo;\nInstall the basic packages needed: pip3 install grpcio grpcio-tools grpcio_reflection\nTo start the server:\npython3 server.py To compile the proto:\npython3 -m grpc_tools.protoc -I. --python_out=. --pyi_out=. --grpc_python_out=. video_service.proto To send requests to the server:\nexport SERVER=localhost:50051 Unary - stream:\ngrpcurl -proto video_service.proto -plaintext $SERVER video_service.VideoService/ListVideos Unary - (with param):\ngrpcurl -proto video_service.proto -plaintext -d \u0026#39;{\u0026#34;id\u0026#34;:\u0026#34;2\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;Sample Video\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;https://example.com/sample.mp4\u0026#34;, \u0026#34;duration\u0026#34;: 120}\u0026#39; $SERVER video_service.VideoService/AddVideo grpcurl -proto video_service.proto -plaintext -d \u0026#39;{\u0026#34;id\u0026#34;:\u0026#34;1\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;Sample Video\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;https://example.com/sample.mp4\u0026#34;, \u0026#34;duration\u0026#34;: 120}\u0026#39; $SERVER video_service.VideoService/AddVideo Stream:\ngrpcurl -proto video_service.proto -plaintext -d @ $SERVER video_service.VideoService/GetVideos \u0026lt;\u0026lt; EOF {\u0026#34;id\u0026#34;:\u0026#34;2\u0026#34;} {\u0026#34;id\u0026#34;:\u0026#34;1\u0026#34;} EOF grpcurl --plaintext $SERVER list Interactive way to send a stream:\ngrpcurl --proto video_service.proto -insecure -vv -d @ $SERVER video_service.VideoService/GetVideo {\u0026#34;id\u0026#34;:\u0026#34;2\u0026#34;} wait for the response for video 2 {\u0026#34;id\u0026#34;:\u0026#34;1\u0026#34;} wait for the response for video 1 Examples # There are some examples worth reading from the gRPC library repository.\nConcepts related # Health checking:\nSpecial servicers that require an extra package Can service health checks with its own thread pools, not affecting the thread pool of other servicers Interceptors:\nServer Interceptors: - Modify the request from the client before processing. - Interrupt each received RPC request. - Call `continuation` to proceed. - Return a handler to override the default handler. Client Interceptors: - Invoked before the RPC request is sent and after the RPC response is received. - Add a middleware to the channel, which can extend multiple per-RPC-type client interceptors. Implement a function per RPC type. - Inside the functions, modify the RPC request from `ClientCallDetails`, then call `continuation`. This will send the RPC call, wait for the response from the server, and then return the RPC response. The response can be modified if desired. Metadata:\nSimilar to headers in HTTP Multiplex:\nOne channel with multiple stubs One server with multiple services Route guide:\nBest example to understand unary-unary, unary-stream, stream-unary, and stream-stream rpc requests Options / Settings related # Keepalive Timeout Load balancing policies Retry Wait for ready - send the request when the service is not busy Compression - gzip ALB # Sticky sessions can be tricky:\nSticky sessions are used when there are many clients that keep requesting the service and want to reuse connections. Sticky sessions in gRPC are not ideal when working with load balancing:\nLoad balancing will not work effectively, as it causes traffic to be sent to the same instance. If there are limited clients but each sends a lot of requests, we want the requests to distribute evenly to the backends. However, sticky sessions cause the same client\u0026rsquo;s requests to be sent to the same instance, overloading the backend. For more information on gRPC load balancing, visit: https://majidfn.com/blog/grpc-load-balancing/\nHealth Check # gRPC is an HTTP request with a binary payload:\nIt uses POST requests and the URI is the method (e.g., video_service.VideoService/ListVideo). Status codes: 0 is normal; 12 is not implemented. ","date":"8 May 2023","externalUrl":null,"permalink":"/posts/understanding-grpc-key-concepts-and-examples/","section":"Posts","summary":"Workflow of RPC server # Receive a RPC request Parse request into message, context Service interceptor RPCMethodHandler Give RPC response [metadata (optional), message protobuf object / raise exception] Metadata has to be sent explicitly.","title":"Understanding gRPC: Key Concepts and Examples","type":"posts"},{"content":" Real Hardware Requirments # Suggestion from doc:\n4 physical CPU cores 9 GB of free memory 35 GB of storage space Recommended (to make it work for development):\nat least 15GB memory. 18GB is better. 60 GB storage Increase Memory: # crc stop crc config set memory \u0026lt;memory_in_mb\u0026gt; crc start Prune images in the cluster: # OpenShift Local uses Podman to run containers for the infrastructure of the cluster. Image cache accumulates and grows quickly. Need to SSH into the KVM and prune the images. Access the host:\nUse the SSH command and the key (~/.crc/machines/crc/id_ecdsa) found in the .crc folder: (See) ssh -i \u0026lt;key\u0026gt; core@\u0026lt;master-hostname\u0026gt; To find the master ip:\nCreate a pod with hostNetwork install iproute2 use ip addr to find the host IP. apiVersion: v1 kind: Pod metadata: name: test spec: hostNetwork: true containers: - name: alpine image: alpine tty: true Prune dangling images:\npodman image prune Enlarge disk size: (Thanks to this discussion) # Enlarge the KVM image: CRC_MACHINE_IMAGE=\u0026#34;$HOME/.crc/machine/crc/crc\u0026#34; crc stop qemu-img resize ${CRC_MACHINE_IMAGE} +24G Find the root partition (the largest one) using virt-filesystems: virt-filesystems --long -h --all -a $HOME/.crc/machines/crc/crc Name Type VFS Label MBR Size Parent /dev/sda1 filesystem unknown - - 1.0M - /dev/sda2 filesystem ext4 boot - 1.0G - /dev/sda3 filesystem xfs root - 39G - /dev/sda1 partition - - - 1.0M /dev/sda /dev/sda2 partition - - - 1.0G /dev/sda /dev/sda3 partition - - - 39G /dev/sda /dev/sda device - - - 40G - Enlarge the partition in KVM: cp ${CRC_MACHINE_IMAGE} ${CRC_MACHINE_IMAGE}.ORIGINAL virt-resize --expand /dev/sda3 ${CRC_MACHINE_IMAGE}.ORIGINAL ${CRC_MACHINE_IMAGE} ","date":"7 May 2023","externalUrl":null,"permalink":"/posts/admin-tips-for-managing-openshift-local-clusters-pruning-images-enlarging-disk-size-and-increasing-memory/","section":"Posts","summary":"Real Hardware Requirments # Suggestion from doc:\n4 physical CPU cores 9 GB of free memory 35 GB of storage space Recommended (to make it work for development):\nat least 15GB memory.","title":"Admin Tips for Managing OpenShift Local Cluster: Pruning Images, Enlarging Disk Size, and Increasing Memory","type":"posts"},{"content":"","date":"7 May 2023","externalUrl":null,"permalink":"/tags/case-study/","section":"Tags","summary":"","title":"case-study","type":"tags"},{"content":"A recent article on Prime Video Tech describes their approach to scaling up their audio-video monitoring service and reducing costs by 90%.\nThis blog post aims to critically analyze the article and address some issues and misconceptions.\nThe Service and the Problem\nThe monitoring service converts video streams into video/audio formats and stores them on Amazon S3. Multiple detectors identify problems in the video, and the results are saved back to S3. Escalating costs were due to data transfer between S3, orchestration with AWS Step Functions, and not being able to use EC2 Savings Plans. Detectors had to download the same data multiple times, causing high costs and inefficient resource use. Scaling bottleneck: orchestration management using AWS Step Functions led to reaching account limits and charges per state transition. My View:\nThe post\u0026rsquo;s motivation is valid as it shares a story about addressing cost and scaling issues in a Prime Video service.\nHowever, bloggers and YouTubers used the article to create headlines pitting microservices against monolithic architectures.\nThe real issue is not about microservices vs. monoliths, but the pricing model on AWS and understanding the context of the solution.\nThe post is missing information: size of video streams, detector runtime, and service profile (CPU, memory, concurrent users, usage patterns, etc.).\nThese factors are essential to understanding scalability and cost implications.\nThe solution involves vertical scaling, which can be more expensive and challenging to adjust based on usage patterns, possibly leading to underutilization during off-peak times.\nMy Advice/Suggestion:\nConsider trade-offs, limitations, and applicability of the solution. Understand that AWS is not synonymous with microservices; the real issue is its pricing model. Know your service\u0026rsquo;s requirements and usage patterns before making architectural decisions. Take a balanced approach and make informed decisions based on the specific needs of your application, rather than being swayed by sensational headlines. ","date":"7 May 2023","externalUrl":null,"permalink":"/posts/critical-review-of-hit-discussions-video-monitoring-service-pitfalls-of-blaming-microservices/","section":"Posts","summary":"A recent article on Prime Video Tech describes their approach to scaling up their audio-video monitoring service and reducing costs by 90%.\nThis blog post aims to critically analyze the article and address some issues and misconceptions.","title":"Critical Review of Hit Discussion's Video Monitoring Service: Pitfalls of Blaming Microservices","type":"posts"},{"content":"","date":"7 May 2023","externalUrl":null,"permalink":"/tags/k8s/","section":"Tags","summary":"","title":"k8s","type":"tags"},{"content":"","date":"7 May 2023","externalUrl":null,"permalink":"/tags/openshift-local/","section":"Tags","summary":"","title":"openshift-local","type":"tags"},{"content":"This post explores various networking concepts and configurations for Linux systems, including:\nbridging VLAN filtering IPvlan Vxlan These concepts are useful for creating and managing virtual networks.\nI\u0026rsquo;m sharing this information based on my own interest and learning experiences. Please note that this may not cover all aspects of networking, and it\u0026rsquo;s recommended to consult additional resources or an expert for more in-depth information.\nBridge # A bridge connects two or more network segments, allowing them to act as a single network. In Linux, you can create network namespaces and connect them using a bridge. This configuration allows the namespaces to communicate with each other, but they won\u0026rsquo;t have internet access unless you set up NAT on the host.\nBy default, Docker is using the Bridge network.\nLinux Networking: Bridge, iptables, and Docker\nA simple lab on it:\nCreate two network namespaces, each with its own individual IP address. Bridge the namespaces together using the same bridge. At this point, the two network namespaces can ping each other, but they won\u0026rsquo;t have internet access. To provide internet access, configure NAT on the host machine, which will allow the network namespaces to access the internet. NS1=\u0026#34;ns1\u0026#34; NS2=\u0026#34;ns2\u0026#34; NS1=\u0026#34;ns1\u0026#34; VETH1=\u0026#34;veth1\u0026#34; VPEER1=\u0026#34;vpeer1\u0026#34; NS2=\u0026#34;ns2\u0026#34; VETH2=\u0026#34;veth2\u0026#34; VPEER2=\u0026#34;vpeer2\u0026#34; BR_ADDR=\u0026#34;10.10.0.1\u0026#34; VPEER_ADDR1=\u0026#34;10.10.0.10\u0026#34; VPEER_ADDR2=\u0026#34;10.10.0.20\u0026#34; BR_DEV=\u0026#34;br0\u0026#34; ip netns add $NS1 ip netns add $NS2 ip link add ${VETH1} type veth peer name ${VPEER1} ip link add ${VETH2} type veth peer name ${VPEER2} ip link add ${BR_DEV} type bridge ip link set ${VETH1} master ${BR_DEV} ip link set ${VETH2} master ${BR_DEV} ip addr add ${BR_ADDR}/16 dev ${BR_DEV} ip link set ${VPEER1} netns ${NS1} ip link set ${VPEER2} netns ${NS2} ip link set ${BR_DEV} up ip link set ${VETH1} up ip link set ${VETH2} up ip netns exec ${NS1} ip link set lo up ip netns exec ${NS2} ip link set lo up ip netns exec ${NS1} ip link set ${VPEER1} up ip netns exec ${NS2} ip link set ${VPEER2} up ip netns exec ${NS1} ip addr add ${VPEER_ADDR1}/16 dev ${VPEER1} ip netns exec ${NS2} ip addr add ${VPEER_ADDR2}/16 dev ${VPEER2} ip netns exec ${NS1} ip route add default via ${BR_ADDR} ip netns exec ${NS2} ip route add default via ${BR_ADDR} bash -c \u0026#39;echo 1 \u0026gt; /proc/sys/net/ipv4/ip_forward\u0026#39; iptables -t nat -A POSTROUTING -s ${BR_ADDR}/16 ! -o ${BR_DEV} -j MASQUERADE VLAN filtering # VLAN filtering allows a single bridge to manage traffic for multiple VLANs, similar to a switch. This enables you to create a single bridge and separate network namespaces into different VLANs, preventing them from communicating with each other.\nWithout VLAN filtering: VLAN Filter Support on Bridge (Without VLAN Filtering) With VLAN filtering: Introduction to Linux Bridging Commands and Features (VLAN Filter) IPvlan # IPvlan allows a single MAC address to have multiple IPs. This can be useful in Docker, where you might want a container to have an IP address in the same subnet as the host network.\nIPvlan Linux Networking\nip netns add ns0 ip link add link enp0s8 ipvl0 type ipvlan mode l2 ip link set dev ipvl0 netns ns0 ip netns exec ns0 bash ip link set dev ipvl0 up ip link set dev lo up ip addr add 192.168.0.16/24 dev ipvl0 ip addr add 127.0.0.1 dev lo ip route add default via 192.168.0.1 dev ipvl0 Vxlan # Vxlan is an overlay network that can connect multiple Linux machines and build a network on top of it. This is used in systems like Kubernetes, where the node network is an underlay, and the pod network is an overlay. From the pod\u0026rsquo;s perspective, the node does not exist, and the overlay network appears as a single entity.\nVirtual Networking Labs: Overlay Networks\n","date":"6 May 2023","externalUrl":null,"permalink":"/posts/linux-virtual-network/","section":"Posts","summary":"This post explores various networking concepts and configurations for Linux systems, including:\nbridging VLAN filtering IPvlan Vxlan These concepts are useful for creating and managing virtual networks.\nI\u0026rsquo;m sharing this information based on my own interest and learning experiences.","title":"[Note] Linux Virual Networking","type":"posts"},{"content":"","date":"6 May 2023","externalUrl":null,"permalink":"/tags/bridge/","section":"Tags","summary":"","title":"bridge","type":"tags"},{"content":"","date":"6 May 2023","externalUrl":null,"permalink":"/tags/docker/","section":"Tags","summary":"","title":"docker","type":"tags"},{"content":"","date":"6 May 2023","externalUrl":null,"permalink":"/tags/ipvlan/","section":"Tags","summary":"","title":"ipvlan","type":"tags"},{"content":"","date":"6 May 2023","externalUrl":null,"permalink":"/tags/vlan/","section":"Tags","summary":"","title":"vlan","type":"tags"},{"content":"","date":"6 May 2023","externalUrl":null,"permalink":"/tags/vxlan/","section":"Tags","summary":"","title":"vxlan","type":"tags"},{"content":"","date":"5 May 2023","externalUrl":null,"permalink":"/tags/acl/","section":"Tags","summary":"","title":"acl","type":"tags"},{"content":"","date":"5 May 2023","externalUrl":null,"permalink":"/tags/alb/","section":"Tags","summary":"","title":"alb","type":"tags"},{"content":"Pacemaker is a powerful cluster resource manager designed to provide high availability (HA) and auto-failover capabilities for services and applications. In this blog post, we will cover some essential aspects of Pacemaker, from its basic functionality to different modes and installation.\nKey Points to Know About Pacemaker:\nRole in a Cluster: Pacemaker works like a \u0026lsquo;systemctl\u0026rsquo; for clusters, ensuring that services remain available and automatically failing over to another node if a primary node goes down.\nDefault Behavior: By default, Pacemaker maintains a single running service in the cluster and adds the service to other nodes when the primary node is down.\nOperating Modes: Pacemaker offers various modes, including clone mode and master-slave mode, to suit different cluster configurations and requirements.\nAuto-Failover: Auto-failover can be set up using a virtual IP (VIP) and configuring constraints for the \u0026lsquo;IPaddr2\u0026rsquo; resource to follow the master node.\nFencing: Pacemaker enables fencing by default to isolate and protect resources. However, it can be disabled by running the command pcs property set stonith-enabled=false.\nTwo-Node Clusters: For two-node clusters, it is necessary to disable quorum with the command pcs property set no-quorum-policy=ignore.\nPacemaker Web GUI: Pacemaker\u0026rsquo;s Web GUI is a fully functional command line interface that allows you to manage all aspects of the cluster.\nGalera and Master-Slave Mode: Galera is a special case of master-slave mode, where it operates as a multi-master configuration. To set up Galera, you need to:\nSet master-max = \u0026lt;no_of_node\u0026gt; Manually boot the cluster to generate grastate.dat Installing Pacemaker: To install Pacemaker, refer to the installation guide provided by ClusterLabs here. Pacemaker is a robust and flexible cluster resource manager that provides essential high availability and auto-failover capabilities for various services and applications. Understanding its core features and configuration options can help ensure that your cluster remains operational and resilient to failures.\n","date":"5 May 2023","externalUrl":null,"permalink":"/posts/understanding-pacemaker-for-high-availability/","section":"Posts","summary":"Pacemaker is a powerful cluster resource manager designed to provide high availability (HA) and auto-failover capabilities for services and applications. In this blog post, we will cover some essential aspects of Pacemaker, from its basic functionality to different modes and installation.","title":"An Introduction to Pacemaker: Ensuring High Availability and Auto-Failover for Your Cluster","type":"posts"},{"content":"","date":"5 May 2023","externalUrl":null,"permalink":"/tags/athena/","section":"Tags","summary":"","title":"athena","type":"tags"},{"content":"","date":"5 May 2023","externalUrl":null,"permalink":"/tags/autofs/","section":"Tags","summary":"","title":"autofs","type":"tags"},{"content":"","date":"5 May 2023","externalUrl":null,"permalink":"/tags/bind9/","section":"Tags","summary":"","title":"bind9","type":"tags"},{"content":"","date":"5 May 2023","externalUrl":null,"permalink":"/tags/cloudtrail/","section":"Tags","summary":"","title":"cloudtrail","type":"tags"},{"content":"","date":"5 May 2023","externalUrl":null,"permalink":"/tags/cloudwatch/","section":"Tags","summary":"","title":"cloudwatch","type":"tags"},{"content":"","date":"5 May 2023","externalUrl":null,"permalink":"/tags/dns/","section":"Tags","summary":"","title":"dns","type":"tags"},{"content":"Enable ALB access logging for debugging:\nFollow this guide: https://docs.aws.amazon.com/elasticloadbalancing/latest/application/enable-access-logging.html Access logs storage:\nLogs are saved in S3 Difficult to view directly Log format:\nNot written to a single file New file with timestamp after a set interval Solution: Set up Athena to query logs\nFollow this guide: https://docs.aws.amazon.com/athena/latest/ug/application-load-balancer-logs.html ","date":"5 May 2023","externalUrl":null,"permalink":"/posts/enabling-alb-access-logs-and-analyzing-with-athena/","section":"Posts","summary":"Enable ALB access logging for debugging:\nFollow this guide: https://docs.aws.amazon.com/elasticloadbalancing/latest/application/enable-access-logging.html Access logs storage:\nLogs are saved in S3 Difficult to view directly Log format:\nNot written to a single file New file with timestamp after a set interval Solution: Set up Athena to query logs","title":"Enabling ALB Access Logs and Analyzing with Athena","type":"posts"},{"content":"","date":"5 May 2023","externalUrl":null,"permalink":"/tags/ha/","section":"Tags","summary":"","title":"ha","type":"tags"},{"content":"","date":"5 May 2023","externalUrl":null,"permalink":"/tags/iptables/","section":"Tags","summary":"","title":"iptables","type":"tags"},{"content":"","date":"5 May 2023","externalUrl":null,"permalink":"/tags/lvm/","section":"Tags","summary":"","title":"lvm","type":"tags"},{"content":"Working with LVM (Logical Volume Management) can simplify the process of managing disk space. This post will guide you through enlarging a disk in LVM, migrating a non-LVM file system to LVM, and migrating the root file system to LVM.\nEnlarging a Disk in LVM # Refer to this article for more information: Manage and Create LVM Partition Using vgcreate, lvcreate, and lvextend.\nTo enlarge an existing LVM:\nCheck if the new disk is detected using lsblk. If not, scan it with: echo \u0026#34;- - -\u0026#34; \u0026gt; /sys/class/scsi_host/host\u0026lt;N=1,2,3,4...\u0026gt;/scan Create a physical volume, extend the volume group, and extend the logical volume: pvcreate \u0026lt;new_disk\u0026gt; vgextend \u0026lt;vg\u0026gt; \u0026lt;new_disk\u0026gt; lvextend -l +100%FREE -r \u0026lt;lvm\u0026gt; Resize the file system:\nFor ext4, use resize2fs. For xfs, use xfsgrow. You should able to see the resize disk on\ndf -h Migrating Non-LVM File System to LVM # For example, if you want to move /var to LVM:\nCreate the LVM. Mount it to a temporary location (e.g., /tmp/lvm). Copy data from /var to the LVM: rsync -avz /var/ /tmp/lvm/ Unmount it. Update /etc/fstab with the new LVM. Use UUID to reference the file system. Find the UUID with blkid. Refer to fstab documentation for more details.\nMigrating Root File System to LVM # Refer to Converting an Existing Root Filesystem to LVM Partition.\nThe steps are similar to migrating a non-LVM file system:\nRepeat the steps of migrating Non-LVM File System Perform additional steps: chroot to update initrd. Edit grub.conf with the new root. Restart the system. Following these steps will help you manage disk space using LVM, whether you need to enlarge a disk, migrate a non-LVM file system to LVM, or migrate the root file system to LVM.\n","date":"5 May 2023","externalUrl":null,"permalink":"/posts/managing-disk-space-with-lvm-a-comprehensive-guide/","section":"Posts","summary":"Working with LVM (Logical Volume Management) can simplify the process of managing disk space. This post will guide you through enlarging a disk in LVM, migrating a non-LVM file system to LVM, and migrating the root file system to LVM.","title":"Managing Disk Space with LVM: A Comprehensive Guide","type":"posts"},{"content":"Reference: https://upcloud.com/resources/tutorials/configure-iptables-centos\nProblem: Non-persistent iptables rule after system restarts.\nThis is how to add a rule to iptables:\nsudo iptables -A INPUT -p tcp --dport ssh -j ACCEPT Solution:\nThis command applies the rule immediately, but it won\u0026rsquo;t persist after a system restart.\nTo make iptables rules persistent, follow these steps:\nInstall iptables-services: sudo yum install iptables-services Start and enable iptables: sudo systemctl start iptables sudo systemctl enable iptables Save the current rules to a file: sudo iptables-save \u0026gt; /etc/sysconfig/iptables To restore the rules, you can either restart iptables or use iptables-restore:\nsudo iptables-restore \u0026lt; /etc/sysconfig/iptables If you need to manage iptables rules alongside Docker, remember:\nAlways restart iptables before Docker: sudo systemctl restart iptables sudo systemctl restart docker If you need to add custom rules to iptables for Docker containers, use the DOCKER-USER chain. This chain is designed to allow user-defined rules.\nTo add a custom rule to the DOCKER-USER chain, for example, allowing HTTP access to Docker containers:\nsudo iptables -A DOCKER-USER -p tcp --dport 80 -j ACCEPT Save and restore the rules to ensure persistence across system restarts: sudo iptables-save \u0026gt; /etc/sysconfig/iptables sudo systemctl restart iptables sudo systemctl restart docker By following these practices, we can manage iptables rules effectively while preventing conflicts with Docker\u0026rsquo;s automatic iptables management. This approach ensures a seamless integration of iptables with Docker and maintains the security of the system.\n","date":"5 May 2023","externalUrl":null,"permalink":"/posts/managing-iptables-as-a-service-and-integrating-with-docker/","section":"Posts","summary":"Reference: https://upcloud.com/resources/tutorials/configure-iptables-centos\nProblem: Non-persistent iptables rule after system restarts.\nThis is how to add a rule to iptables:\nsudo iptables -A INPUT -p tcp --dport ssh -j ACCEPT Solution:\nThis command applies the rule immediately, but it won\u0026rsquo;t persist after a system restart.","title":"Managing iptables as a Service and Integrating with Docker","type":"posts"},{"content":"After updating the command or env parameters in docker-compose.yml file, I restarted the container with docker-compose restart to make the change effect.\nHowever, I realize that the changes were not applied, and the container is still using the old config.\nTo apply the changes, I have to re-create the container:\ndocker-compose down docker-compose up -d This will stop and remove the existing container and create a new one with the updated config.\n","date":"5 May 2023","externalUrl":null,"permalink":"/posts/mistake-when-updating-docker-compose/","section":"Posts","summary":"After updating the command or env parameters in docker-compose.yml file, I restarted the container with docker-compose restart to make the change effect.\nHowever, I realize that the changes were not applied, and the container is still using the old config.","title":"Mistake when updating docker-compose.yml","type":"posts"},{"content":"","date":"5 May 2023","externalUrl":null,"permalink":"/tags/nat/","section":"Tags","summary":"","title":"nat","type":"tags"},{"content":"To set up a virtual networking lab on docker, I need to have the right to modify the network interface.\nOne way to achieve this is by running a Docker container with the --cap-add=NET_ADMIN flag, which grants the container the ability to configure network interfaces.\nAdditionally, adding --net=host flag to allow the container allow me to access the host network.\ndocker run -it --rm --cap-add=NET_ADMIN --net=host alpine Granting the NET_ADMIN capability and using the --net=host flag can be potentially dangerous because it allows a container to modify the network configuration of the host system.\nHowever, in a playground or testing environment, these capabilities can be useful for experimenting with different network configurations and scenarios.\nIt\u0026rsquo;s important to exercise caution and not use these capabilities in a production environment where security is a top priority.\n","date":"5 May 2023","externalUrl":null,"permalink":"/posts/network-acl-in-containers-for-virtual-networking-labs/","section":"Posts","summary":"To set up a virtual networking lab on docker, I need to have the right to modify the network interface.\nOne way to achieve this is by running a Docker container with the --cap-add=NET_ADMIN flag, which grants the container the ability to configure network interfaces.","title":"Network ACL in Containers for Virtual Networking Labs","type":"posts"},{"content":"","date":"5 May 2023","externalUrl":null,"permalink":"/tags/nfs/","section":"Tags","summary":"","title":"nfs","type":"tags"},{"content":"","date":"5 May 2023","externalUrl":null,"permalink":"/tags/pacemaker/","section":"Tags","summary":"","title":"pacemaker","type":"tags"},{"content":" Setup guide # With Ubuntu: https://www.digitalocean.com/community/tutorials/how-to-configure-bind-as-a-private-network-dns-server-on-ubuntu-20-04.\nWith Docker: https://github.com/hugotkk/lab-docker-bind.\nZone Transfer # With zone transfer enable, when a DNS record is updated in the primary DNS server, the update will be transferred automatically to the secondary DNS server.\nIn the primary server, we should set\ntype private allow-transfer { \u0026lt;secondary-ip\u0026gt;; } In the secondary server, we should set\ntype slave; masters { \u0026lt;primary-ip\u0026gt; }; Recursive Queries # Also, if we want to enable recursive queries (forwarding queries to a forwarder if the answer isn\u0026rsquo;t in the local DNS), we should only allow trusted servers to do so, especially if we\u0026rsquo;re a private DNS server.\nacl \u0026#34;trusted\u0026#34; { \u0026lt;trusted_servers\u0026gt;; }; options { .... recursion yes; allow-recursion { trusted; }; forwarders { 8.8.8.8; 8.8.4.4; }; To accomplish this, create an ACL that includes the IP addresses of trusted servers.\nIn the options section,\nenable recursion specify the trusted ACL in the allow-recursion. add one or more forwarders (e.g., public DNS servers such as 8.8.8.8 and 8.8.4.4) in the forwarders. ","date":"5 May 2023","externalUrl":null,"permalink":"/posts/setting-up-a-master-slave-dns-server-with-bind9-on-ubuntu-and-docker/","section":"Posts","summary":"Setup guide # With Ubuntu: https://www.digitalocean.com/community/tutorials/how-to-configure-bind-as-a-private-network-dns-server-on-ubuntu-20-04.\nWith Docker: https://github.com/hugotkk/lab-docker-bind.\nZone Transfer # With zone transfer enable, when a DNS record is updated in the primary DNS server, the update will be transferred automatically to the secondary DNS server.","title":"Setting Up a Master-Slave DNS Server with Bind9 on Ubuntu \u0026 Docker","type":"posts"},{"content":"Set up NFS:\nFollow this guide (for ubuntu): https://www.digitalocean.com/community/tutorials/how-to-set-up-an-nfs-mount-on-ubuntu-20-04 Autofs is a file system management tool that offers several key features:\nDynamic Mounting: File systems are mounted on-demand when they are accessed, reducing the number of mounts that need to be managed manually. Improved Performance: By reducing the number of mounted file systems, Autofs can improve system performance by reducing the amount of system resources required to manage mounts. Increased Security: Autofs can increase system security by reducing the number of mounted file systems available at any given time. This reduces the attack surface of the system. Autofs mount methods:\nDirect Indirect Let\u0026rsquo;s say nfs server is 192.168.0.101.\nDirect mapping example:\nMap 192.168.0.101:/data to /data vi /etc/master.auto /- /etc/direct.auto /home /etc/home.auto vi /etc/direct.auto /data 192.168.0.101:/data Indirect mapping example:\nMap 192.168.0.101:/data/hugotse to /home/hugotse Map 192.168.0.101:/data/ to /home/ vi /etc/home.auto hugotse 192.168.0.101:/data/hugotse * 192.168.0.101:/data/\u0026amp; Autofs man page:\nhttps://manpages.ubuntu.com/manpages/focal/en/man5/autofs.5.html ","date":"5 May 2023","externalUrl":null,"permalink":"/posts/setting-up-autofs-for-nfs-mounts/","section":"Posts","summary":"Set up NFS:\nFollow this guide (for ubuntu): https://www.digitalocean.com/community/tutorials/how-to-set-up-an-nfs-mount-on-ubuntu-20-04 Autofs is a file system management tool that offers several key features:\nDynamic Mounting: File systems are mounted on-demand when they are accessed, reducing the number of mounts that need to be managed manually.","title":"Setting Up Autofs for NFS Mounts","type":"posts"},{"content":"In this article, I will cover the basics of setting up firewalls on different distros, including firewalld, ufw, and iptables.\nWe will also discuss the daily tasks of firewall management, such as allowing or blocking specific source addresses/destination ports, port forwarding, and NAT.\nWeb Application Example: # A web server at 192.168.0.100 A node.js app at 192.168.0.200:8000 that is not publicly accessible Tasks:\nAllow port 80 from public: # iptables -A INPUT -p tcp -m tcp --dport 80 -j ACCEPT firewall-cmd --zone=public --add-port=80/tcp --permanent ufw allow 80/tcp Allow SSH only from a trusted IP, e.g., 43.164.66.12: # iptables -A INPUT -p tcp -s 43.164.66.12 --dport 22 -j ACCEPT firewall-cmd --zone=public --add-rich-rule \u0026#39;rule family=\u0026#34;ipv4\u0026#34; source address=43.164.66.12 port port=22 accept\u0026#39; ufw allow from 43.164.66.12 to any port 22/tcp Forward port 8080 requests to 192.168.0.200:8000: # iptables -t nat -A PREROUTING -p tcp --dport 8080 \\ -j DNAT --to-destination 192.168.0.200:8000 iptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE firewall-cmd --zone=public --add-masquerade firewall-cmd --zone=\u0026#34;public\u0026#34; --add-forward-port=port=8080:proto=tcp:toaddr=198.51.100.0:toport=8000 Drop traffic from bad IP, e.g., 233.228.5.86: # firewall-cmd --zone=public --add-rich-rule \u0026#39;rule family=\u0026#34;ipv4\u0026#34; source address=233.228.5.86 drop\u0026#39; ufw deny from 233.228.5.86 When working with the NAT table, we need to enable ip_forward as well in /etc/ufw/sysctl.conf:\nnet.ipv4.ip_forward = 1 NAT Example: # A Linux router with IP 192.160.0.100 with internet access Internal servers 192.160.0.200, 192.160.0.201, 192.160.0.203 using 192.160.0.100 as router. Tasks:\nConfig NAT for internal server to have internet access: # iptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE firewall-cmd --zone=public --add-masquerade By following these guidelines, you can set up your firewall effectively and ensure the security of your system.\nReference: # iptables allow/deny with filter NAT \u0026amp; port forwarding firewalld ufw ","date":"5 May 2023","externalUrl":null,"permalink":"/posts/summary-of-setting-up-firewall-on-different-distros/","section":"Posts","summary":"In this article, I will cover the basics of setting up firewalls on different distros, including firewalld, ufw, and iptables.\nWe will also discuss the daily tasks of firewall management, such as allowing or blocking specific source addresses/destination ports, port forwarding, and NAT.","title":"Summary of Setting up Firewall on Different Distros","type":"posts"},{"content":"","date":"5 May 2023","externalUrl":null,"permalink":"/tags/ufw/","section":"Tags","summary":"","title":"ufw","type":"tags"},{"content":"AWS logs\u0026rsquo; default interface can be challenging to navigate for in-depth analysis. Amazon Athena can help address two common issues.\nFirst, CloudTrail logs\u0026rsquo; default filters can be limiting. However, with Athena, you can use SQL to apply filters to each log field, allowing for more detailed analysis and improved insights.\nSecond, while CloudWatch Logs Insights is useful for log analysis, it lacks user-friendly options for exporting reports or searching historical data. Athena can help with this as well.\nTo get started with Athena, follow these steps:\nTo query CloudTrail logs using Athena, refer to Setting up CloudTrail Logs with Athena. To query CloudWatch logs using Athena and connectors, check out Setting up CloudWatch Logs Connector for Athena. ","date":"5 May 2023","externalUrl":null,"permalink":"/posts/using-amazon-athena-for-easier-aws-log-analysis/","section":"Posts","summary":"AWS logs\u0026rsquo; default interface can be challenging to navigate for in-depth analysis. Amazon Athena can help address two common issues.\nFirst, CloudTrail logs\u0026rsquo; default filters can be limiting. However, with Athena, you can use SQL to apply filters to each log field, allowing for more detailed analysis and improved insights.","title":"Using Amazon Athena for Easier AWS Log Analysis","type":"posts"},{"content":"","date":"5 May 2023","externalUrl":null,"permalink":"/tags/virtual-networking/","section":"Tags","summary":"","title":"virtual-networking","type":"tags"},{"content":"","date":"5 May 2023","externalUrl":null,"permalink":"/tags/vpn/","section":"Tags","summary":"","title":"vpn","type":"tags"},{"content":"","date":"5 May 2023","externalUrl":null,"permalink":"/tags/wireguard/","section":"Tags","summary":"","title":"wireguard","type":"tags"},{"content":"In this blog post, we will explore setting up WireGuard, a modern and secure VPN solution, as well as understanding the NAT approach and various network topologies used in VPN configurations.\nSetting Up WireGuard # Although the official WireGuard website doesn\u0026rsquo;t provide a configuration reference or examples, Stavros\u0026rsquo; blog post offers a clear guide on configuring WireGuard: How to Configure WireGuard. The WireGuard website does have a quickstart guide, but it focuses on command line setup: WireGuard Quickstart.\nwith NAT # For a lab setup using Docker, refer to this repository: docker-compose.yml.\nThe lab setup contains two networks:\ninternet (public subnet) hugo_home (private subnet) The \u0026ldquo;server\u0026rdquo; and \u0026ldquo;client\u0026rdquo; containers face the public network, with the VPN set up between them.\nThe goal is for the client to access services in the private subnet (e.g., \u0026ldquo;nginx\u0026rdquo;, \u0026ldquo;httpd\u0026rdquo;, \u0026ldquo;httpbin\u0026rdquo;) using NAT.\nMore use cases # Three topologies are discussed in this article: WireGuard Point-to-Site Routing.\nThese topologies resemble virtual networking architectures rather than VPN.\nSite-to-Site # To set up a site-to-site connection with WireGuard:\nConfigure services behind SiteA and SiteB endpoints to use their respective endpoints as routers. On the router, add a rule to route traffic going to the other site\u0026rsquo;s prefix to the router at the other site\u0026rsquo;s endpoint. Port Forwarding # Configure SiteA\u0026rsquo;s endpoint to access services behind SiteB\u0026rsquo;s endpoint, as explained in Summary of Setting Up Firewall on Different Distros. Point-to-Site (Site Gateway) # Configure SiteA\u0026rsquo;s endpoint to use SiteB\u0026rsquo;s endpoint as the router. SiteB\u0026rsquo;s endpoint will configure a route to direct traffic from SiteA\u0026rsquo;s endpoint prefix to SiteA\u0026rsquo;s endpoint directly. By following these guidelines, we can set up WireGuard VPN, understand the NAT approach, and apply various network topologies for needs.\n","date":"5 May 2023","externalUrl":null,"permalink":"/posts/wireguard-vpn-setup-and-network-topologies/","section":"Posts","summary":"In this blog post, we will explore setting up WireGuard, a modern and secure VPN solution, as well as understanding the NAT approach and various network topologies used in VPN configurations.","title":"WireGuard VPN Setup and Network Topologies","type":"posts"},{"content":"Comparing varchar DateTime:\nUse \u0026lsquo;\u0026gt;\u0026rsquo; and \u0026lsquo;\u0026lt;\u0026rsquo; operators Example: time \u0026gt;= \u0026#39;2022-07-20T00:00:00Z\u0026#39; Comparing int64 DateTime:\nConvert int64 and varchar to timestamp: FROM_UNIXTIME(time/1000) \u0026lt; TIMESTAMP \u0026#39;2022-07-20 00:00:00\u0026#39; Convert int64 to varchar: DATE_FORMAT(FROM_UNIXTIME(time/1000), \u0026#39;%Y-%m-%dT%H:%i:%sZ\u0026#39;) \u0026lt;= \u0026#39;2022-02-13T00:00:00Z\u0026#39; ","date":"5 May 2023","externalUrl":null,"permalink":"/posts/working-with-datetime-in-aws-athena/","section":"Posts","summary":"Comparing varchar DateTime:\nUse \u0026lsquo;\u0026gt;\u0026rsquo; and \u0026lsquo;\u0026lt;\u0026rsquo; operators Example: time \u0026gt;= \u0026#39;2022-07-20T00:00:00Z\u0026#39; Comparing int64 DateTime:\nConvert int64 and varchar to timestamp: FROM_UNIXTIME(time/1000) \u0026lt; TIMESTAMP \u0026#39;2022-07-20 00:00:00\u0026#39; Convert int64 to varchar: DATE_FORMAT(FROM_UNIXTIME(time/1000), \u0026#39;%Y-%m-%dT%H:%i:%sZ\u0026#39;) \u0026lt;= \u0026#39;2022-02-13T00:00:00Z\u0026#39; ","title":"Working with DateTime in AWS Athena","type":"posts"},{"content":"","date":"4 May 2023","externalUrl":null,"permalink":"/tags/2fa/","section":"Tags","summary":"","title":"2fa","type":"tags"},{"content":"There are two ways to mount volumes in Docker:\nHostpath: docker run -it --rm -v $PWD/myvol:/app ubuntu Volume: docker volume create myvol docker run -it --rm -v myvol:/app ubuntu When dealing with volume ownership, incorrect folder ownership can cause the container to crash, requiring time to debug logs.\nCommon issues:\nPermission denied errors in logs Bitnami images built with non-root user OpenShift starts containers with random UID Examples of volume permissions in Docker:\nPrometheus:\nData store: /prometheus Config path: /etc/prometheus Rules for volume ownership and permissions:\nIf the host volume doesn\u0026rsquo;t exist, Docker daemon will create it with root ownership. Example:\ndocker run -it --rm -v $PWD/data:/data prometheus $PWD/data will be owned by root.\nIf the host volume doesn\u0026rsquo;t exist but the path exists in the container, a new volume will be created, and the files inside that path will be copied to it. Ownership will follow the one inside the container. Example:\ndocker run -it --rm -v $PWD/data:/etc/prometheus prometheus $PWD/data ownership will be the same as /etc/prometheus. Files in /etc/promethues will be copied as well.\nIf the volume exists, it will follow its own ownership. Example: (@ubuntu)\nmkdir data docker run -it --rm -v $PWD/data:/etc/prometheus prometheus $PWD/myvol will be owned by ubuntu.\nIssues arise when the container is running as a non-root user. In these cases, folders may be writable by the container or the host, but not both.\nSolutions for non-root user containers (not ideal):\nStart the container with root user (may not work for applications that require a specific user, e.g., MariaDB).\nOn the host, create the folder first, then use sudo to change the volume owner to the container user.\nChange the folder permission to 777, allowing both host and container to write to it.\n","date":"4 May 2023","externalUrl":null,"permalink":"/posts/docker-volume-mounting-permissions-and-ownership-explained/","section":"Posts","summary":"There are two ways to mount volumes in Docker:\nHostpath: docker run -it --rm -v $PWD/myvol:/app ubuntu Volume: docker volume create myvol docker run -it --rm -v myvol:/app ubuntu When dealing with volume ownership, incorrect folder ownership can cause the container to crash, requiring time to debug logs.","title":"Docker Volume Mounting: Permissions and Ownership Explained","type":"posts"},{"content":"","date":"4 May 2023","externalUrl":null,"permalink":"/tags/google-authentication/","section":"Tags","summary":"","title":"google-authentication","type":"tags"},{"content":" HostPath: Shares storage between nodes Can be used for data persistence across container restarts EmptyDir: Independent for each pod Shared between containers within a pod Data is not persisted across container restarts ","date":"4 May 2023","externalUrl":null,"permalink":"/posts/k8s-volumes-hostpath-and-emptydir-explained/","section":"Posts","summary":" HostPath: Shares storage between nodes Can be used for data persistence across container restarts EmptyDir: Independent for each pod Shared between containers within a pod Data is not persisted across container restarts ","title":"k8s Volumes: HostPath and EmptyDir Explained","type":"posts"},{"content":"","date":"4 May 2023","externalUrl":null,"permalink":"/tags/mount/","section":"Tags","summary":"","title":"mount","type":"tags"},{"content":" Docker # ubuntu@ip-172-31-6-200:~$ docker run --rm -it -P httpd ubuntu@ip-172-31-6-200:~$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 6f6f53ee7dde httpd \u0026#34;httpd-foreground\u0026#34; 12 seconds ago Up 10 seconds 0.0.0.0:32768-\u0026gt;80/tcp, :::32768-\u0026gt;80/tcp dreamy_goodall Docker automatically knows which ports a container needs due to the EXPOSE directive in the container\u0026rsquo;s Dockerfile. (httpd\u0026rsquo;s Dockerfile) Removing EXPOSE from the Dockerfile will not publish the port automatically anymore when using the -P flag. Howerver, containers can still reach ports within the container via its IP/alias. Docker does not enforce any security policies between containers. We can test it by:\nCreate a custom network Start two containers, connected to the same network Install net-tools on both containers Listen to port 3000 on container u1 Connect to u1:3000 on container u2 Connection successful! On host:\ndocker network create mynetwork docker run -it --rm --name u1 --network mynetwork ubuntu docker run -it --rm --name u2 --network mynetwork ubuntu On both container:\napt update apt install -y iproute2 netcat On u1:\nroot@b7bfb1fa159e:/# while true; do nc -l 3000; done On u2:\nroot@1bd1cee0ce53:/# nc b7bfb1fa159e 3000 -v Connection to b7bfb1fa159e (172.18.0.2) 3000 port [tcp/*] succeeded! Kubernetes # Pods can reach each other, even in different namespaces. Containers only need to listen on a port inside, and there is not nessessary to specify the exposed port in the Dockerfile or mention the pod config. The ports field in the container object is used to list the ports to expose from the container. Not specifying a port here does not prevent that port from being exposed. Any port that is listening on the default 0.0.0.0 address inside a container will be accessible from the network. Modifying the ports array with a strategic merge patch may corrupt the data. The ports field is more like a self-document purpose. Reference: https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/pod-v1/#ports\nWe can re-test it in k8s, starting the pods in different namespace.\nOn host:\nkubectl create ns ns1 kubectl create ns ns2 kubectl run -n ns1 -it u1 --image=ubuntu kubectl run -n ns2 -it u2 --image=ubuntu repeat what we did in Docker - u2 is able to connect to u1:3000 with ip.\n","date":"4 May 2023","externalUrl":null,"permalink":"/posts/networking-in-docker-and-kubernetes-port-expose-and-pod-communication/","section":"Posts","summary":"Docker # ubuntu@ip-172-31-6-200:~$ docker run --rm -it -P httpd ubuntu@ip-172-31-6-200:~$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 6f6f53ee7dde httpd \u0026#34;httpd-foreground\u0026#34; 12 seconds ago Up 10 seconds 0.","title":"Networking in Docker and Kubernetes: Port Expose and Pod Communication","type":"posts"},{"content":"","date":"4 May 2023","externalUrl":null,"permalink":"/tags/service-account/","section":"Tags","summary":"","title":"service-account","type":"tags"},{"content":"Follow Digital Ocean tutorial for the setup: https://www.digitalocean.com/community/tutorials/how-to-set-up-multi-factor-authentication-for-ssh-on-centos-7\nNotes on /etc/ssh/sshd_config:\nWith UsePAM yes, both password and keyboard-interactive follow /etc/pam.d/sshd PasswordAuthentication yes is the same as AuthenticationMethods password Differences between AuthenticationMethods password and keyboard-interactive:\npassword accepts only 1 response from the user keyboard-interactive accepts multiple responses from user Enforcing both password and Google Authenticator to login:\nUse keyboard-interactive to ensure successful authentication with 2FA and password Prompt differences:\npassword: (hugo@192.168.0.14) Password: keyboard-interactive: Password: Time synchronization:\nCrucial for accurate authentication For Docker, ensure the host\u0026rsquo;s clock is synchronized ","date":"4 May 2023","externalUrl":null,"permalink":"/posts/setting-up-google-auth-on-ssh-on-centos-7/","section":"Posts","summary":"Follow Digital Ocean tutorial for the setup: https://www.digitalocean.com/community/tutorials/how-to-set-up-multi-factor-authentication-for-ssh-on-centos-7\nNotes on /etc/ssh/sshd_config:\nWith UsePAM yes, both password and keyboard-interactive follow /etc/pam.d/sshd PasswordAuthentication yes is the same as AuthenticationMethods password Differences between AuthenticationMethods password and keyboard-interactive:","title":"Setting Up Google Auth on SSH on CentOS-7","type":"posts"},{"content":"","date":"4 May 2023","externalUrl":null,"permalink":"/tags/ssh/","section":"Tags","summary":"","title":"ssh","type":"tags"},{"content":"The token will be stored at /var/run/secrets/kubernetes.io/serviceaccount/token in container. (See here)\nThree ways to use:\nBy code: Kubernetes Python library With kubectl binary: it will use the token automatically as your credentials. So we can do kubectl get po within the pod curl: eg: TOKEN=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token) API_ENDPOINT=$KUBERNETES_SERVICE_HOST:$KUBERNETES_SERVICE_PORT_HTTPS curl --cacert /var/run/secrets/kubernetes.io/serviceaccount/ca.crt \\ -H \u0026#39;Authentification: Bearer $TOKEN\u0026#39; \\ https://$API_ENDPOINT/v1/api/pods ","date":"4 May 2023","externalUrl":null,"permalink":"/posts/three-ways-to-use-kubernetes-service-account-token/","section":"Posts","summary":"The token will be stored at /var/run/secrets/kubernetes.io/serviceaccount/token in container. (See here)\nThree ways to use:\nBy code: Kubernetes Python library With kubectl binary: it will use the token automatically as your credentials.","title":"Three Ways to Use Kubernetes Service Account Token","type":"posts"},{"content":"","date":"4 May 2023","externalUrl":null,"permalink":"/tags/ubuntu/","section":"Tags","summary":"","title":"ubuntu","type":"tags"},{"content":"Working with Ubuntu 22.04 autoinstall can be challenging due to incomplete documentation and time-consuming debugging. This post shares my experience and offers tips to help others facing similar issues.\nHere is the documentation of autoinstall and curtain.\nAnd my work: https://github.com/hugotkk/labs/blob/main/ubuntu-quickstart/user-data\nRemember the #cloud-config Header # The #cloud-config header is crucial for a successful autoinstall. Without it, the process won\u0026rsquo;t work.\nCustomizing Storage: Fixing the \u0026lsquo;fstype\u0026rsquo; Error # While customizing storage, I encountered an error: \u0026ldquo;LVM_LogicalVolume object has no attribute \u0026lsquo;fstype\u0026rsquo;.\u0026rdquo; To resolve this, modify the \u0026lsquo;mount\u0026rsquo; reference to \u0026lsquo;format\u0026rsquo; instead of LVM, as shown below:\n- id: lv-root type: lvm_partition name: root volgroup: vg size: 10G - id: root-fs type: format fstype: ext4 volume: lv-root - id: mount-root type: mount path: / device: lv-root # should reference to root-fs not lv-root Installing Software in late-commands # When trying to install software in the late-commands section, keep in mind that during autoinstall, Ubuntu is mounted on /target, not /. To address this, use curtin:\nlate-commands: - curtin in-target --target=/target -- \u0026lt;INSTALL_CMD_LIKE_APT_INSTALL\u0026gt; Two Ways to Use autoinstall # There are two methods for using autoinstall: over HTTP and with an additional volume. I found the latter more practical in my case as I\u0026rsquo;m using Virtualbox.\n","date":"4 May 2023","externalUrl":null,"permalink":"/posts/ubuntu-2204-autoinstall/","section":"Posts","summary":"Working with Ubuntu 22.04 autoinstall can be challenging due to incomplete documentation and time-consuming debugging. This post shares my experience and offers tips to help others facing similar issues.\nHere is the documentation of autoinstall and curtain.","title":"Ubuntu 22.04 autoinstall","type":"posts"},{"content":"","date":"4 May 2023","externalUrl":null,"permalink":"/tags/volume/","section":"Tags","summary":"","title":"volume","type":"tags"},{"content":"","date":"26 April 2023","externalUrl":null,"permalink":"/tags/monitor/","section":"Tags","summary":"","title":"monitor","type":"tags"},{"content":"","date":"26 April 2023","externalUrl":null,"permalink":"/tags/prometheus/","section":"Tags","summary":"","title":"prometheus","type":"tags"},{"content":"An example of Prometheus data:\nhttp_request_total{method=\u0026#34;GET\u0026#34;, endpoint=\u0026#34;/contact-us\u0026#34;, status=\u0026#34;200\u0026#34;} 1 2 3 4 5 http_request_total{method=\u0026#34;POST\u0026#34;, endpoint=\u0026#34;/auth\u0026#34;, status=\u0026#34;400\u0026#34;} 1 6 8 10 15 Key Concepts # Metric: Quantity measurement (e.g.: http_request_total) Metric label: Metadata for the measurement (e.g.: method=\u0026quot;GET\u0026quot;) Sample: Data point at a certain time (e.g.: 5) - float64 Series: Unique combination of metric labels (e.g.: http_request_total{method=\u0026quot;GET\u0026quot;, endpoint=\u0026quot;/contact-us\u0026quot;, status=\u0026quot;200\u0026quot;} and http_request_total{method=\u0026quot;POST\u0026quot;, endpoint=\u0026quot;/auth\u0026quot;, status=\u0026quot;400\u0026quot;}) Time series: Samples over time (e.g.: 1 2 3 4 5) Data Types # Instant vector: http_request_total{method=\u0026quot;GET\u0026quot;} Range vector: http_request_total{method=\u0026quot;GET\u0026quot;}[5m] Scalar: numbers Metric Types # Prometheus supports four metric types:\nGauge: Values can go up and down (e.g.: logged_users) Counter: Values can only increase (e.g.: http_request_total) Histogram: Provides \u0026lt;metric_name\u0026gt;_bucket, \u0026lt;metric_name\u0026gt;_sum, \u0026lt;metric_name\u0026gt;_count. Use histogram_quantile() for server-side quantile calculation (e.g.: http_request_duration_seconds) Summary: Similar to Histogram, but quantiles are calculated client-side (application). Thus, it cannot be further aggregated. promql # Operator Precedence # Prometheus supports a range of binary operators with different precedence levels. From highest to lowest precedence:\n^ *, /, %, atan2 +, - ==, !=, \u0026lt;=, \u0026lt;, \u0026gt;=, \u0026gt; and, unless or Reference\nModifiers # @ 1609746000 - pretend the query time is 1609746000 offset 5m - pretend the query time is 5 minutes ago Have to use right after the select (before any function call)\nVector Matching # Vector scalar:\nExample: http_request_total / 2 Vector Vector:\nTypes of matching: One-to-One One-to-Many Many-to-One Matches vectors using labels by default Customize matching key with ignore() or in() Use group_right() or group_left() for many side Use group_left(labels) to bring labels from one to many side Example:\nmethod_code:http_errors:rate5m{method=\u0026#34;get\u0026#34;, code=\u0026#34;500\u0026#34;} 24 method_code:http_errors:rate5m{method=\u0026#34;get\u0026#34;, code=\u0026#34;404\u0026#34;} 30 method_code:http_errors:rate5m{method=\u0026#34;put\u0026#34;, code=\u0026#34;501\u0026#34;} 3 method_code:http_errors:rate5m{method=\u0026#34;post\u0026#34;, code=\u0026#34;500\u0026#34;} 6 method_code:http_errors:rate5m{method=\u0026#34;post\u0026#34;, code=\u0026#34;404\u0026#34;} 21 method:http_requests:rate5m{method=\u0026#34;get\u0026#34;, foo=\u0026#34;bar\u0026#34;} 600 method:http_requests:rate5m{method=\u0026#34;del\u0026#34;, foo=\u0026#34;bar1\u0026#34;} 34 method:http_requests:rate5m{method=\u0026#34;post\u0026#34;, foo=\u0026#34;bar2\u0026#34;} 12 method_code:http_errors:rate5m{code=\u0026#34;500\u0026#34;} / ignoring(code) group_left(foo) method:http_requests:rate5m {method=\u0026#34;get\u0026#34;, code=\u0026#34;500\u0026#34;, foo=\u0026#34;bar\u0026#34;} 0.04 {method=\u0026#34;get\u0026#34;, code=\u0026#34;404\u0026#34;, foo=\u0026#34;bar\u0026#34;} 0.05 {method=\u0026#34;post\u0026#34;, code=\u0026#34;500\u0026#34;, foo=\u0026#34;bar2\u0026#34;} 0.05 {method=\u0026#34;post\u0026#34;, code=\u0026#34;404\u0026#34;, foo=\u0026#34;bar2\u0026#34;} 0.175 If no group_left(foo), foo=”bar” will gone\nReference\nCommon Prometheus Functions # changes(): Number of changes over time time(): Current timestamp timestamp(): Timestamp of the sample Derivative and Rate: deriv(): gauge; rate(), irate(): counter Delta and Increase: delta(), idelta(): gauge increase(): counter irate() vs rate(): irate(): (last - first datapoint)/time range rate(): (projected end - start time datapoint)/time range Aggregration: \u0026lt;aggregation\u0026gt;: sum, count, max, min, avg, etc: Aggregates across dimensions (group by labels) \u0026lt;aggregation\u0026gt;_over_time(): Aggregates across time (group by time) Examples:\nsum(http_request_total) Result:\n{} 9 sum_over_time(http_request_total{method=\u0026#34;GET\u0026#34;}[5m]) Result:\n{method=\u0026#34;GET\u0026#34;, endpoint=\u0026#34;/contact-us\u0026#34;, status=\u0026#34;200\u0026#34;} 10 # 1+2+3+4+5 {method=\u0026#34;POST\u0026#34;, endpoint=\u0026#34;/auth\u0026#34;, status=\u0026#34;400\u0026#34;} 25 #1+6+8+10+15 Reference\nPrometheus Client Library Usage # Instrumentation Writing exporters Pushing metrics to Pushgateway Gist reference\nStorage # Not recommended to use NFS for storage: reference for storage Agent Mode # Disables query, alert, and recording rule functions Scrapes metrics from target and remotely writes to other instances Reference Service Discovery # Static: Define target servers in the config file *_sd_config: Use built-in configurations (e.g.: EC2, Kubernetes, file) Custom: Use file_sd_config. Update the file periodically. Each scrape config can have:\ninterval timeout proxy metrics_path Relabeling # relabel_configs: Modify scrape parameters before scraping (e.g.: Blackbox exporter) metrics_relabel_configs: Modify data collected after scraping (e.g.: remove unwanted metrics) Alerting in Prometheus # Evaluates rules, fires alerts, routes to destination Does not handle notifications Routes by matching rules with labels Labels: alert identity Annotations: longer-form description Annotations support templating with go lang syntax Reference labels in annotations can be done by {{ $labels.foo }} Alertmanager # Silencing alerts use cases:\nProvisioning new servers Decommissioning servers Maintenance Inhibiting:\nStop a group of alerts when another alert is triggered Example: Cluster down alert inhibits memory or disk check alerts ","date":"26 April 2023","externalUrl":null,"permalink":"/posts/note-about-prometheus-2/","section":"Posts","summary":"An example of Prometheus data:\nhttp_request_total{method=\u0026#34;GET\u0026#34;, endpoint=\u0026#34;/contact-us\u0026#34;, status=\u0026#34;200\u0026#34;} 1 2 3 4 5 http_request_total{method=\u0026#34;POST\u0026#34;, endpoint=\u0026#34;/auth\u0026#34;, status=\u0026#34;400\u0026#34;} 1 6 8 10 15 Key Concepts # Metric: Quantity measurement (e.g.: http_request_total) Metric label: Metadata for the measurement (e.","title":"Understanding Prometheus - Metrics, Data Types, and Querying","type":"posts"},{"content":"","date":"19 April 2023","externalUrl":null,"permalink":"/tags/ad/","section":"Tags","summary":"","title":"ad","type":"tags"},{"content":"","date":"19 April 2023","externalUrl":null,"permalink":"/tags/adfs/","section":"Tags","summary":"","title":"adfs","type":"tags"},{"content":"","date":"19 April 2023","externalUrl":null,"permalink":"/tags/api-gateway/","section":"Tags","summary":"","title":"api-gateway","type":"tags"},{"content":" Two connections maintained # ALB to target servers. Client to ALB. Keepalive/h2 setting # Applies to ALB-to-target connection, not client-to-target servers. Why Keepalive help to improve the performance: Limited number of connections between ALB to target servers, keepalive helps. Sticky session # Cookie-based. Idle_timeout # Applies to both ALB-to-target and client-to-ALB connections. Closes connections without data sent/received within timeout period. Timeout configuration # ALB idle_timeout should be longer than the application timeout. Prevents traffic being sent to dead targets and causing 502 errors. ","date":"19 April 2023","externalUrl":null,"permalink":"/posts/aws-alb/","section":"Posts","summary":"Two connections maintained # ALB to target servers. Client to ALB. Keepalive/h2 setting # Applies to ALB-to-target connection, not client-to-target servers. Why Keepalive help to improve the performance: Limited number of connections between ALB to target servers, keepalive helps.","title":"AWS ALB","type":"posts"},{"content":" WebSocket Overview # WebSocket Characteristics Bi-directional, persistent TCP connection between client and server Scaling Limitations WebSocket is stateful, cannot horizontally scale without a backend to store state (eg: Redis) API Gateway with Websocket # Benefits\nAids scaling by maintaining WebSocket connections Serverless - Client and Lambda interact with API Gateway Lambda uses APIGatewayConnection API to send data to open connections Client sends data to gateway directly Limitations\nWebSocket connection lasts for 2 hours Idle timeout is 10 minutes Lab: Building a Chat Room # Setup # I watched this tutorial on how to build a chat using Lambda, WebSocket, and API Gateway with Node.js\nAccess the code in this GitHub Gist\nServer Setup\nserver.py is a simple HTTP server with Python to serve the client-side code Lambda Function\nindex.mjs is the Lambda code for the chat room Implements $connect, $disconnect, join, leave, and sendAll functions Uses DynamoDB to store connectionIds and roomId with the connectionIds that joined Common Errors and Solutions # Global Variable Persistence # The video stores the connections and rooms data in global variables in lambda. Initially, it was thought that it would not work because each invocation is stateless. However, variables in the global scope can be shared between Lambda invocations. This is mentioned in the Comparing the effect of global scope. However, it is more reliable to use a database for storing data. The use of global variables should be limited to demo purposes only and should never be used in the real world. Connection Endpoint # The endpoints can be found in the AWS Console under \u0026ldquo;API Gateway\u0026rdquo; \u0026gt; \u0026ldquo;Stages\u0026rdquo;. One is WebSocket, starting with wss://, and another is HTTPS, starting with https://. When posting the data back to the client from Lambda, use the HTTPS endpoint but remove @connections at the end. For example, if the endpoint on the console is https://d6sq1c52q5.execute-api.us-east-1.amazonaws.com/production/@connections, we should omit the @connection when using PostToConnectionCommand. Lambda Permissions # Lambda needs permission to call invoke and manage connection Use AmazonAPIGatewayInvokeFullAccess or create a custom policy Custom policy example:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;execute-api:Invoke\u0026#34;, \u0026#34;execute-api:ManageConnections\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:execute-api:us-east-1:059035646743:d8ck9i69wa/production/*\u0026#34;\u0026#34; } ] } Refer to the AWS documentation for more information.\n\u0026ldquo;Going Away\u0026rdquo; Exception # You might encounter a \u0026ldquo;Going away\u0026rdquo; exception if you try to post data to a connection in $connect This is not supported, as mentioned in this Stack Overflow post Using Node.js 18 in Lambda Function # If you use Node.js 18, require('aws-sdk') will not work due to the use of ES module style When connecting to the WebSocket, it may return a 502 error, which is also logged in the API Gateway access log but not useful for debugging purposes. To diagnose the issue, check the Lambda error log on CloudWatch To tail the CloudWatch log, use the following command:\naws logs tail \u0026lt;log_group\u0026gt; --follow Refer to the AWS CLI documentation for more information.\n","date":"19 April 2023","externalUrl":null,"permalink":"/posts/aws-api-gateway-wss/","section":"Posts","summary":"WebSocket Overview # WebSocket Characteristics Bi-directional, persistent TCP connection between client and server Scaling Limitations WebSocket is stateful, cannot horizontally scale without a backend to store state (eg: Redis) API Gateway with Websocket # Benefits","title":"Building a Chat Room with Lambda and Websocket in AWS API Gateway","type":"posts"},{"content":"To enable authorization the api with Amazon Cognito User Pools:\nCreate a user pool. Check out Secure your API Gateway with Amazon Cognito User Pools for a video tutorial. In the \u0026ldquo;Method Request\u0026rdquo; \u0026gt; \u0026ldquo;Auth\u0026rdquo; section of the API Gateway console, select the user pool. Access the API with an ID token: curl --location \u0026#39;https://\u0026lt;my_api_gateway_domain\u0026gt;/\u0026lt;my_api\u0026gt;\u0026#39; \\ --header \u0026#39;Authorization: Bearer \u0026lt;id_token\u0026gt;\u0026#39; ``` To generate the ID token from Cognito:\nGet an authorization token with the authorize API: https://\u0026lt;my_cognito_domain\u0026gt;/oauth2/authorize?response_type=code\u0026amp;client_id=\u0026lt;client_id\u0026gt;\u0026amp;redirect_uri=\u0026lt;redirect_url\u0026gt;\u0026amp;state=STATE\u0026amp;scope=openid%20email Use the authorization token to exchange it for an ID token from the token API: curl --location \u0026#39;https://\u0026lt;my_cognito_domain\u0026gt;/oauth2/token\u0026#39; \\ --header \u0026#39;Content-Type: application/x-www-form-urlencoded\u0026#39; \\ --data-urlencode \u0026#39;grant_type=authorization_code\u0026#39; \\ --data-urlencode \u0026#39;client_id=\u0026lt;client_id\u0026gt;\u0026#39; \\ --data-urlencode \u0026#39;client_secret=\u0026lt;client_secret\u0026gt;\u0026#39; \\ --data-urlencode \u0026#39;code=\u0026lt;auth_token\u0026gt;\u0026#39; \\ --data-urlencode \u0026#39;redirect_uri=\u0026lt;redirect_uri\u0026gt;\u0026#39; ``` For more details about the authorize and token endpoints, check out the following links:\nAuthorize endpoint Token endpoint A more straightforward way to obtain the tokens is by using Postman, which supports OAuth 2.0 authentication. Check out OAuth2 0 Authorization with Postman for a video tutorial.\n","date":"19 April 2023","externalUrl":null,"permalink":"/posts/aws-api-gateway-with-cognito/","section":"Posts","summary":"To enable authorization the api with Amazon Cognito User Pools:\nCreate a user pool. Check out Secure your API Gateway with Amazon Cognito User Pools for a video tutorial. In the \u0026ldquo;Method Request\u0026rdquo; \u0026gt; \u0026ldquo;Auth\u0026rdquo; section of the API Gateway console, select the user pool.","title":"Building a Secure API Gateway with Cognito","type":"posts"},{"content":"","date":"19 April 2023","externalUrl":null,"permalink":"/tags/cognito/","section":"Tags","summary":"","title":"cognito","type":"tags"},{"content":" Main difference # REST HTTP Quota Management Per group Not supported API key Management Supported Not supported Authorization Lambda/Cognito Lambda/IAM/JWT Lambda Input Payload only Request details (Event) VTL model Supported Not supported SDK and Documentation Generation Supported Not supported Lambda integration # Both REST and HTTP Lambda integrations offer a powerful and flexible way to integrate Lambda functions with API Gateway, with some differences in input/output format and response handling.\nInput / Output Format # Lambda Proxy Integration in REST and Lambda Integration in HTTP both pass the request information to the event object in the Lambda function. However, the input and output format are different. REST Lambda Proxy Integration Input Format HTTP Lambda Integration Proxy Format In HTTP APIs, if the Lambda function returns valid JSON and doesn\u0026rsquo;t return a statusCode or a string, API Gateway assumes that the result is the body of the response. In REST APIs, the Lambda function has to construct the response object with the appropriate status code, headers, and body. This is not possible in HTTP APIs. VTL # In REST APIs, the Lambda function can use VTL templates to transform the response data. VTL templates allow for specifying how the response data should be transformed from one format to another. In HTTP APIs, only JSON data is supported and response transformation using VTL templates is not possible. * The VTL is used in REST APIs for the following:\nRequest \u0026amp; Response Validation Generate an SDK Initialize a mapping template Data Transformations # Integration Type Path/ Query/ Header Mapping Template lambda N/A Y lambda proxy N N http Y Y http proxy Y Y Data that Backend Receives # Integration Type Path Query Headers Payload lambda N N N Y lambda proxy Y Y Y Y http Y Y Y Y http proxy Y Y Y Y ","date":"19 April 2023","externalUrl":null,"permalink":"/posts/aws-api-gateway-comparson/","section":"Posts","summary":"Main difference # REST HTTP Quota Management Per group Not supported API key Management Supported Not supported Authorization Lambda/Cognito Lambda/IAM/JWT Lambda Input Payload only Request details (Event) VTL model Supported Not supported SDK and Documentation Generation Supported Not supported Lambda integration # Both REST and HTTP Lambda integrations offer a powerful and flexible way to integrate Lambda functions with API Gateway, with some differences in input/output format and response handling.","title":"Comparing HTTP and REST API Gateways","type":"posts"},{"content":"","date":"19 April 2023","externalUrl":null,"permalink":"/tags/devops/","section":"Tags","summary":"","title":"devops","type":"tags"},{"content":"","date":"19 April 2023","externalUrl":null,"permalink":"/tags/display-filter/","section":"Tags","summary":"","title":"display-filter","type":"tags"},{"content":"","date":"19 April 2023","externalUrl":null,"permalink":"/tags/dynamodb/","section":"Tags","summary":"","title":"dynamodb","type":"tags"},{"content":"","date":"19 April 2023","externalUrl":null,"permalink":"/tags/ecs/","section":"Tags","summary":"","title":"ecs","type":"tags"},{"content":"","date":"19 April 2023","externalUrl":null,"permalink":"/tags/iam/","section":"Tags","summary":"","title":"iam","type":"tags"},{"content":"","date":"19 April 2023","externalUrl":null,"permalink":"/tags/identity-provider/","section":"Tags","summary":"","title":"identity-provider","type":"tags"},{"content":"","date":"19 April 2023","externalUrl":null,"permalink":"/tags/lambda/","section":"Tags","summary":"","title":"lambda","type":"tags"},{"content":"","date":"19 April 2023","externalUrl":null,"permalink":"/tags/nlb/","section":"Tags","summary":"","title":"nlb","type":"tags"},{"content":"","date":"19 April 2023","externalUrl":null,"permalink":"/tags/secret-manager/","section":"Tags","summary":"","title":"secret-manager","type":"tags"},{"content":" Create a new Windows Server 2012 instance and install the following Roles and Features:\nDNS ADFS AD Promote the server to a Domain Controller and create a new forest. I named mine hhuge9.com.\nFollow a tutorial (such as this one: https://www.youtube.com/watch?v=9eq3IeDAkvA) to configure ADFS.\nYou can skip the process of generating the certificate in those tutorial, as it can be self-signed and needs to be in PFX format for ADFS to use it.\nIf you only have an OpenSSH key and certificate, you can convert it to PFX format using the following command:\nopenssl pkcs12 -export -out certificate.pfx -inkey hhuge9.com.key -in hhuge9.com.crt -certpbe PBE-SHA1-3DES -keypbe PBE-SHA1-3DES -nomac\nThen, copy the resulting PFX file to the Windows server, double-click it, and start the import process. Once it\u0026rsquo;s imported, the certificate should be shown in the ADFS Wizard.\nIn Active Directory, create a new user (I named mine tsek) and include their email address (which is a required field for the RoleSessionName).\nCreate a group called AWS-437735673474-ADFS-Admin (replace 437735673474 with your actual AWS account ID and ADFS-Admin with the name of the IAM role you want to assume in AWS).\nAdd the tsek user to the AWS-437735673474-ADFS-Admin group.\nFollow the tutorial at https://aws.amazon.com/blogs/security/aws-federated-authentication-with-active-directory-federation-services-ad-fs/ to set up a relay party trust on ADFS and add claim rules.\nIn AWS IAM, add an identity provider and set the IAM role to \u0026ldquo;ADFS-Admin\u0026rdquo;.\nUse the ADFS login page (https://hhuge9.com/adfs/ls/idpinitiatedsignon) to log in to AWS using your AD credentials.\n","date":"19 April 2023","externalUrl":null,"permalink":"/posts/aws-adfs/","section":"Posts","summary":"Create a new Windows Server 2012 instance and install the following Roles and Features:\nDNS ADFS AD Promote the server to a Domain Controller and create a new forest. I named mine hhuge9.","title":"Setting up ADFS Login in AWS with Windows Server 2012","type":"posts"},{"content":" gRPC # Concept: Similar to REST API and JSON. Payload format: Binary Performance: Faster as smaller size than JSON. Usage: Primarily for service-to-service communication. Browser-to-service not widely supported. Protocol: Built on HTTP/2 client -\u0026gt; ALB: HTTPS only; ALB -\u0026gt; target: HTTP/HTTPS ","date":"19 April 2023","externalUrl":null,"permalink":"/posts/aws-alb-grpc/","section":"Posts","summary":" gRPC # Concept: Similar to REST API and JSON. Payload format: Binary Performance: Faster as smaller size than JSON. Usage: Primarily for service-to-service communication. Browser-to-service not widely supported. Protocol: Built on HTTP/2 client -\u0026gt; ALB: HTTPS only; ALB -\u0026gt; target: HTTP/HTTPS ","title":"Setup gPRC with AWS ALB","type":"posts"},{"content":"","date":"19 April 2023","externalUrl":null,"permalink":"/tags/tls/","section":"Tags","summary":"","title":"tls","type":"tags"},{"content":" TLS Passthrough with AWS NLB # To setup TLS passthrough with NLB, follow these steps:\nListen: TCP - can be 80 or 443 Target: TCP - 443 Backend: HTTPS on 443 Here are some interesting features of NLB:\nProxy Protocol v2 TCP Adds a binary header before the HTTP payload Provides the information about the client like X-Forward-For in ALB To enable Proxy Protocol v2, the target server has to understand the protocol Health check has to be TCP as well Nginx can listen to Proxy Protocol v2 - config available The backend is a simple HTTP server -\u0026gt; it can see the X-Forward-To headers that were passed in the request No security group policy on NLB Health check HTTPS: 443 with invalid cert -\u0026gt; OK HTTP: if HTTP redirect to HTTPS was set up, the health check status will be 301 Can check the health check request in the server access log If 400 - wrong protocol. H2 or sending HTTPS to HTTP port If no request comes in - security group blocked - in ALB or EC2 Can shorten the health threshold and interval to speed up the health check initially Preserve client IP addresses Can preserve client IP. In PHP, the $_SERVER['REMOTE_ADDR'] will change to the client\u0026rsquo;s IP Zone level load balancing By default, NLB load balances within the zone Imagine that the NLB is a group of EC2 in different zones. The CNAME has to return multiple IPs to each EC2 The route algorithm is affected on the load balancer EC2 zone itself. Not cross-zone, but ALB is cross-zone by default ","date":"19 April 2023","externalUrl":null,"permalink":"/posts/aws-nlb/","section":"Posts","summary":"TLS Passthrough with AWS NLB # To setup TLS passthrough with NLB, follow these steps:\nListen: TCP - can be 80 or 443 Target: TCP - 443 Backend: HTTPS on 443 Here are some interesting features of NLB:","title":"Understanding TLS Passthrough and Other Features of AWS NLB","type":"posts"},{"content":"I was surprised to find that AWS Lambda cannot directly reference records from Secrets Manager in environment, especially considering that ECS can reference records from both Parameter Store and Secrets Manager. There are two ways to overcome this limitation in Lambda:\nUse the AWS API. Use a Lambda Extension to retrieve secrets from Secrets Manager. ","date":"19 April 2023","externalUrl":null,"permalink":"/posts/unexpected-limitations-aws-lambdas-inability-to-directly-reference-secrets-manager-and-parameter-store/","section":"Posts","summary":"I was surprised to find that AWS Lambda cannot directly reference records from Secrets Manager in environment, especially considering that ECS can reference records from both Parameter Store and Secrets Manager.","title":"Unexpected Limitations: AWS Lambda's Inability to Directly Reference Secrets Manager and Parameter Store","type":"posts"},{"content":"","date":"19 April 2023","externalUrl":null,"permalink":"/tags/websocket/","section":"Tags","summary":"","title":"websocket","type":"tags"},{"content":"","date":"19 April 2023","externalUrl":null,"permalink":"/tags/windows/","section":"Tags","summary":"","title":"windows","type":"tags"},{"content":"","date":"19 April 2023","externalUrl":null,"permalink":"/tags/wireshark/","section":"Tags","summary":"","title":"wireshark","type":"tags"},{"content":" Changing Column Display\nDecrypting HTTPS Traffic:\nClose all Chrome instances Start Chrome with custom param to log certificates to ssh-key.log Reference ssh-key.log in Preferences \u0026gt; Protocols \u0026gt; TLS \u0026gt; (Pre)-Master-Secret log Common Display Filter Examples\n","date":"19 April 2023","externalUrl":null,"permalink":"/posts/wireshark-customization-https-traffic-decryption-and-common-display-filters/","section":"Posts","summary":"Changing Column Display\nDecrypting HTTPS Traffic:\nClose all Chrome instances Start Chrome with custom param to log certificates to ssh-key.log Reference ssh-key.log in Preferences \u0026gt; Protocols \u0026gt; TLS \u0026gt; (Pre)-Master-Secret log Common Display Filter Examples","title":"Wireshark: Customization, HTTPS Traffic Decryption and Common Display Filters","type":"posts"},{"content":"","date":"13 April 2023","externalUrl":null,"permalink":"/tags/api-request/","section":"Tags","summary":"","title":"api-request","type":"tags"},{"content":"","date":"13 April 2023","externalUrl":null,"permalink":"/tags/authorization/","section":"Tags","summary":"","title":"authorization","type":"tags"},{"content":" To create a signed AWS API request, follow the detailed guide provided in Create a signed AWS API request.\nTo generate a signed request in Python, you can use the following script created by ChatGPT:\n","date":"13 April 2023","externalUrl":null,"permalink":"/posts/aws-sign-v4/","section":"Posts","summary":"To create a signed AWS API request, follow the detailed guide provided in Create a signed AWS API request.\nTo generate a signed request in Python, you can use the following script created by ChatGPT:","title":"Creating a Signed AWS API Request: A Handy Guide","type":"posts"},{"content":"","date":"13 April 2023","externalUrl":null,"permalink":"/tags/python/","section":"Tags","summary":"","title":"python","type":"tags"},{"content":"","date":"6 April 2023","externalUrl":null,"permalink":"/tags/oauth2/","section":"Tags","summary":"","title":"oauth2","type":"tags"},{"content":"","date":"6 April 2023","externalUrl":null,"permalink":"/tags/oidc/","section":"Tags","summary":"","title":"oidc","type":"tags"},{"content":" Create a Certificate Authority (CA)\nIssue certificates for authorized users\nDefine CA public key and principals name in ~/.ssh/authorized_keys instead of users\u0026rsquo; public keys\nUse key (~/.ssh/id_rsa) with CA-issued certificate (~/.ssh/id_rsa-cert.pub) to SSH into server\nPer-account SSH certificate setup under ~/; can also be configured at system level (/etc/ssh/sshd_config)\nAdvantages:\nKey rotation simplified; admin issues new certificate for new key, no need to update public keys on server You can follow the full procedure for using a CA with SSH at https://www.lorier.net/docs/ssh-ca.html or Creating SSH CA Certificate Signing Keys.\n","date":"6 April 2023","externalUrl":null,"permalink":"/posts/ssh-certificate/","section":"Posts","summary":"Create a Certificate Authority (CA)\nIssue certificates for authorized users\nDefine CA public key and principals name in ~/.ssh/authorized_keys instead of users\u0026rsquo; public keys\nUse key (~/.ssh/id_rsa) with CA-issued certificate (~/.","title":"Secure and Simplified SSH Key Management with Certificate Authority (CA)","type":"posts"},{"content":" Difference between OAuth2.0, OIDC and SAML2 # OAuth 2.0 provides authorization using ID token. OIDC provides authentication using access token. SAML2 provides both authentication and authorization. OAuth 2.0 # allows users to log in, agree to the OAuth permission grant, and generate an access token (like an API key). Access tokens have permission to access the API (resource server), but they are not related to the user\u0026rsquo;s identity. Access tokens can be renewed using a refresh token. ID tokens can show who the user is. For example, when logging into a website with Google, Google will generate an access token for the user, which the website can use to call APIs to get the user\u0026rsquo;s email and name. The browser cookie will link the user with the access and refresh token. Amazon Cognito # User pools can integrate with ALB and API Gateway. Identity pools can manage access control on AWS resources such as S3 and DynamoDB. User mapping can associate IAM roles with specific users or groups. By default, user mapping maps into authenticated and unauthenticated IAM roles. Rules can be configured to assign specific IAM roles based on conditions (e.g., specific attributes or group membership). With an ID token obtained through Amazon Cognito, temporary AWS access keys and secrets can be obtained using aws get-credentials-for-identity. These AWS credentials can be used to access AWS resources. Authentication Process # Clicking the login button and redirecting to the authorize URL. Entering login credentials on the Cognito login page. Redirecting to a specified redirect URL upon successful authentication. Making additional requests to obtain access tokens and refresh tokens depending on the grant type. Using the authorized code obtained from the initial request to obtain access tokens and refresh tokens if the grant type is authorization_code. Including the response_type=token parameter in the authorization request if the grant type is implicit, which returns the access token and refresh token directly in the response without additional requests to the token URL. Authenticated Role Selection # Amazon Cognito user pools come with two default roles: Unauthenticated and Authenticated. Role selection allows for different roles to be assigned to authenticated users. \u0026ldquo;Choose role with rules\u0026rdquo; allows for rules to be set based on the JWT. The following attributes can be referenced in the JWT: phone, email, username, default attributes, and custom attributes. To reference the default and custom attributes in the IAM policy, the authorize scope needs to include \u0026ldquo;profile\u0026rdquo;. Users are allowed to be assigned to groups, and each group can be assigned multiple IAM roles. cognito:roles and cognito:preferred_role will be added to the JWT token when \u0026ldquo;Choose role from token\u0026rdquo; is used to grant permission to user. cognito:groups can also be found in the JWT if the user is in a group. Lab1: Cognito Identity Pool # What I Need # Cognito User and Identity Pool IAM roles for authenticated and unauthenticated user Both roles grant PutObject right to the S3 bucket S3 bucket Upload Object to S3 Bucket with Cognito # With AWS CLI # https://gist.github.com/hugotkk/bf3daf2148d9bc82303f62cb360e6401\nGet ID token from Cognito Get AWS credentials from Cognito API Use the credentials to upload, download, or presign an S3 object With Cognito and AWS JavaScript SDK on Browser # Similar to AWS CLI, but on a web page Need additional CORS policy to allow object update from the webpage The Python script will start a simple HTTP server and serve the file https://gist.github.com/hugotkk/21385eb08a366b587f1f434eff43f381\nLab2: Authenticated role selection # Create a new user in the Cognito user pool In the IAM roles for authenticated and unauthenticated users, remove S3 permissions Create a new IAM role with S3 permissions For \u0026ldquo;Choose role with rules\u0026rdquo; testing: Assign the attribute profile: admin to the first user Create a new rule that checks if the claim has profile and is equal to admin, and assigns the new IAM role For \u0026ldquo;Choose role from token\u0026rdquo; testing: Create a new group and add the first user to the group Attach the new IAM role to the group Test by uploading an object using login.html (code provided in previous example) Expected result: The first user should be able to update objects in S3, while the second user should not be able to. ","date":"6 April 2023","externalUrl":null,"permalink":"/posts/aws-cognito/","section":"Posts","summary":"Difference between OAuth2.0, OIDC and SAML2 # OAuth 2.0 provides authorization using ID token. OIDC provides authentication using access token. SAML2 provides both authentication and authorization. OAuth 2.0 # allows users to log in, agree to the OAuth permission grant, and generate an access token (like an API key).","title":"Understanding OAuth 2.0: Explore with Amazon Cognito","type":"posts"},{"content":" DC Cannot Replicate # Problem: Target principal name incorrect during AD replication Solution (Demote and Promote AD): Demote the DC at \u0026ldquo;Remove Roles and Features\u0026rdquo; Manually clean update the metadata: Remove the DC record at AD Users and Computers Remove the DC-related record at AD Sites and Services \u0026amp; in DNS Promote the DC back Result: Replication resumes, syncing with other DCs For \u0026ldquo;PRC server not operating\u0026rdquo; error, check DNS issues with dcdiag and try dns scavenging PDC Gone, Operation Master Role not Transferred # Solution: Forceful Takeover Use Move-ADDirectoryServerOperationMasterRole to move the Operation Master. Removing an Orphaned Domain # Problem: Trusted domain removed without demoting DC, resulting in ghost domain/DC Solution: Use ntdsutil metadata cleanup for both domain controller and domain Delete domain controller with How to remove a domain controller that no longer exists? guide Delete domain with remove orphan domain guide ","date":"5 April 2023","externalUrl":null,"permalink":"/posts/windows-ad-issues/","section":"Posts","summary":"DC Cannot Replicate # Problem: Target principal name incorrect during AD replication Solution (Demote and Promote AD): Demote the DC at \u0026ldquo;Remove Roles and Features\u0026rdquo; Manually clean update the metadata: Remove the DC record at AD Users and Computers Remove the DC-related record at AD Sites and Services \u0026amp; in DNS Promote the DC back Result: Replication resumes, syncing with other DCs For \u0026ldquo;PRC server not operating\u0026rdquo; error, check DNS issues with dcdiag and try dns scavenging PDC Gone, Operation Master Role not Transferred # Solution: Forceful Takeover Use Move-ADDirectoryServerOperationMasterRole to move the Operation Master.","title":"Active Directory Issues and Solutions","type":"posts"},{"content":"","date":"31 March 2023","externalUrl":null,"permalink":"/tags/ai/","section":"Tags","summary":"","title":"ai","type":"tags"},{"content":"","date":"31 March 2023","externalUrl":null,"permalink":"/tags/chatgpt/","section":"Tags","summary":"","title":"chatgpt","type":"tags"},{"content":" My Use Cases # Summarizing Video or Article Content Use https://downsub.com for video subtitles and https://chatgptsplitter.com/ for text Ask GPT-4 to summarize content in bullet points and elaborate on each point in detail For GPT-3.5, the chunk size is 1500. For GPT-4.0, it is 15000 Preparing Open-Ended Questions and Lab Exercises for Revision Provide notes to GPT-4 Request generation of open-ended questions and lab exercises for revision and deeper understanding Memorization Techniques Using Visual Mnemonics Ask GPT-4 for visual memory techniques suggestions to remember specific phrases or names Rewriting Emails or Paragraphs Use GPT-4 to rewrite and improve structure, clarity, and tone of emails or paragraphs SEO Suggestions for Meta Description, Title, and Tags Request GPT-4\u0026rsquo;s assistance in generating optimized meta descriptions, titles, and tags for improved search engine visibility Learning Code Provide code to GPT-4 Request an explanation Requesting Code Templates Tips for Using GPT-4 Effectively # If the answer is not relevant:\nRegenerate the result Edit the prompt for specificity or clarity Start a new chat to reset context Best practices # Best practices for prompt engineering with OpenAI API Useful links # Sam Altman: OpenAI CEO on GPT-4, ChatGPT, and the Future of AI - Known Issues # Unreliable answers when out of token window or lacking data Verification of results needed Inability to reply in markdown format We can ask GPT to output in code block format for copying. ","date":"31 March 2023","externalUrl":null,"permalink":"/posts/chatgpt/","section":"Posts","summary":"My Use Cases # Summarizing Video or Article Content Use https://downsub.com for video subtitles and https://chatgptsplitter.com/ for text Ask GPT-4 to summarize content in bullet points and elaborate on each point in detail For GPT-3.","title":"Maximizing Productivity and Learning with GPT-4: Use Cases and Best Practices","type":"posts"},{"content":"","date":"31 March 2023","externalUrl":null,"permalink":"/tags/openai/","section":"Tags","summary":"","title":"openai","type":"tags"},{"content":"","date":"31 March 2023","externalUrl":null,"permalink":"/tags/prompt/","section":"Tags","summary":"","title":"prompt","type":"tags"},{"content":" Concepts # Block Public Access # Prevent any public access in ACL or bucket policy. ACL: If you block public access in ACL, any public access granted to everyone will be ignored. Bucket Policy: If you block public access in bucket policy, any policy that allows S3 operations by * will be ignored. eg: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;PublicReadGetObject\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:GetObject\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::Bucket-Name/*\u0026#34; ] } ] } Object Ownership # Ensure that all objects are owned by the bucket owner and disable ACLs. Choose between disabling ACL enabling ACL while keeping ownership the same as the bucket owner by default but it can be overrided by the object writer enabling ACL while keeping ownership the same as the object writer Static Website Hosting # Set a default index page and error page. Provide a URL for access. (not same as the s3 URL) Access Control # Use ACL to control access rights on an object level. Note that enabling ACL may cause varied object ownership in shared buckets. Use bucket policy to control access at the bucket level. Keep in mind that bucket policies only apply to objects owned by the bucket owner. Access Points # Simplify policy management by attaching a policy to an access point. Use Access point policy to override existing bucket policies for granular control. Use S3 Access Points to reduce the risk of policy errors or inconsistencies. Steps (How to setup a public accessible S3 bucket) # Configure Static Website Hosting in Properties Disable Object Ownership in Permissions Enable public access With ACL Disable block public access of ACL in the Permissions. Go to Objects, select objects, then click \u0026ldquo;make public using ACL\u0026rdquo; (This can be done by AWS CLI as well). Or via Bucket Policy Disable block public access of bucket policy in the Permissions. Create a bucket policy that allows everyone to get objects. Integrate with CloudFront # Instead of using static website hosting or making S3 public, we can use CloudFront to host a static web host with S3.\nHere\u0026rsquo;s how:\nCreate a distribution and S3 origin. To define the index page, set the root object in the distribution level for requesting /. We can use a private S3 bucket and use Origin Access Control to grant permission for CloudFront to access the bucket. OAI (Origin Access Identity) is now considered legacy, and it\u0026rsquo;s recommended to use Origin Access Control instead. We can control policies for caching and manage headers, cookies, and query strings: Viewer Viewer + CloudFront Viewer + Exclude Specific (filter from Viewer + CloudFront) Add or remove headers, CORS, and security headers. Add a whitelist or blacklist Geo Restriction to restrict access based on geography. ","date":"29 March 2023","externalUrl":null,"permalink":"/posts/aws-s3-hosting/","section":"Posts","summary":"Concepts # Block Public Access # Prevent any public access in ACL or bucket policy. ACL: If you block public access in ACL, any public access granted to everyone will be ignored.","title":"How to host a static website on S3","type":"posts"},{"content":" Identity Management # CyberArk AWS SSM SSH Certificate SSO # Kerberos ADFS 2FA # Google Authentificator Vault # Hashicorp Vault AWS Secret Manager Vulnerability Scanner # OpenVAS Web Security Scanner # Acunetix OWASP ZAP ","date":"29 March 2023","externalUrl":null,"permalink":"/posts/security-tools/","section":"Posts","summary":" Identity Management # CyberArk AWS SSM SSH Certificate SSO # Kerberos ADFS 2FA # Google Authentificator Vault # Hashicorp Vault AWS Secret Manager Vulnerability Scanner # OpenVAS Web Security Scanner # Acunetix OWASP ZAP ","title":"Identity Management, Vault, and Security Scanner Tools for Cybersecurity","type":"posts"},{"content":"","date":"29 March 2023","externalUrl":null,"permalink":"/tags/s3/","section":"Tags","summary":"","title":"s3","type":"tags"},{"content":"","date":"15 December 2022","externalUrl":null,"permalink":"/tags/calculator/","section":"Tags","summary":"","title":"calculator","type":"tags"},{"content":"","date":"15 December 2022","externalUrl":null,"permalink":"/tags/converter/","section":"Tags","summary":"","title":"converter","type":"tags"},{"content":"","date":"15 December 2022","externalUrl":null,"permalink":"/tags/grafana/","section":"Tags","summary":"","title":"grafana","type":"tags"},{"content":"","date":"15 December 2022","externalUrl":null,"permalink":"/tags/ip/","section":"Tags","summary":"","title":"ip","type":"tags"},{"content":"","date":"15 December 2022","externalUrl":null,"permalink":"/tags/mx/","section":"Tags","summary":"","title":"mx","type":"tags"},{"content":" Installation # Prometheus \u0026amp; the related components (eg: pushgateway, blackbox_exporter, alertmanager) - scrape engine - it\u0026rsquo;s better to look at the README.md and Dockerfile on their github repo to get the information about the docker setup\nGrafana - for visualization; drawing graphs\nNode Exporter (via package manager) - Centos7 (need epel repo); Ubuntu20.04 - monitor VM\u0026rsquo;s states\ncAdvisor - monitor docker host\u0026rsquo;s states\nOthers exporters\nkube-prometheus-stack (eks) - included grafana and prometheus\nReferences # docker-compose.yml for prometheus and node exporter grafana.ini prometheus.yml alertmanager.yml web-config.yml - tls and basic auth settings - pass --web.config.file when starting the binary. This works on any prometheus related products eg: prometheus, node_exporter, alertmanager prometheus query functions Grafana Dashboards # cadvisor node-exporter Prometheus # Customise the storage config Setup an alert Setup federation - a primary prometheus will work as an aggregator to collect metrics from other prometheus instances. This can reduce the loading on a single prometheus server. Setup recording rules Setup node-exporter with push gatewaty - but this isn\u0026rsquo;t recommended. The reasons are in here Snapshot (backup) EC2 host discovery - use for dynamically adding the ec2 instances as the scrape targets Thanos # To archive the HA, third-party solutions like thanos will be needed.\nThese two articles are very good introduction of the concept of thanos\nDeep Dive into Thanos-Part I Deep Dive into Thanos-Part II Also, there is a hand-on lab provided by killercoda which is recommended by thanos\nThis shows the architecture of thanos.\nThe thanos gateway works as a proxy on each prometheus instance (sidecar) to communicate to other thanos components. Also, data can be optionally saved to object stores (like aws s3 / mino) for long-term storage as well.\nQueries will add an additional layer to the system. Instead of obtaining the data directly from prometheus, the client (grafana / end user) will now talk to the thanos querier instead. The queries will\ndeduplicate the data they get from the replicas combine the data from the long term object store and shards Thus, with the queries, prometheus instances can logically group into different shards / replicas.\nGrafana # Provision dashboards and Data sources But we can\u0026rsquo;t provision the users, orgs, alerts Add prometheus to grafana Import or export a dashboard Backup grafana Create repeated row / panel Instead of creating a field overriding, we can choose the series color quickly by just clicking the color bar next to the series HA in Grafana - setup database as a backend to share the persistent data ","date":"15 December 2022","externalUrl":null,"permalink":"/posts/notes-about-prometheus-grafana/","section":"Posts","summary":"Installation # Prometheus \u0026amp; the related components (eg: pushgateway, blackbox_exporter, alertmanager) - scrape engine - it\u0026rsquo;s better to look at the README.md and Dockerfile on their github repo to get the information about the docker setup","title":"Notes about Prometheus \u0026 Grafana","type":"posts"},{"content":"","date":"15 December 2022","externalUrl":null,"permalink":"/tags/nslookup/","section":"Tags","summary":"","title":"nslookup","type":"tags"},{"content":"","date":"15 December 2022","externalUrl":null,"permalink":"/tags/web/","section":"Tags","summary":"","title":"web","type":"tags"},{"content":" Checker # nslookup MX Toolbox - mx, spx, dkim, dmac record, blacklist What Is My Ip Address - ip lookup who.is - domain lookup SSL Labs - check ssl setting Calculator # Kubernetes Instance Calculator EC2 Price Converter # Convert 500 Megabytes to Bytes Epoch \u0026amp; Unix Timestamp Conversion Tools IOPS, MB/s, GB/day Converter JSON to Go - Convert JSON to golang struct HKT to GMT watt to kwh jwk generator jwk encoder / decoder ","date":"15 December 2022","externalUrl":null,"permalink":"/posts/web-tools/","section":"Posts","summary":" Checker # nslookup MX Toolbox - mx, spx, dkim, dmac record, blacklist What Is My Ip Address - ip lookup who.is - domain lookup SSL Labs - check ssl setting Calculator # Kubernetes Instance Calculator EC2 Price Converter # Convert 500 Megabytes to Bytes Epoch \u0026amp; Unix Timestamp Conversion Tools IOPS, MB/s, GB/day Converter JSON to Go - Convert JSON to golang struct HKT to GMT watt to kwh jwk generator jwk encoder / decoder ","title":"Web Tools","type":"posts"},{"content":"","date":"14 December 2022","externalUrl":null,"permalink":"/tags/blockchain/","section":"Tags","summary":"","title":"blockchain","type":"tags"},{"content":"","date":"14 December 2022","externalUrl":null,"permalink":"/tags/nft/","section":"Tags","summary":"","title":"nft","type":"tags"},{"content":" Useful links # Explorer # solana explorer NFT marketplace # solanart solasea Good articles to understand the concept of solana # Solana NFT 101 Solana’s Token Program, Explained Understanding Solana’s Mint Accounts and Token Accounts How NFTs are represented in Solana In the Solana world, data will be stored in different accounts. Accounts will be associated with each other by cross-referencing. The Solana program will be used to manipulate those accounts (create, delete, set link). Business logic will be introduced from a separated program.\nCommon tasks eg: creating token / nft can be done with some universal solana programs (spl Token program, metaplex) on solana network. Developers don\u0026rsquo;t need to write their own solana program. This makes things become more standard. People write their own business logic by extending the functions of those universal programs but won\u0026rsquo;t start from zero.\nNotes # solfaucet can claim free SOL (devnet) for development Most of the people use metaplex to create nft Metaplex\u0026rsquo;s Token Metadata Program defined the standard of a solana NFT Metaplex\u0026rsquo;s candy machine provided abilities to sell the nft eg: loot box, start/end date, whitelist sugar is a cli tool to create candy machines. It simplified the task but I got a \u0026ldquo;Request header too large\u0026rdquo; error when uploading a large file (100MB) Metaplex javascript sdk can solve the issue of sugar but their documentation at this moment is a bit old (even on github README). The examples are still in V1 but sugar cli has already moved V2. The syntax of V2 is quite different from V1. Therefore, I have to refer to this sdk doc for development. If \u0026ldquo;animation_url\u0026rdquo; is set to a mp4 / gif, it will playable inside the solana wallet (like phantom). I thought it was related to the \u0026ldquo;properties.files\u0026rdquo; field. \u0026ldquo;properties.files\u0026rdquo; should include all related resources. For examples, if the NFT have a mp4 \u0026ldquo;animation_url\u0026rdquo; and jpg \u0026ldquo;image\u0026rdquo;, \u0026ldquo;properties.files\u0026rdquo; should include both When uploading files to arweave with metaplex sdk, it will not append ?ext=\u0026lt;extension\u0026gt; to the URL. This caused the image and videos to fail to be shown correctly on the phantom wallet. The file extension should always be included like https://www.arweave.net/efgh1234?ext=mp4. this calculator is useful to calculator how much do we need to pay for the persistent storage on arweave spl token program cli is used to create token / nft but it provides the basic functions only. For instance, there is no metadata for the nft token created from the token program. Metaplex is kind of extension which added the metadata and venting machine functions on top of the token program solana cli - will be used to create accounts / transfer / check balances. More use cases are in here. When create a solana account, it will print out 12-words seed phases and save the private key in ~/.config/solana/id.json. Please save the seed phases. The private key can be derived from seed phases but we can\u0026rsquo;t recover the seed phases from private keys. Solana-cli-created accounts can only be imported to the phantom wallet with a private key. The default derivation path of Phantom is different from solana-cli. Changing the derivation path is only supported when importing a hardware wallet to phantom. Therefore, there is no way to import the account from seed phases. To do that, we have to convert uint8array (id.json) to base58. This can be done by library like base58-js web3.js - web3 library of solana but people usually use the anchor\u0026rsquo;s version because most of the solana programs are developed from anchor this shows the public clusters in the solana network. I use quicknode to create a dedicated solana endpoint instead. More examples can be found on Solana Cookbook, Solana Web3 Demo, Solana Development Guide ","date":"14 December 2022","externalUrl":null,"permalink":"/posts/notes-about-solana/","section":"Posts","summary":"Useful links # Explorer # solana explorer NFT marketplace # solanart solasea Good articles to understand the concept of solana # Solana NFT 101 Solana’s Token Program, Explained Understanding Solana’s Mint Accounts and Token Accounts How NFTs are represented in Solana In the Solana world, data will be stored in different accounts.","title":"Notes about Solana","type":"posts"},{"content":"","date":"14 December 2022","externalUrl":null,"permalink":"/tags/solana/","section":"Tags","summary":"","title":"solana","type":"tags"},{"content":" Dockerfile # Most containers are not well-documented, making it hard to find essential information about them on Docker Hub or GitHub\u0026rsquo;s README. Some of the critical information to know includes:\nWhich user does the container run as? Which ports are used by the application? Where are the config files/directory? Where will the persistent data be stored? The best way to find these answers is by examining the Dockerfile. For example:\nPrometheus\nUser: nobody Port: 9090 Config path: /etc/prometheus/prometheus.yml Data path: /prometheus php:8-apache-buster\nPort: 80 PHP config: /usr/local/etc/php/php.ini Apache config: /etc/apache2/conf-available/docker-php.conf Document root: /var/www/html/ Disk Usage # Analyse the disk usage of docker\nRun the command: docker system df -v Reference: https://docs.docker.com/engine/reference/commandline/system_df/ Live Restore # This will keep containers running during the docker upgrade and systemctl stop dockerd\nEdit the file /etc/docker/daemon.json Add the following content to the file: { \u0026#34;live-restore\u0026#34;: true, } Address pool # To override the default address pool, add the following content to the daemon.json file: { \u0026#34;default-address-pools\u0026#34;: [ { \u0026#34;base\u0026#34;: \u0026#34;10.1.0.0/16\u0026#34;, \u0026#34;size\u0026#34;: 24 } ] } Reference: https://forums.docker.com/t/docker-default-address-pool-customization-question/112969 Logging # Docker logging does not rotate by default. To change the logging driver to JSON, you need to modify the Docker daemon configuration file (/etc/docker/daemon.json) and add the following line: { \u0026#34;log-driver\u0026#34;: \u0026#34;json-file\u0026#34; } To set the retention policy for the JSON logging driver, you can add the following configuration options to the same file: { \u0026#34;log-opts\u0026#34;: { \u0026#34;max-size\u0026#34;: \u0026#34;10m\u0026#34;, \u0026#34;max-file\u0026#34;: \u0026#34;3\u0026#34; } } This will retain a maximum of 3 log files, each with a maximum size of 10 MB. After making changes to the configuration file, you need to restart the Docker daemon for the changes to take effect: sudo systemctl restart docker Docker Swarm # Service Management # To reference a service, use tasks.\u0026lt;service-name\u0026gt; instead of a specific replica. Publishing a port will expose it on all Swarm nodes, requiring a load balancer to balance the service. A load balancer distributes incoming requests to nodes running the service, which will handle the request using one of its replicas. Horizontal scaling of the service is achieved by adding additional replicas to handle increased traffic. Volume Management # Volumes/local mount points are independent on each node. When creating a volume/local mount point for a service, it is created on the node where the task is assigned. Changes made to a volume/local mount point on one node will not be reflected on other nodes. To ensure data consistency across nodes, use a distributed file system like GlusterFS or NFS, or a cloud-based storage service like Amazon EFS or Google Cloud Storage. When using a distributed file system or cloud-based storage service, the volume/local mount point is shared across all nodes in the Swarm, ensuring that all nodes have access to the same data. iptables # Docker uses iptables for traffic routing\nBe cautious when restarting iptables service to avoid overriding Docker\u0026rsquo;s rules\nServices like firewalld or ufw may interfere with Docker\u0026rsquo;s iptables rules\nTurning off iptables may have side effects\nRouting in iptables forwards published ports/container subnets to NAT table\nNAT table routes traffic between containers\nDocker host defaults to IPv4 forwarding enabled, making containers public on subnets\nDocker host can be used as router to access container directly\nExample: Limiting access to specific IPs for Node.js container with port 80 and published port 8080 on Docker host\nDropping port 8080 in firewalld won\u0026rsquo;t work; traffic is forwarded to NAT table Dropping container port 80 in Docker\u0026rsquo;s custom chain (DOCKER_USER) will work but it will also block all traffic into the container, not just from outside Docker host Useful links # install docker Dockerfile reference docker-comopse.yml reference /etc/docker/daemon.json reference how to trust the tls certificate of a registry with TLS enabled ","date":"13 December 2022","externalUrl":null,"permalink":"/posts/docker/","section":"Posts","summary":"Dockerfile # Most containers are not well-documented, making it hard to find essential information about them on Docker Hub or GitHub\u0026rsquo;s README. Some of the critical information to know includes:","title":"Docker","type":"posts"},{"content":"","date":"6 September 2022","externalUrl":null,"permalink":"/tags/istio/","section":"Tags","summary":"","title":"istio","type":"tags"},{"content":" Main concept # Injecting a proxy in front of each pods - so we can do traffic management\nTraffic Shifting Fault Injection Circuit Breaking Mirror and apply security features\nmtls between pods security policy or some logging / auditing works\nImagine there is a proxy server in front of each pod.\nVirtualService and DestinationRule are the configure of the proxy server\nActually the offical documentation is good enough. here is a good course to learn about istio.\nIstio Settings # min tls protocol memory and cpu limit on istiod - I use this to reduce the memory limit on istiod. The default profile is 2G memory by default. block un-registry egress by default - See Service Entry dns proxing - See Service Entry enable egress gateway Install Multicluster # Setup Guide\nThe Before you Begin section is important. For Multi-Primary Multicluster, we need to generate a root certificate to issue the certificates for cluster1 and cluster2. See Configure Trust\nMulti-primary: istiod on each cluster Primary-remote: only primary will have istiod - remote will share the istiod with primary On different networks: the pods are in different networks at 2 k8s clusters. EKS support vpc networking, the pods can be in the vpc subnet so the pods in different cluster can be configured on the same network. I have set up 2 k3s clusters on aws ec2. The pods are on different networks. Install VM Workload # istio can bring VM into the service mesh. treat vm as a k8s pod\nWorkload Entry: Pod Workload Group: Deployment Service Entry: Service I follow this guide to set this up\nThere are few options for the installation. I chose\nMulti Network - as the vm and k8s pods are in different networks (For example: vm 192.168.0.1/24 k8s pods 10.0.1.0/24. In AWS, we pods can use the aws vpc network which mean if we have a subnet 192.168.0.1/24 both vm and eks pods can be in same subnet) Automated WorkloadEntry Creation - workload entry will automatically create after joining the mesh dns # Before the installation, we need to setup a dns server first. VM need to know about the k8s cluster (*.cluster.local eg: httpbin.default.svc.cluster.local).\nHere are some notes on setting up dnsmasq on ec2.\n/etc/dnsmasq.conf\nbind-interfaces listen-address=127.0.0.1 server=169.254.169.254 address=/cluster.local/\u0026lt;k8s_cluster_ip\u0026gt;/ bind-interfaces + listen-address make the dns server work. bind-interfaces will bind port 53 on all interfaces but listen-address will restrict it on specific interface need to forward queries to aws\u0026rsquo;s dns (169.254.169.254) /etc/resolv.conf\nnameserver 127.0.0.1 update resolv.conf to tell the vm to use the dnsmasq\nFor ubuntu, the resolv.conf will be managed by systemd-resolved so it is better to disable it\nsystemctl disable systemd-resolved systemctl stop systemd-resolved system-resolved will create a symbolic link to /etc/resolv.conf. When systemd-resolved is disable, the resolv.conf may disappear after the server restart. So be careful of that.\nInstall with revision # If istio is installed with the canary method, the istiod service name will be different.\nwithout revision: istiod.istio-system.svc.cluster.local with revision: istiod-\u0026lt;revision\u0026gt;.istio-system.svc.cluster.local The templates at Expose services inside the cluster via the east-west gateway are using istiod.istio-system.svc so we have to change it manually\nAlso we have to add --revision when deploy the east west gateway\nService vs ServiceEntry # the vm can be selected with Service - ServiceEntry is not necessary If we want to use ServiceEntry, DNS proxy need to be enabled Upgrade # In-placed # run istioctl upgrade with the updated istioctl binary\nCanary # For example, we want to upgrade istio from 1.13.0 to 1.14.3.\nWe can\ninstall new version (let 2 versions coexist) switch to new version uninstall the old version Firstly, istio has to be installed with --revision\nistioctl operator init --revision 1-13-0 and apply IstioOperator config with specific revision\nspec: ... revision: 1-13-0 To enable auto injection in namespace (example is default), instead of\nkubectl label ns default istio-injection=enabled we have to specify the revision\nkubectl label ns default istio.io/rev=1-13-0 To upgrade istio:\nDownload 1.14.3 istioctl binary init 1.14.3 operator istioctl operator init --revision 1-14-3 apply IstiOperator config with revision: 1-14-3 Update injection label - kubectl label ns default istio.io/rev=1-14-3 --overwrite Uninstall the old version - istioctl x uninstall --revision 1-13-0 Sidecar injection # cannot enable globally - not like mTLS - need to be apply per namespace / per pod hostNetwork: true in pod will stop the injection injection can be override in pod level Service Entry # This is confusing me so much.\nWhy the hostname resolution does not work? # My expectation: like k8s\u0026rsquo; service but it\u0026rsquo;s for external services - so we are adding an A record to the k8s DNS\nFor example:\napiVersion: networking.istio.io/v1alpha3 kind: ServiceEntry metadata: name: mysql spec: hosts: - db.hhuge9.com location: MESH_EXTERNAL endpoints: - XXXXX Expected that I can connect to my own mysql service with mysql -h db.hhuge9.com at any pod in the mesh after apply this config\nBut the result is - db.hhuge9.com cannot be resolved\nThe reason is - ServiceEntry will not do anything on the dns unless we enable the DNS proxying feature\nWhen we apply this ServiceEntry\napiVersion: networking.istio.io/v1alpha3 kind: ServiceEntry metadata: name: google spec: hosts: - google.hhuge9.com location: MESH_EXTERNAL resolution: DNS endpoints: - 142.250.200.14 with enabling dns proxying:\nISTIO_META_DNS_CAPTURE: true - will add entry to k8s dns. google.hhuge9.com will be resolved with the endpoints ips (142.250.200.14) ISTIO_META_DNS_AUTO_ALLOCATE: true - will allocate an internal ip (like service in k8s). google.hhuge9.com will be resolve with that ips We can leave ISTIO_META_DNS_AUTO_ALLOCATE: false and assign an internal ip at the addresses field\n... hosts: - google.hhuge9.com addresses: - 192.168.0.1 ... endpoints: - 142.250.200.14 Therefore nslookup google.hhuge9.com will return 192.168.0.1\nBut for the host name resolution, we need ISTIO_META_DNS_CAPTURE: true\nIs it just a canonical name / A record? # I doubted why ServiceEntry is needed. We can access the service directly without applying a config, right? It looks so useless\nAcutualy, It becomes useful when we want to control the egress traffic When the REGISTRY_ONLY policy is enabled, only registered services can be accessed within the mesh. Therefore, a service entry of facebook.com will be needed if we want to access facebook inside the pod\nAuthorizationPolicy # Useful examples can be found on Concept \u0026gt; Security \u0026gt; Authorization policies and Reference \u0026gt; Security \u0026gt; AuthorizationPolicy Best practise is applying allow-nothing first then apply our policies allow-all and deny-all are useful in debug apiVersion: security.istio.io/v1beta1 kind: AuthorizationPolicy metadata: name: httpbin namespace: foo spec: selector: matchLabels: app: httpbin version: v1 action: ALLOW rules: - from: - source: principals: [\u0026#34;cluster.local/ns/default/sa/sleep\u0026#34;] - source: namespaces: [\u0026#34;dev\u0026#34;] to: - operation: methods: [\u0026#34;GET\u0026#34;] when: - key: request.header[foo] values: [\u0026#34;bar\u0026#34;] This policy means - allow access to the pod in namespace foo with label httpbin and v1 with the existence of\nGET request and foo: bar header and the source is from namespace dev or with principals cluster.local/ns/default/sa/sleep principal: cluster.local/ns/default/sa/sleep means sleep.default.svc.cluster.local. It is the service account name when the mtls is enabled\nThe account name can be confirmed by printing out the environment variables with env in the pod\nIngress Gateway # http https - tls termination on proxy https - tls termination on pod (tls passthrough) Egress Gateway # http https initial the request with http but connect to the target with https Traffic Management # Most of the examples can be found on here\nFault Injection - like: return 500 / add delay to the requests Traffic Shifting - eg: 50% to v1; 50% to v2 - good for canary / blue green deployment / AB testing Circuit Breaking - exclude the pods if face consecutive 5XX error / set max connections Mirror - mirror the traffic. good for logging / testing / monitoring ","date":"6 September 2022","externalUrl":null,"permalink":"/posts/exam-istio/","section":"Posts","summary":"Main concept # Injecting a proxy in front of each pods - so we can do traffic management\nTraffic Shifting Fault Injection Circuit Breaking Mirror and apply security features","title":"Istio Note","type":"posts"},{"content":"","date":"6 September 2022","externalUrl":null,"permalink":"/tags/service-mesh/","section":"Tags","summary":"","title":"service-mesh","type":"tags"},{"content":" Notes # installation - curl -sSL https://bit.ly/2ysbOFE | bash -s \u0026ndash; 2.2.7 1.5.3 to start a basic test network (two org, 1 application channel) network.sh up createChannel -c mychannel -s couchdb without fabric-ca network.sh up createChannel -ca -c mychannel -s couchdb with fabric-ca organizations/cryptogen/crypto-config-org3.yaml + cryptogen will be used to bootstrap all required crypto stuff like certificates / tls / user identity for you - this is what I call \u0026ldquo;without fabric-ca\u0026rdquo; in the with fabric-ca way, you have to cretae a fabric-ca-server and register the peer, generate the user identity and certificates manually with fabric-ca-client configtx/configtx.yaml + configtxgen will be used to generate the genesis block, channel transaction and org definition for you peer environment variables setup - useful when performing channel update and chaincode installation / execution peer0.org1.example.com peer0.org2.example.com peer1.org3.example.com tls setup for orderer, peer and fabric-ca configtx.yaml reference Leader, Anchor (Gossip) # leader peer it will always receive the block from orderer it will then send the block to others peers it can be set to keep the peer always up-to-date it can be auto-elected or config statically anchor peer The network uses the GOSSIP mechanism to know peers from other org with anchor peer. For example: org1 peer1 has to communicate with org2 peer1. it can know org2 peer1 from org1 anchor peer. an anchor peer can talk with another anchor peer from different org to enable this in peer, define CORE_PEER_GOSSIP_EXTERNALENDPOINT to expose peer to the GOSSIP channel create channels # orderer system channel use configtxgen -outputBlock to generate genesis block config ORDERER_GENERAL_GENESISFILE with genesis.block in orderer application channel use configtxgen -outputCreateChannelTx to create transaction create block with peer channel create add peer to the channel with peer channel join -b add org3 # update organizations/cryptogen/crypto-config-org3.yaml\ngenerate certificates for the org3 peer\nupdate configtx/config.yaml with org3 definition\nprint out the definition\nupdate channel config\nfetch channel config make a change request for adding new org definition to the channel config sign the change request by org commit the change bring up new org\u0026rsquo;s peer\nadd peer # same as add org but no need to update channel config add orderer # same as add org but need to update both orderer and application channel config (update the Addresses and Consensers list) bring up the new orderer with the latest orderer config block (don\u0026rsquo;t use the original genesis.block) service discovery # configure external endpoints add anchor peers to channel config setup persistent config query peers query channel config query endorsers when the chaincode wasn\u0026rsquo;t installed on the peer. it will return an error: chaincode definition wasn\u0026rsquo;t found. chaincode # use peer lifecycle chaincode package to package the chaincode source code. no need to call npm install / go mod vendor before packaging because the package command will exclude the dependencies folders (vendors / node_modules) for you use peer lifecycle chaincode install to install the chaincode. a chaincode container will be started in each installed peers To commit a chaincode, suggests to follow the Using Private Data in Fabric tutorial\npeer lifecycle chaincode approveformyorg peer lifecycle chaincode commit It is a good example as it show how to add the extra params to the chaincode definition\nFor example,\n--signature-policy: define which peers need to endorse during the invoke --collections-config: config the private data definition --init-required: need to add -I when first time invoke the chaincode To check which org has approved the chaincode, use\npeer lifecycle chaincode checkcommitreadiness Notice that those extra params have to be kept in the whole commit process.\nFor example if you set --init-required in approveformyorg. you have to keep this in approveformyorg for other org, commit and checkcommitreadiness.\nSome information need to know to commit a new version of chaincode:\ncc_package_id - peer lifecycle chaincode queryinstalled channel_name - peer channel list chaincode version and sequence - peer lifecycle chaincode querycommitted or peer lifecycle chaincode queryapproved However, there is no way to know what extra parameters are used in approveformyorg by other orgs.\nIt is hard to approve a chaincode without knowing the extra params being used in other org.\nTo execute an approved chaincode,\nuse peer chaincode invoke --waitForEvent and peerAddresses are needed in peer chaincode invoke --transient can be used to pass an object to the chaincode --tls means use tls to connect to the orderer but not peers if tls is enabled in peers, you must connect peer with tls by export CORE_PEER_TLS_ENABLED=true To define which / how many org are required in peer lifecycle chaincode commit\nchange \u0026ldquo;LifecycleEndorsement\u0026rdquo; policy in channel config To define which / how many org are required to endorse in peer chaincode invoke\nchange \u0026ldquo;Endorsements\u0026rdquo; policy in channel config use --signature-policy to approve and commit the chaincode. this will override the channel config fabric-ca-client # fabric-ca-client will look at the fabric-ca-client-config.yml and msp (user identity) at the $FABRIC_CA_CLIENT_HOME folder.\nby setting up this environment variable, you could run commands with that user identity directly without enrollment\nfabric-ca-client enroll will generate the fabric-ca-client-config.yml and msp (user identity) at the $FABRIC_CA_CLIENT_HOME folder.\nyou have to specify -M when enrolling a user so you won\u0026rsquo;t override the user identity config.\neverytime when you call fabric-ca-client enroll. a new pair of cert and key will be generated in the $FABRIC_CA_CLIENT_HOME/msp folder\nUsing Private Data in Fabric has a good example to show how to create a user in fabric-ca\nlogin as org1.example.com admin ($FABRIC_CA_CLIENT_HOME/msp) export FABRIC_CA_CLIENT_HOME=${PWD}/organizations/peerOrganizations/org1.example.com/ create new user owner fabric-ca-client register --caname ca-org1 --id.name owner --id.secret ownerpw --id.type client --tls.certfiles \u0026#34;${PWD}/organizations/fabric-ca/org1/tls-cert.pem\u0026#34; generate owner\u0026rsquo;s certificate in ${PWD}/organizations/peerOrganizations/org1.example.com/users/owner@org1.example.com/msp fabric-ca-client enroll -u https://owner:ownerpw@localhost:7054 --caname ca-org1 -M \u0026#34;${PWD}/organizations/peerOrganizations/org1.example.com/users/owner@org1.example.com/msp\u0026#34; --tls.certfiles \u0026#34;${PWD}/organizations/fabric-ca/org1/tls-cert.pem\u0026#34; if remove -M, $FABRIC_CA_CLIENT_HOME/msp will be overridden so the user identity will change to owner. If you don\u0026rsquo;t have any backup for the msp folder, you have to enroll in an admin account to re-generate the config again. fabric-ca-client enroll -u https://admin:adminpw@localhost:7054 --caname ca-org1 --tls.certfiles \u0026#34;${PWD}/organizations/fabric-ca/org1/tls-cert.pem\u0026#34; Fabric CA User\u0026rsquo;s Guide recorded many useful examples for daily operations\ncreate a user with type: client - fabric-ca-client register login \u0026amp; generate the certificate - fabric-ca-client enroll list out the certs in ca - fabric-ca-client certificate list set user\u0026rsquo;s affiliation - \u0026ndash;affiliation org2 set user as revoker of peer and client - \u0026ndash;id.attrs \u0026lsquo;\u0026ldquo;hf.Registrar.Roles=peer,client\u0026rdquo;,hf.Revoker=true\u0026rsquo; revoke a user identity - fabric-ca-client revoke -e update the crl - fabric-ca-client gencrl renew the cert - fabric-ca-client reenroll fabric-ca-server # use fabric-ca-server init -b to generate the dummy server config. They will be stored in $FABRIC_CA_HOME\nupdate csr.cn, csr.names, csr.hosts, ca.name, tls.enabled in $FABRIC_CA_HOME/fabric-ca-server-config.yaml\nbefore starting the server, make sure to unset all conflict fabric-ca-server environment variables. They will override the setting in config.yaml. I spent lots of time debugging because of this.\nfabric-ca-server start -b to start the ca server\nenable hsm\nthis page tells how to enable hsm in fabric-ca-server. basically, we need the libsofthsm2.so pin and label to config the bccsp session in fabric-ca-server-config.yaml. then the private keys will be stored in hsm instead of the msp folder however they don\u0026rsquo;t provide any fabric-ca binary with pscs11 enabled to play with fabric-ca-server with hsm. we have to compile softhsm2 and fabric-ca by ourselves enable mysql\nby default, the user identities will be stored in a sqlite file if you plan to create a ca in cluster - build multiple ca servers and use haproxy to load balancer them - you need a mysql server to store the user identities globally (among the cluster) the config can be found in fabric-ca-server-config.yaml db:type you may need to set sql_mode='' in the mysql server to fix the incapability intermediate ca\nstart ca with fabric-ca-server start -b admin:adminpw -u http://\u0026lt;enrollmentID\u0026gt;:\u0026lt;secret\u0026gt;@\u0026lt;parentserver\u0026gt;:\u0026lt;parentport\u0026gt; to enroll a peer with intermediate ca. you need to concat root ca + intermediate certs for the org definition and msp (user identity) Note for exam # remote desktop environment XFCE 4.14 Guacamole 1.4.0 XFCE Terminal Emulator (black background, white font) Ubuntu 20.04 Firefox Browser PSI secure browser interface retake policy - one free retake per Exam purchase Update on Certification Exam Proctoring Migration mentioned a few important things about the exam: no personal bookmarks anymore - it\u0026rsquo;s very stupid\u0026hellip; copying and pasting the yaml in vim will cause incorrect indentation - fix it by :set paste! copy and paste from the terminal will be Copy = CTRL+SHIFT+C, Paste = CTRL+SHIFT+V for Paste - you may need to get used to it External monitor: only 1 active monitor is allowed - If you are using macbook, you will see 2 monitors in \u0026ldquo;About This Macs \u0026gt; Displays\u0026rdquo;. There is no way for me to disable the built-in monitor unless I close the lid. So I can\u0026rsquo;t use the monitor during the exam. ","date":"18 July 2022","externalUrl":null,"permalink":"/posts/exam-hyperledger-fabric/","section":"Posts","summary":"Notes # installation - curl -sSL https://bit.ly/2ysbOFE | bash -s \u0026ndash; 2.2.7 1.5.3 to start a basic test network (two org, 1 application channel) network.sh up createChannel -c mychannel -s couchdb without fabric-ca network.","title":"Hyperledger fabric","type":"posts"},{"content":" api # api gateway throttling limits aws throttling limit (region level) per account per-api per-stage (methods) per-client (usage plan) three type of endpoint edge-optimized (default) - route to nearest cloudfront regional private application discovery service # for migration planning connection type application discovery agent -\u0026gt; install on server. support vm / physical server discovery connector -\u0026gt; install on vCenter (is a ova) Migration Hub import =\u0026gt; import the details directly asg # will automatically tag the instances by default cooldown will start after last instances launched if there are multiple instance scale at the moment athena # performance tunning partition data compression (glue) optimise the file size (aws glue) use columnar (apache orc, parquet with spark or hive on EMR) prevent select * use limit by (guide for columnar) billing # cost allocation tags - tags will show in the cost \u0026amp; usage report budget - create alert if cost exceed the budget setup for cost analysis enable cost allocation tags in billing allow user access the billing cost allocation tags =\u0026gt; tags user-define =\u0026gt; user:XXXX aws generated =\u0026gt; aws:XXXX cost explorer =\u0026gt; ui for search and filtering cost category =\u0026gt; filter in cost explorer (saved filter) cost budget =\u0026gt; billing alarm with foretasted charged + filtering + linked account; billing alert =\u0026gt; amount already be charged billing alert include recurring fee like premium support ec2 instance-hours but exclude one off fee refund forecast cf # access control cf + waf + elb, it should be cf (set custom header) \u0026gt; waf (validate the rule) \u0026gt; alb cf + s3, =\u0026gt; cf with oai \u0026gt; s3 bucket policy cf + alb =\u0026gt; cf with custom header \u0026gt; alb rule reason of cfn with s3 access denied errors s3 block public access must turn off if no oas policy is set - because it will override the permissions that allow public read access if request pays is turn on, the request must include the payer header object cannot be kms encrypted cfn # can use automatic deployment to auto deploy existing stackset to new accounts in organisation cloudhsm # need tcp/3389 for windows and tcp/22 for linux to connect to ec2 to install cloudhsm client; tcp/2223-2225 to communicate with the cluster cloudtrail # best practice to migrate to org trail create org trail in central account create bucket for org (need to set bucket policy to allow member account to write to it) enable cloudtrail feature in org create org trail through cli move old trail data from member accounts to org trail bucket stop cloudtrail in member accounts and remove the old trail buckets codecommit # data protection use macie =\u0026gt; can help protecting data in s3 codedeploy # need to connect to s3 and codedeploy endpoints cw # cw embedded metric format =\u0026gt; can automatrically create metric from log cw endpoints =\u0026gt; monitoring.us-east-2.amazonaws.com monitoring.XXXX no az treats each unique combination of dimensions as a separate metric, even if the metrics have the same metric name data pipeline # components pipeline definition pipeline schedule task runner swf which is specific for data engineering task runner can run on on-premise hosts task runners can be run on any compute resources (ec2 and on-premise servers) use resources in multiple regions only supported in limited region data sync # use for transfer data between on-premise and aws or between aws service support cross-region (s3 \u0026lt;=\u0026gt; s3, efx \u0026lt;=\u0026gt; efx, efs \u0026lt;=\u0026gt; efs) sync source location destination location ddb # support atomic counter local secondary index only can create at table creation eb # can stop start eb environment with lambda at scheduled time doesn\u0026rsquo;t support HTTPS_PROXY ebs # aws only recommend raid0 summary table for different volume types gp2 range: 100-16k iops baseline: base on volume (limited by burst credits) provision: no gp3 range: 3k-16k iops baseline: consistent 3k iops provision: 500iops/gb io2, io1 range: 100-32k iops / 32k-64 iops (only available for nitro system ) provision: io1: 50iops/gb; io2: 500iops/gb io2 block express only support with specific instance (R5b, X2idn, and X2iedn) range: 256k iops provision: 1000iops/gb instance store - temporary block-level storage (physically attach to the host so not an network drive) (support in specific instance type. it is free) the i/o performance are limited by ec2 instance type. although you can use raid0 to increase iops but still have a max for that queue length on ssd: 1/1000iops default block size is 4kb use case of each volume type gp2, gp3 =\u0026gt; boot, dev, test io1, io2 =\u0026gt; db st1 (hdd) =\u0026gt; large sequential workloads like data / log processing (EMR, ETL, data warehouse) sc1 =\u0026gt; save costs ec2 # use cases of dual home separate the traffic by role (frontend, backend) ha (move the eni to other instance) security appliance reason eni is binding to subnet eni - when creating the eni, it inherits the public ipv4 address attribute from subnet efs # HA - regional replication =\u0026gt; notice that it means multi az not multi regions cross region backup create 1st lambda to backup data from efs to s3 in region a; turn on cross-region replication in s3; create 2nd lambda to restore data from s3 \u0026gt; efs in region b data sync backup solution which does not work in cross region data pipeline =\u0026gt; the backup instance cannot mount 2 efs in different regions efs-to-efs =\u0026gt; same as data pipeline solution but implemented by lambda function only aws backup =\u0026gt; does not support cross region backup dns name - file-system-id.efs.aws-region.amazonaws.com (like cw. us-east-2) efs can deliver sub or low single digit millisecond latencies with \u0026gt; 10gbps through and 500k iops launching instance is limited by the number of vcpu running per account per region running elasticache # caching strategies lazy load - set cache when select from db write through - set cache when write to db ttl - write through + lazy load but set an expire date connection endpoints node ep - read and write primary ep for write; reader ep for read (cluster mode disabled) configuration ep for read and write like node ep (cluster mode enable) automatically cache query to elasticache for rds, aurora and redshift (use proxy) support up to 500 nodes and shards elb # classic load balancer only support at most one subnet per az iam # set SAML session tags for access control (add attribute to idp metadata) policy to deny access on specific region - deny all except the global service in console, the instance profile are automatically created along with the iam role with the same name ArnLike is case-sensitive but support wildwards like * and ? group name limit is 128 characters temporary security credentials are valid until they expire mTurk # submit a request to mTurk. outsource manual tasks like taking survey, text recognition, data migration to public opsworks # setup custom recipe to config the application with other aws services information =\u0026gt; solid example for adding redis cluster connection information to rail application org # org features to enable all consolidated billing scp is one of the aws organisation feature default is allow all =\u0026gt; can only use deny list only use allow list =\u0026gt; have to remove FullAWSAccess (the default allow all policy) other # Public Data Sets - data set for public access. more details fileb:// is supported in kms (key) ec2 user data (gzip) s3 (encryption key) govcloud comparson billing and using can be viewed in standard account only us citizen employees can administer the govcloud authentication is isolated from amazon.com network is isolated from other region migrate IBM MQ to Amazon MQ migrate ibm db2 luw to rds (mysql postgresql) iot monitor can check whether the rule has been executed rds # RMAN restore isn\u0026rsquo;t supported for Amazon RDS for Oracle DB instances (RMAN is an backup and store tool for oracle db) route53 # health check must respond with 2xx or 3xx. support tcp and http support DNSSEC s3 # access control to object c (account c) from request user a (account a) and s3 bucket (account b) check the iam role in account a check the bucket policy in account b check the object acl in object owner event notification support object and bucket level but it will resend the notification and sometimes will delay so people use cloudwatch event instead there is a check in trusted advisor for the check open access in s3 bucket but no remediation for that. to fix the bucket permission automatically and use lambda + cloudwatch event cf cannot cache if the request is larger than 30gb. can use range request to chunk the large file into smaller object requester pays don\u0026rsquo;t support 1. anonymous request 2. SOAP request default 100, max 1000 in each account genomics data processing use case sync data to s3 with data sync use s3 for data storage use storage gateway (on-premise access) / fsx (ec2 access) s3 encryption - only support symmetric keys when downloading encrypted s3 object, have to download the encrypted object along with a cipher blob version of the data key. client send the cipher blob to kms to get the plaintext version of data key to decrypt the object reduced redundancy is one of the storage class in s3 but not recommend bylaws - may have a chance to lose the object secret manager # set RotationSchedule to schedule an auto rotation (to run a custom lambda) the rds password snowball # tips to increase performance batch small file multiple copy operations at one time (2 terminals 2 cp command) connect to multiple workstation (1 snowball can connect to multiple workstation) step for using snowball start the snowball setup workstation by download ova image and import to the vmware use cp command (something like aws s3api cp) to copy file to snowball can upload through gui / command line send the device back to aws. they will import your data to s3 take at least 1 weeks sso # permission sets - 1 permission set has multiple iam policies =\u0026gt; associate to user / group sso ad (identity provider) -\u0026gt; aws sso -\u0026gt; application (github, dropbox) / aws accounts sources of identity provider aws sso ad connector aws managed ad external ad (two way trust) server -\u0026gt; client server = adfs create an app config app sign-in and sign-out url client = integrated website trusted idp config idP\u0026rsquo;s sign-in and sign-out url + cert user login with ad\u0026rsquo;s app endpoint =\u0026gt; ad post data to app\u0026rsquo;s sign-in url =\u0026gt; app receive and decrypt the data from ad and give permission to user aws iam federation =\u0026gt; single account only storage gateway # need to download ova and import the vm to create a endpoint to bridge on-premise and aws storage gateway type volume =\u0026gt; mount as a disk (iSCSI) =\u0026gt; s3 =\u0026gt; ebs cached =\u0026gt; save some frequently used data to vm stored =\u0026gt; completely on s3 file =\u0026gt; smb / nfs tape =\u0026gt; tape backup software support # paid support plans allow unlimited number of users to open technical support cases swf # swf vs step function: use step function first. if does not fit =\u0026gt; swf use case: processing large product catalogue using Amazon Mechanical Turk vpc # 5 sg / eni for dx, need to enabled route propagation resource arn supported in dx tenancy vpc will determine the instance tenancy by default. for vpce, need to ensure the private dns option is enabled worksplace # use connection aliases for cross-region workspaces redirection create connection alias share to other account associate with directories in each region setup route53 for failover setup connection string maintenance - support regular maintenance windows (eg 15:00-16:00) or manual maintenance but cannot set something like patching on tue 3:00 workspaces application manager - package manager to help installing software workspaces support Windows 10 desktop but no Windows server ","date":"6 April 2022","externalUrl":null,"permalink":"/posts/exam-aws-sa/","section":"Posts","summary":"api # api gateway throttling limits aws throttling limit (region level) per account per-api per-stage (methods) per-client (usage plan) three type of endpoint edge-optimized (default) - route to nearest cloudfront regional private application discovery service # for migration planning connection type application discovery agent -\u0026gt; install on server.","title":"AWS SA Professional","type":"posts"},{"content":"","date":"6 April 2022","externalUrl":null,"permalink":"/tags/cert/","section":"Tags","summary":"","title":"cert","type":"tags"},{"content":"","date":"6 April 2022","externalUrl":null,"permalink":"/tags/exam/","section":"Tags","summary":"","title":"exam","type":"tags"},{"content":"","date":"6 April 2022","externalUrl":null,"permalink":"/tags/sa/","section":"Tags","summary":"","title":"sa","type":"tags"},{"content":" DevOps choices # Deployment # faster boot time - opsworks slower; ami faster using chef - opsworks need to update config when new node online - opsworks Configure lifecycle event less administrative overhead: eb \u0026gt; cloudformation when both solutions works auto healing: opsworks, codedeploy, eb (bcoz of the asg) rolling: eb, opsworks (not ideal, it is manual deploy), cloudformation+asg+AutoScalingRollingUpdate policy, codedeploy rolling = drop traffic to n instance \u0026gt; deploy \u0026gt; allow traffic in-place = deploy the deploy to all instances (parallel) blue/green deployment: eb (cname swap), codedeploy, 2x(cfn+asg+elb)+route53 or 2(cfn+asg)+elb(weighted target groups) blue/green deployment and want to delay the old asg termination: codedeploy canary deployment: codedeploy only on lambda / ecs, eb (traffic splitting), api gateway eb\u0026rsquo;s immutable deployment: create 2nd asg. deploy code to new asg and create new resources in batch \u0026gt; delete old asg after the deployment. (kind of rolling deployment. not a blue/green, the new resource will accept the traffic during the deployment) a/b test for a long time: (cloudformation+asg+alb)x2+route53 weighted round robin multi app, multi dependencies: use docker: cfn, eb rollback by CloudWatch alarm: codedeploy, eb not work (only by health check) some nodes are not updated after a successful codedeploy deployment: asg create new node during deployment codedeploy sucks in the lifecycle event hook (ec2 ~1hr): script error codedeploy sucks at the AllowTraffic lifecycle event: elb health check failed opsworks sucks at booting: agent doesn\u0026rsquo;t start or incorrect iam role in instance profile all lifecycle event skipped in codedeploy: agent doesn\u0026rsquo;t start / security group blocked the communication limit the resources on cfn launching: use catalog (like a marketplace). more iam control to the cloudformation template. With cloudfromation, you cannot limit user upload what cloudformation template. Backup \u0026amp; Restore # cross region efs backup: lambda: (at region1) use ec2 with efs mount put data to region2 s3 -\u0026gt; (region2) ec2 with efs mount pull data from s3 Cloudformation # work as teams / multi tiers (network, security, application): nested stack / importvalue ASG # troubleshoot asg instance: standby / asg lifecycle hook (1hr only) handle predictable traffic: scheduled scaling prevent scale-in: instance scale-in protection sending log / licence register deregister: asg lifecycle hook Data analysis / Loggings # batch jobs / reporting (like spark): EMR visualise data (like BI): Redshift / Quicksight (cost effective) log searching: elastic search (ES) (renamed to opensearch) query S3 data: athena report time slow: offload the job to other application(like lambda) with kinesis stream / scale-up the cluster with asg apache hive ~= aws glue DB # dynamodb stream = kinesis stream (more advance) throttle on dynamodb stream: limited to 2 connections at the moment. use 1 lambda \u0026gt; sns \u0026gt; other lambda(s) dynamodb with many read: dynamodb accelerator (like redis) multi-region read \u0026amp; write: dynamodb global table multi-region read only: aurora short DR time: read replica -\u0026gt; promote (/aurora global database) long DR time (few hours): lambda to backup and restore conditionalcheckfailedexception at dynamodb: too many write on same record data inconsistency in dynamodb: need to use strong consistent read DB security: auth with iam Config # config aggreation: use stackset to enable config cross accounts \u0026gt; assign a dedicated administrator \u0026gt; auth config aggregator (like peer connection, request at one side and accept at other account) | use org organisation config organisation rule: can use this to put rules to all account in organisation Application Discovery Service # Setting up # Discovery Connector - agentless - install on vmware centre Discovery Agent - agent - install on host CodeCommit # approval rule (pull request) # targets: branch approval pool members (iam user) protect branches # migrate from git # git clone git_repo_url --mirror to create bare repo git push codecommit_repo_url --all git push codecommit_repo_url --tags CodeBuild # find the branch name in codebuild: CODEBUILD_SOURCE_VERSION one codebuild can have one builspec.yml data encryption: at rest and in transit can use aws managed / custom docker image at build environment custom docker image can be chosen from ecr (same/cross account) or custom registry buildspec.yml # many phases but all are inline commmands can use parameter-store and secrets-manager can set \u0026ldquo;finally\u0026rdquo; block in each phase \u0026amp; on-failure behavior CodeDeploy # appspec.yml # resources + hooks (ecs \u0026amp; lambda) files + permission + hooks (ec2) lifecycle # notification # targets: sns / aws chatbot event: any activities (push, merge, delete branch\u0026hellip;) trigger # targets: sns / lambda event: push branches or tags api gateway # can do canary deploy targets # lambda step functions http event sqs kinesis data stream config # trigger type: configuration changes / periodic scope: aws resource \u0026gt; ec2:securitygroup notifications # Settings \u0026gt; Delivery method \u0026gt; sns topic - give you all changes (summary) Settings \u0026gt; Amazon Cloudwatch Events rule - good for watching specific resource config change Events # can send to another account can send to another account in organisation Cloudwatch # create alarm from logs # logs \u0026gt; metric filter \u0026gt; metric \u0026gt; alarm \u0026gt; sns send logs to other place for analysis # logs \u0026gt; subscription filter \u0026gt; lambda (cannot cross account, kinesis can) \u0026gt; s3 \u0026gt; athena logs \u0026gt; subscription filter \u0026gt; kinesis firehose \u0026gt; s3 \u0026gt; athena logs \u0026gt; subscription filter \u0026gt; kinesis stream \u0026gt; kinesis firehose \u0026gt; s3 \u0026gt; athena Kinesis # kinesis stream -\u0026gt; real time data stream (like enhanced version of DYNAMODB Stream) for analysis / aggregation kinesis stream firehose -\u0026gt; for storage (s3) but can do some pre-processing Supported Writer \u0026amp; Reader # aws sdk / agent / library (KPL) \u0026gt; stream \u0026gt; library (client) / firehose / lambda aws sdk / agent / stream / CloudWatch event / CloudWatch logs \u0026gt; firehose \u0026gt; S3 / ES / Redshift / MongoDB / Splunk Security # GuardDuty: threat detection Macie: data level eg: s3 Security Hub: give advises / integrated with different aws products like inspector: cvs scanning / hardending (cis) S3 # Cross account replication # AcctA BuckA AcctB BuckB iam role in acctA trust s3 to assume role give permission to s3 to get buckA object give permission to acctB to replica buckA object give permission to s3 to encrypt \u0026amp; decrypt buckA object bucket policy in acctB allow roleA to replica and put object to buckB Cloudformation # Custom Resource # bind to lambda / sns need to handle the create / update / delete event from the stack ECS # AMI # use ECS-optimised AMI - have container agent installed\nloggings # container log - awslogs - need container agent system log - cloudwatch agent ","date":"6 March 2022","externalUrl":null,"permalink":"/posts/exam-aws-devops/","section":"Posts","summary":"DevOps choices # Deployment # faster boot time - opsworks slower; ami faster using chef - opsworks need to update config when new node online - opsworks Configure lifecycle event less administrative overhead: eb \u0026gt; cloudformation when both solutions works auto healing: opsworks, codedeploy, eb (bcoz of the asg) rolling: eb, opsworks (not ideal, it is manual deploy), cloudformation+asg+AutoScalingRollingUpdate policy, codedeploy rolling = drop traffic to n instance \u0026gt; deploy \u0026gt; allow traffic in-place = deploy the deploy to all instances (parallel) blue/green deployment: eb (cname swap), codedeploy, 2x(cfn+asg+elb)+route53 or 2(cfn+asg)+elb(weighted target groups) blue/green deployment and want to delay the old asg termination: codedeploy canary deployment: codedeploy only on lambda / ecs, eb (traffic splitting), api gateway eb\u0026rsquo;s immutable deployment: create 2nd asg.","title":"AWS Certified DevOps Engineer - Professional","type":"posts"},{"content":"","date":"21 February 2022","externalUrl":null,"permalink":"/tags/aurora/","section":"Tags","summary":"","title":"aurora","type":"tags"},{"content":" RDS Read Replica # cross AZ: OK cross region: OK promote / point-in-time recovery / restore from snapshot will create a new db instance which is not in my expectation. When DR, user need to update dns / db endpoint in application Aurora # Pros over non-aurora rds # read replica auto scaling failover (failover master to read replica in same region) multi-az by default endpoint for writer and reader instance (also custom endpoint) Cons in aurora # Bad integration in cross-region read replica # For example:\nRegion1: master, read-replica Region2: master, read-replica The replication between the multi region db clusters is:\nmaster-slave replication between region1 master and read-replica master-slave replication between region2 master and read-replica master-slave replication between region1 master and region2 master When write record to region1 master, the record will be replicated to\nregion1 read-replica (by region1 master) region2 master (by region1 master) region2 read-replica (by region2 master) When writing a record to region2 master, the record will only be synchronised to region2 read-replica.\nAnd both region1 and region2 masters are writables!!!\nAWS \u0026ldquo;assume\u0026rdquo; people will use the region2 cluster as a readonly database.\nThis is not mentioned in aws doc. I found the answer from aws:repost instead.\nwhen create a cross-region read-replica, it will create another db cluster in the new region you cannot see the cross-region db in the same aws console endpoint cannot cross region, only within the region need to manually set binlog_format in the parameter group to before creating the read replica need to manually set read_only true in the parameter group at second region db cluster to prevent its writer instance from writable by other endpoint writer / reader instance depends on role so the writer endpoint in second region is incorrect and should be not used route53 + latency based routing policy will need to be set up to route both reader endpoints to utilise those read-replicas. but the reader endpoint does not include the master. if you want to utilise the second region master, You need to create a custom endpoint for this So AWS created Aurora Global Database to solve these drawbacks. But it does not support on small db instance type!\naurora serverless # features Pros # scale up quickly (v1: 1min, v2: immediately) scale down quickly (v1: 15min, v2: 1min) save cost - can scale down to 0 when no active traffic MySQL \u0026amp; PostgreSQL compatible no need to manage the cluster Cons # price 30% higher than same power\u0026rsquo;s ec2 instance cannot cross-region no public access ","date":"21 February 2022","externalUrl":null,"permalink":"/posts/aws-aurora/","section":"Posts","summary":"RDS Read Replica # cross AZ: OK cross region: OK promote / point-in-time recovery / restore from snapshot will create a new db instance which is not in my expectation.","title":"Aurora","type":"posts"},{"content":" config aggregator # aggreagate all account under organization # enable service role in organization\nset up iam with\nviewing the organization service role give config.amazonaws.com access for the config resource\nadditional iam right to view accounts in organization\nfrom management account or delegated admin to use this option\naggreagate specfic account # authorization cfn stackset # add stack to stackset = deploy stack delete stack from stackset = delete the stack\norganziation # use managed service mode enable service role in organization change choose account in organization\nother discount # use iam service role\nadministrator role: from ac\ntrust cloudwatch can assume as execute role executor role: to ac\ntrust admintrator ac have access to create resource for example\nRaphael = Cloudformation Wayne = Administrator (trust Cloudformation) Hugo = Executor (trust Adminstrator) that will be\nWayne trust Raphael Hugo trust Wayne Hugo can do the job Wayne can use Hugo Wayne ask Raphael to ask Hugo to do the job = admin ac let cloudformation to assume hugo\u0026rsquo;s right to create resource in hugo account\n","date":"21 February 2022","externalUrl":null,"permalink":"/posts/aws-config/","section":"Posts","summary":"config aggregator # aggreagate all account under organization # enable service role in organization\nset up iam with\nviewing the organization service role give config.amazonaws.com access for the config resource","title":"AWS Config","type":"posts"},{"content":" eb # eb init eb use web_dev eb list deploy code in repo\neb deploy web_dev edit config\neb config web_dev save config\neb config save apply save config\neb config put .elasticbeanstalk/saved_configs/Web-env-sc.cfg.yml Folder Structure\n▾ .ebextensions/ app.config ▾ .elasticbeanstalk/ ▸ saved_configs/ config.yml ▸ .git/ .gitignore index.php README swap name, configuration with not swap\n","date":"21 February 2022","externalUrl":null,"permalink":"/posts/aws-eb/","section":"Posts","summary":"eb # eb init eb use web_dev eb list deploy code in repo\neb deploy web_dev edit config\neb config web_dev save config\neb config save apply save config","title":"AWS Elastic Beanstalk","type":"posts"},{"content":" aws org # o = organization = it9 r = root = root ou = organization unit = dev / prod / uat / management / network ou path: o-3ywbznlomt/r-t9g3/ou-t9g3-knls7nau/\n","date":"21 February 2022","externalUrl":null,"permalink":"/posts/aws-organization/","section":"Posts","summary":"aws org # o = organization = it9 r = root = root ou = organization unit = dev / prod / uat / management / network ou path: o-3ywbznlomt/r-t9g3/ou-t9g3-knls7nau/","title":"AWS Organization","type":"posts"},{"content":"","date":"21 February 2022","externalUrl":null,"permalink":"/tags/config/","section":"Tags","summary":"","title":"config","type":"tags"},{"content":"","date":"21 February 2022","externalUrl":null,"permalink":"/tags/eb/","section":"Tags","summary":"","title":"eb","type":"tags"},{"content":"","date":"21 February 2022","externalUrl":null,"permalink":"/tags/opsworks/","section":"Tags","summary":"","title":"opsworks","type":"tags"},{"content":" OpsWorks # stack = cookbooks layer = frontend, backend, api, rds (how to config to instance) app = source code. can deploy and redeploy Stack # I am exploring this cookbook from aws The nodejs demo cookbook repo does not include its dependencies In the opsworks demo, it uses opsworks-linux-demo-cookbooks-nodejs.tar.gz as the cookbook source We need to convert the repo to the archive before using it Aftering changing the cookbook source, we need to fetch the cache by running commands on instances Inside the opsworks-linux-demo-cookbooks-nodejs.tar.gz, we have\nnodejs_demo recipes default.rb To run this cookbook, we set nodejs_demo::default in the layer\nDocker Support # eb -\u0026gt; multi docker / docker + elb opsworks -\u0026gt; ecs with linux + ec2 Layer # auto-healing = restart the instance when losing connection with opsworks agent\nelb\nsg\nebs\neip\ncloudwatch\ninstance\nrds service layer\nelb service layer\necs cluster layer\nApp # source code env domain ssl db source code are placed at /svr/\u0026lt;app_name\u0026gt;\nInstance Types # 24/7 = normal instance time = start at specific period load-based = asg Behaviors on auto-healing between these two instance type\nebs-backed = re-create the instance instance = stop and start Issue # Instances are stuck in booting # Reason: incorrect instance iam role\nWe can find the error message at\ncat tail -f /var/logs/aws/opsworks/* My instance iam role is incorrect because it should trust ec2.amazonaws.com instead of opsworks.\nand I don\u0026rsquo;t realise that opsworks need 2 iam roles:\nservice role: use by opsworks. need to trust opsworks.amazonaws.com and passrole to ec2.amazonaws.com instance iam role: use by ec2. need to trust ec2.amazonaws.com and passrole to opsworks.amazonaws.com ","date":"21 February 2022","externalUrl":null,"permalink":"/posts/aws-opsworks/","section":"Posts","summary":"OpsWorks # stack = cookbooks layer = frontend, backend, api, rds (how to config to instance) app = source code. can deploy and redeploy Stack # I am exploring this cookbook from aws The nodejs demo cookbook repo does not include its dependencies In the opsworks demo, it uses opsworks-linux-demo-cookbooks-nodejs.","title":"Opsworks","type":"posts"},{"content":"","date":"21 February 2022","externalUrl":null,"permalink":"/tags/orgnaization/","section":"Tags","summary":"","title":"orgnaization","type":"tags"},{"content":" Resources # Hyperledger Fabric Solidity Notes # what is blockchain # The blockchain solution in IoT / Supply chain are changing \u0026ldquo;Mesh\u0026rdquo; network to a \u0026ldquo;Star Network each parties access and write to the \u0026ldquo;blockchain network\u0026rdquo;. not more api integration between parties How to use it # How it work # use cryptography methods to ensure the integrity of a public writable database proof of work, proof of stake, proff How to implement # solidity node.js truffle web3.js Any product (e.g crypto currency/ nft) # Networks # Use Cases Public # btc eth bsc (binance smark contract) Private (have acl) # each parties setup servers to join the blockchain network\nall parties hold the data but they can set access right to the data\nHyperledger Fabric\nDecentralized Storage # IPFS Staking # CrowdSale # user -\u0026gt; sale contract -\u0026gt; nft contract\nLottery # ashisherc/advanced-solidity-lottery-application NFT # Auction (dutch and English)\nmint\nCrowdSale\nERC1155\nLootBox\nopensea\nscv.finance\nLa Collection\nSwap / Farming # ask people input proportion of coin into the pool, like eth:bnb. so people can trade between eth and bnb\nuse Oracles for pricing\u0026hellip;a smart contract which referencing different sources from other swap exchange \u0026amp; their coin pair pool size\npancakeswap(bsc)\nuniswap(eth)\nWhat is ECR20 721 1155 # ECR20 = coin ECR721 = NFT ECR1155 = 1 contract with multi token, eg: game item What is smart contract # Code stored on blockchain Demo # NFT Contract # NFT Example\nContract Wizard\nCommon Functions\nMintable (increase supply) Burnable (reduce supply) Pausable (freeze the contract, for upgrade) Access control: Roles (multi ac, multi actions) / Ownable (single ac, all actions) Upgradeability (Transpart / UUPS) - proxy pattern Timelock Votes NFT metadata # { \u0026#34;boosterId\u0026#34;: 10000000195586, \u0026#34;id\u0026#34;: \u0026#34;10000586756\u0026#34;, \u0026#34;txHash\u0026#34;: \u0026#34;0xff318896b78fd77aadf19f94b7434d1a0ea5ffddfc88b6966d92d75c69f80dd1\u0026#34;, \u0026#34;randomNumber\u0026#34;: \u0026#34;0xc55ec3b26d0bdec23fd2f9e29ae34d54e0d457ef1d413b5c537b0383330575f0\u0026#34;, \u0026#34;image\u0026#34;: \u0026#34;https://assets.polkamon.com/images/Unimons_T08C02H06B04G00.jpg\u0026#34;, \u0026#34;external_url\u0026#34;: \u0026#34;https://polkamon.com/polkamon/T08C02H06B04G00\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Mindful of a million things, the Unisheep are Polkamon creatures that can graze and gander the day away. Creating an environment to actively direct these creatures helps them focus on one particular topic.\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Unisheep\u0026#34;, \u0026#34;initialProbabilities\u0026#34;: { \u0026#34;horn\u0026#34;: 0.2, \u0026#34;color\u0026#34;: 0.345, \u0026#34;background\u0026#34;: 1, \u0026#34;glitter\u0026#34;: 0.99, \u0026#34;type\u0026#34;: 0.1847 }, \u0026#34;attributes\u0026#34;: [ { \u0026#34;trait_type\u0026#34;: \u0026#34;Type\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;Unisheep\u0026#34; }, { \u0026#34;trait_type\u0026#34;: \u0026#34;Horn\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;Spiral Horn\u0026#34; }, { \u0026#34;trait_type\u0026#34;: \u0026#34;Color\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;Green\u0026#34; }, { \u0026#34;trait_type\u0026#34;: \u0026#34;Background\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;Mountain Range\u0026#34; }, { \u0026#34;trait_type\u0026#34;: \u0026#34;Opening Network\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;Binance Smart Chain\u0026#34; }, { \u0026#34;trait_type\u0026#34;: \u0026#34;Glitter\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;No\u0026#34; }, { \u0026#34;trait_type\u0026#34;: \u0026#34;Special\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;No\u0026#34; }, { \u0026#34;display_type\u0026#34;: \u0026#34;date\u0026#34;, \u0026#34;trait_type\u0026#34;: \u0026#34;Birthday\u0026#34;, \u0026#34;value\u0026#34;: 1628902374 }, { \u0026#34;display_type\u0026#34;: \u0026#34;number\u0026#34;, \u0026#34;trait_type\u0026#34;: \u0026#34;Booster\u0026#34;, \u0026#34;value\u0026#34;: 10000000195586 } ], \u0026#34;opening_network\u0026#34;: \u0026#34;Binance Smart Chain\u0026#34;, \u0026#34;background_color\u0026#34;: \u0026#34;FFFFFF\u0026#34;, \u0026#34;animation_url\u0026#34;: \u0026#34;https://assets.polkamon.com/videos/Unimons_T08C02H06B04G00.mp4\u0026#34;, ... } Read Contract # Owner account: 0xaB111a3EaFE79b3110162d0e8b6FF1102ed25E2A token id: 10000586756 Functions:\nbalanceOf (3) getApproved (4) Role: DEFAULT_ADMIN_ROLE / MINTER_ROLE (1,2) getRoleMember (6) ownerOf (11) Write Contract # Functions:\nburn (2) mint (4) grantRole (3) safeTranferFrom setTokenURI Wallet # wordphase - BIP39 Connect from browser # PancakeSwap Remix IDE bscscan.com Contract deployment # Code Example will need a wallet and Blockchain endpoint References # https://www.euromoney.com/learning/~/media/0E855B86EDF04F3C8EABAFC42917C8C6.png?la=en\u0026hash=B67568B63CBB2C0C7311DE742F5B9E48E86DC8B9 ","date":"22 January 2022","externalUrl":null,"permalink":"/posts/blockchain-introduction/","section":"Posts","summary":"Resources # Hyperledger Fabric Solidity Notes # what is blockchain # The blockchain solution in IoT / Supply chain are changing \u0026ldquo;Mesh\u0026rdquo; network to a \u0026ldquo;Star Network each parties access and write to the \u0026ldquo;blockchain network\u0026rdquo;.","title":"Blockchain Introduction","type":"posts"},{"content":"","date":"17 January 2022","externalUrl":null,"permalink":"/tags/202201/","section":"Tags","summary":"","title":"202201","type":"tags"},{"content":"","date":"17 January 2022","externalUrl":null,"permalink":"/tags/booklist/","section":"Tags","summary":"","title":"booklist","type":"tags"},{"content":" 語理分析的思考方法 - 李天命 Salt Sugar Fat: How the Food Giants Hooked Us Sapiens: A Brief History of Humankind (人類大歷史) 機器學習書單 SRE Book 列夫·托爾斯泰作品集 ","date":"17 January 2022","externalUrl":null,"permalink":"/posts/booklist/","section":"Posts","summary":" 語理分析的思考方法 - 李天命 Salt Sugar Fat: How the Food Giants Hooked Us Sapiens: A Brief History of Humankind (人類大歷史) 機器學習書單 SRE Book 列夫·托爾斯泰作品集 ","title":"Booklists","type":"posts"},{"content":"","date":"17 January 2022","externalUrl":null,"permalink":"/tags/firefox/","section":"Tags","summary":"","title":"firefox","type":"tags"},{"content":" go to about:profile create a new profile Profile Manager - Create, remove or switch Firefox profiles\n","date":"17 January 2022","externalUrl":null,"permalink":"/posts/multi-profiles-in-firefox/","section":"Posts","summary":"go to about:profile create a new profile Profile Manager - Create, remove or switch Firefox profiles","title":"Multiple profile in firefox","type":"posts"},{"content":"https://www.youtube.com/watch?v=_MwNgO0aJo0\nhttps://www.youtube.com/watch?v=namlZpQ6W_k\n","date":"23 December 2021","externalUrl":null,"permalink":"/posts/aws-ground-station/","section":"Posts","summary":"https://www.youtube.com/watch?v=_MwNgO0aJo0\nhttps://www.youtube.com/watch?v=namlZpQ6W_k","title":"Aws Ground Station","type":"posts"},{"content":"https://www.youtube.com/watch?v=iwNg_fusT9A\nCopy on Write - copy will not perform immediately only when write operation is happend Snapshot - use the tech of copy on write to save some space. size few bytes to same size as original data ","date":"23 December 2021","externalUrl":null,"permalink":"/posts/btrfs/","section":"Posts","summary":"https://www.youtube.com/watch?v=iwNg_fusT9A\nCopy on Write - copy will not perform immediately only when write operation is happend Snapshot - use the tech of copy on write to save some space. size few bytes to same size as original data ","title":"BTRFS","type":"posts"},{"content":"","date":"23 December 2021","externalUrl":null,"permalink":"/tags/file-system/","section":"Tags","summary":"","title":"file-system","type":"tags"},{"content":"In this post, he wrote a web app to randomly generate the result with QRNG@ANU api.\nIt can generate Quantum random numbers. so interesting\u0026hellip;\n","date":"23 December 2021","externalUrl":null,"permalink":"/posts/random-numbers/","section":"Posts","summary":"In this post, he wrote a web app to randomly generate the result with QRNG@ANU api.\nIt can generate Quantum random numbers. so interesting\u0026hellip;","title":"Random number","type":"posts"},{"content":"","date":"23 December 2021","externalUrl":null,"permalink":"/tags/random-number/","section":"Tags","summary":"","title":"random-number","type":"tags"},{"content":"","date":"23 December 2021","externalUrl":null,"permalink":"/tags/space/","section":"Tags","summary":"","title":"space","type":"tags"},{"content":"","date":"23 December 2021","externalUrl":null,"permalink":"/tags/use-case/","section":"Tags","summary":"","title":"use-case","type":"tags"},{"content":"","date":"14 December 2021","externalUrl":null,"permalink":"/tags/cloud-centre/","section":"Tags","summary":"","title":"cloud-centre","type":"tags"},{"content":"https://www.youtube.com/watch?v=tj3F0tX8eyc\n6:26 - 5 steps\nAssess # Cloud Economic Assessment (8:20) # understand resources needed on-premise (mem, storage, bandwidth)\nPlan # use cloud adoption framework (10:45) Migrate # Cloud Migration Strategies (4:30) # Rehost - create vm on cloud and move application to it\nRefactor - Containerize\nRearchitech \u0026amp; rebuild - Microservice\n5-week migration process (17:00) # Week T-3 plan Week T-2 build Week T-1 test Week T-0 go migration and cleanup Week T+1 transition Use azure migrate (17:59) # Setup migration Centre of Excellence (CoE) (20:18) # Goven \u0026amp; Manage # Monitor/ automation # Five discipline of cloud governance (21:51) # principles on how to manage / goven\nCost management # Bill insight / report Security baseline # Patch manager Compliance software Resource consistency # Configuration Management (ansible) Infrastructure as code (terraform) Immutable application(container) Identity baseline # IAM policy Zero trust approach Deployment acceleration # Automation tools (scripts, ansible) Pipeline (jenkins) Optimise # Unifycloud - cloud pilot # Static Code Analytics Cost Management App \u0026amp; DB Readiness Migration Effort Estimate ","date":"14 December 2021","externalUrl":null,"permalink":"/posts/az-migration-strategies/","section":"Posts","summary":"https://www.youtube.com/watch?v=tj3F0tX8eyc\n6:26 - 5 steps\nAssess # Cloud Economic Assessment (8:20) # understand resources needed on-premise (mem, storage, bandwidth)\nPlan # use cloud adoption framework (10:45) Migrate # Cloud Migration Strategies (4:30) # Rehost - create vm on cloud and move application to it","title":"Coretek's cloud migration methodology","type":"posts"},{"content":"","date":"14 December 2021","externalUrl":null,"permalink":"/tags/migration-centre/","section":"Tags","summary":"","title":"migration-centre","type":"tags"},{"content":"","date":"14 December 2021","externalUrl":null,"permalink":"/tags/ml/","section":"Tags","summary":"","title":"ml","type":"tags"},{"content":"","date":"14 December 2021","externalUrl":null,"permalink":"/tags/spark/","section":"Tags","summary":"","title":"spark","type":"tags"},{"content":"Playlist\nApache Spark on EKS # https://github.com/kubernetes/kubernetes/tree/release-1.3/examples/spark\n","date":"14 December 2021","externalUrl":null,"permalink":"/posts/spark/","section":"Posts","summary":"Playlist\nApache Spark on EKS # https://github.com/kubernetes/kubernetes/tree/release-1.3/examples/spark","title":"Spark","type":"posts"},{"content":"https://www.youtube.com/watch?v=LK6KbAlQRIg\nprovide a GUI interface manage k8s clusters Cons (21:12) # Still using docker (should use cri-o) Only support old version k8s (1.18) Mutable approach (not using image to provision, it installs docker and setup the cluster for you) Very slow to create the cluster Pros # Manage multiple clusters GUI ","date":"13 December 2021","externalUrl":null,"permalink":"/posts/rancher/","section":"Posts","summary":"https://www.youtube.com/watch?v=LK6KbAlQRIg\nprovide a GUI interface manage k8s clusters Cons (21:12) # Still using docker (should use cri-o) Only support old version k8s (1.18) Mutable approach (not using image to provision, it installs docker and setup the cluster for you) Very slow to create the cluster Pros # Manage multiple clusters GUI ","title":"Review of Rancher","type":"posts"},{"content":"https://www.youtube.com/watch?v=mUTGdSA60Ao\nintroduction - Cloud Centre of Excellence is a team that can help the organisation for cloud adoption role \u0026amp; responsible characteristic for the team member best practice Characteristic # driving the organisation forward has ability to stand up - open-minded be confident why and how to do it - clear minded / big picture thinkers diverse and cross-functional Skill Set # Cost management Project Management (Transparent) Cloud operation engineer (CI/CD) Data engineer (new currency, capture data from IoT, video stream) Application Security (IAM, Policy, Firewall) Best Practise # Keep passion \u0026amp; excitement Start small (5-7 ppl in a team) Rotate people (training) Communication Scaling and reorganising Think of the failover and backup plan Cloud training for organisation Bill, cost optimization Empower the team (courage them trying new role, new service, bleeding-edge tech) Gain transparency (everyone knows the progress\u0026hellip;why you do that\u0026hellip;) Notes # cloud makes anything as code -\u0026gt; code makes things more transparent -\u0026gt; the complexity of the project is reduced many jobs can be automated through cloud and they are automatically documented as code is it possible to create a roadmap / resources library for the whole organisation (like AWS workshop)? - everybody can gear up the knowledge through videos / materials follow the framework and make planing\u0026hellip;like scaling\u0026hellip;we can plan it on the first day when starting the business ","date":"10 December 2021","externalUrl":null,"permalink":"/posts/cloud-center-of-excellenace/","section":"Posts","summary":"https://www.youtube.com/watch?v=mUTGdSA60Ao\nintroduction - Cloud Centre of Excellence is a team that can help the organisation for cloud adoption role \u0026amp; responsible characteristic for the team member best practice Characteristic # driving the organisation forward has ability to stand up - open-minded be confident why and how to do it - clear minded / big picture thinkers diverse and cross-functional Skill Set # Cost management Project Management (Transparent) Cloud operation engineer (CI/CD) Data engineer (new currency, capture data from IoT, video stream) Application Security (IAM, Policy, Firewall) Best Practise # Keep passion \u0026amp; excitement Start small (5-7 ppl in a team) Rotate people (training) Communication Scaling and reorganising Think of the failover and backup plan Cloud training for organisation Bill, cost optimization Empower the team (courage them trying new role, new service, bleeding-edge tech) Gain transparency (everyone knows the progress\u0026hellip;why you do that\u0026hellip;) Notes # cloud makes anything as code -\u0026gt; code makes things more transparent -\u0026gt; the complexity of the project is reduced many jobs can be automated through cloud and they are automatically documented as code is it possible to create a roadmap / resources library for the whole organisation (like AWS workshop)?","title":"A Roadmap to Cloud Centre of Excellence Adoption","type":"posts"},{"content":"","date":"10 December 2021","externalUrl":null,"permalink":"/tags/bandwidth/","section":"Tags","summary":"","title":"bandwidth","type":"tags"},{"content":"","date":"10 December 2021","externalUrl":null,"permalink":"/tags/blog/","section":"Tags","summary":"","title":"blog","type":"tags"},{"content":"","date":"10 December 2021","externalUrl":null,"permalink":"/tags/ec2/","section":"Tags","summary":"","title":"ec2","type":"tags"},{"content":" Objective # Test if the ec2 can reach 10Gbps bandwidth in same placement group Test if the 10Gbps is the limited by the ec2 instance type or aws AWS Limit # https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-network-bandwidth.html\nin single-flow traffic,\nplacement group: 10Gbps w/o placement group: 5Gbps flow = worker (thread) in the network driver. depends on the ec2 instance type.\nEC2 Instance Type Limit # https://aws.amazon.com/ec2/instance-types/m5/\nm5.8xlarge can reach 10Gbps\nSetup # Created 3 m5.8xlarge instances which can reach 10Gbps bandwidth each (i use spot)\n2 in same placement group 1 in other subnet Tool # iperf (BENCHMARK NETWORK THROUGHPUT) Test # Not in same placement group and parallel 1 # [root@ip-172-31-33-28 ~]# iperf -c 172.31.12.68 --parallel -i 2 -t 2 iperf: ignoring extra argument -- 2 ------------------------------------------------------------ Client connecting to 172.31.12.68, TCP port 5001 TCP window size: 812 KByte (default) ------------------------------------------------------------ [ 3] local 172.31.33.28 port 38880 connected with 172.31.12.68 port 5001 [ ID] Interval Transfer Bandwidth [ 3] 0.0- 2.0 sec 1.15 GBytes 4.93 Gbits/sec [root@ip-172-31-33-28 ~]# iperf -c 172.31.12.68 --parallel -i 2 -t 2 iperf: ignoring extra argument -- 2 In same placement group and parallel 1 # [root@ip-172-31-7-236 ~]# iperf -c 172.31.12.68 --parallel -i 2 -t 2 iperf: ignoring extra argument -- 2 ------------------------------------------------------------ Client connecting to 172.31.12.68, TCP port 5001 TCP window size: 1.90 MByte (default) ------------------------------------------------------------ [ 3] local 172.31.7.236 port 50544 connected with 172.31.12.68 port 5001 [ ID] Interval Transfer Bandwidth [ 3] 0.0- 2.0 sec 2.22 GBytes 9.54 Gbits/sec Not in same placement group and parallel 2 # [root@ip-172-31-7-236 ~]# iperf -c 172.31.12.68 --parallel 2 -i 1 -t 2 ------------------------------------------------------------ Client connecting to 172.31.12.68, TCP port 5001 TCP window size: 2.00 MByte (default) ------------------------------------------------------------ [ 4] local 172.31.7.236 port 50538 connected with 172.31.12.68 port 5001 [ 3] local 172.31.7.236 port 50536 connected with 172.31.12.68 port 5001 [ ID] Interval Transfer Bandwidth [ 4] 0.0- 1.0 sec 603 MBytes 5.06 Gbits/sec [ 3] 0.0- 1.0 sec 585 MBytes 4.91 Gbits/sec [SUM] 0.0- 1.0 sec 1.16 GBytes 9.96 Gbits/sec [ 4] 1.0- 2.0 sec 604 MBytes 5.07 Gbits/sec [ 4] 0.0- 2.0 sec 1.18 GBytes 5.06 Gbits/sec [ 3] 1.0- 2.0 sec 579 MBytes 4.85 Gbits/sec [SUM] 1.0- 2.0 sec 1.16 GBytes 9.92 Gbits/sec [ 3] 0.0- 2.0 sec 1.14 GBytes 4.88 Gbits/sec [SUM] 0.0- 2.0 sec 2.32 GBytes 9.94 Gbits/sec Outcome # w/i the placement group # Flow Bandwidth/Flow Bandwidth 1 10Gbps 10Gbps 2 5Gbps 10Gbps we can use up all m5.8xlarge\u0026rsquo;s bandwidth in a single-flow traffic.\nBut the traffic splits 50%,50% in double-flow traffic.\nTherefore, 10Gbps is the max bandwidth of the m5.8xlarge.\noutside the placement group # Flow Bandwidth/Flow Bandwidth 1 5Gbps 5Gbps 2 5Gbps 10Gbps we need two flows to use up all m5.8xlarge\u0026rsquo;s bandwidth (5Gbps, 5Gbps)\nso it reaches the limit of AWS\u0026hellip;\n","date":"10 December 2021","externalUrl":null,"permalink":"/posts/aws-ec2-bandwidth-test/","section":"Posts","summary":"Objective # Test if the ec2 can reach 10Gbps bandwidth in same placement group Test if the 10Gbps is the limited by the ec2 instance type or aws AWS Limit # https://docs.","title":"EC2 Bandwidth test","type":"posts"},{"content":"","date":"10 December 2021","externalUrl":null,"permalink":"/tags/markdown/","section":"Tags","summary":"","title":"markdown","type":"tags"},{"content":"I found this markdown software when watching this video\nI am interested in that map and think if I can organise my markdown notes with it.\nTutorials # How To Use VIM Bindings in Obsidian | Beginners Guide # https://www.youtube.com/watch?v=yX_Qdr9-7k\nI Tried Obsidian Note Taking for a Week\u0026hellip; (MD App Review, Tips, Features, Guide, and Setup) # https://www.youtube.com/watch?v=TDhTpPIjsDg\nmost of the features do not attract to me Will prefer organising the notes with tags not folder Spacemacs is better for editing. Happy with marco, fuzzy file searching, find and replace, vim keybinding and git Template function (like marco) Use to preview and visualise my notes but not editing Graph view support tags to virtualize the notes so no need to refactor my Hugo notes (Graph View \u0026gt; Filters \u0026gt; Tags) ","date":"10 December 2021","externalUrl":null,"permalink":"/posts/obsidian/","section":"Posts","summary":"I found this markdown software when watching this video\nI am interested in that map and think if I can organise my markdown notes with it.\nTutorials # How To Use VIM Bindings in Obsidian | Beginners Guide # https://www.","title":"Obsidian","type":"posts"},{"content":"","date":"1 December 2021","externalUrl":null,"permalink":"/tags/bluetooth/","section":"Tags","summary":"","title":"bluetooth","type":"tags"},{"content":"https://www.youtube.com/watch?v=1I1vxu5qIUM\nTransmit an electromagnetic wave (binary) with different wavelength\neg: 121mm = 1 124mm = 0\nthe waves send in all directions\nBluetooth range: 2.4835GHz - 2.4GHz\nDivided in different sections (79 channels), each section has a pair of wavelengths representing 0 and 1\nthe bluetooth range is shared with some other device like a microwave oven\nlike the power of a microwave oven is too large for bluetooth.\nit may destroy the bluetooth device if you put them inside the microwave oven (danger)\nData Integrity # Packet # Access Codes (72 bits) Header (54bits) Payload (vary, depends on the function) Frequency Hopping # Two bluetooth devices will have a set of channels to send data\nThey will change the channels frequently\nError Detection # Noise Filtering # Data Transmit # Frequency Shift Keying # carrier wave adjusts the wavelength to send 0 and 1\nPhase Shift Keying # constant wavelength and attitude\nuse the wave function to determine 0 and 1 (like sin x = 0, cos x = 1, instead of A sin x and B sin X)\n","date":"1 December 2021","externalUrl":null,"permalink":"/posts/bluetooth/","section":"Posts","summary":"https://www.youtube.com/watch?v=1I1vxu5qIUM\nTransmit an electromagnetic wave (binary) with different wavelength\neg: 121mm = 1 124mm = 0\nthe waves send in all directions\nBluetooth range: 2.4835GHz - 2.4GHz\nDivided in different sections (79 channels), each section has a pair of wavelengths representing 0 and 1","title":"Bluetooth","type":"posts"},{"content":" Upgradable contract # https://www.youtube.com/watch?v=kWUDTZhxKZI\u0026list=WL\u0026index=19\nproxy pattern # user interact with the proxy contract\nthe proxy will point to another smart contract\nWhen upgrading the contract, the admin of the proxy contract points the proxy to another smart contract storage\nImplementation # Transparent UUPS ","date":"1 December 2021","externalUrl":null,"permalink":"/posts/openzeppelin/","section":"Posts","summary":"Upgradable contract # https://www.youtube.com/watch?v=kWUDTZhxKZI\u0026list=WL\u0026index=19\nproxy pattern # user interact with the proxy contract\nthe proxy will point to another smart contract\nWhen upgrading the contract, the admin of the proxy contract points the proxy to another smart contract storage","title":"OpenZeppelin","type":"posts"},{"content":"","date":"1 December 2021","externalUrl":null,"permalink":"/tags/smart-contract/","section":"Tags","summary":"","title":"smart-contract","type":"tags"},{"content":"","date":"1 December 2021","externalUrl":null,"permalink":"/tags/study/","section":"Tags","summary":"","title":"study","type":"tags"},{"content":" Zero to Full-Time Programmer in 5 Steps # Zero to Full-Time Programmer in 5 Steps\nstart from small consistently study every day The scoreboard that is changing my life # The scoreboard that is changing my life\nuse scoreboard to track your goal\ngoal vs actual\nWeekly review (like a logbook)\nlagging metric (review past performance)\nleading metric (measure future performance)\nHow I consistently study with a full time job # https://www.youtube.com/watch?v=INymz5VwLmk\nthe key is consistency, motivation doesn\u0026rsquo;t last long (always show up, always try) keep tracking the process ME # small step every day never being late, start today If you feel hesitating, then just start it do not need to make things perfect or well-prepared to start on something Archive Goal # https://www.youtube.com/watch?v=gHSh1O1HUqA\nFind the way out instead of focusing on the obstacle About the dream job # https://www.youtube.com/watch?v=mtgt1ElOo0U\nno dream job but can make your own dream career 【上篇】背單字絕不要○○！會20種語言的達人分享如何儘早擺脫初級階段！Feat. Steve Kaufmann先生 【外文學習法】 日本人Haru老師【#51】日語發音，全程中文字幕 # https://www.youtube.com/watch?v=aBjZHomO2N0\naccept the fact that you will forget what you learn Stanford Engineering Hero Lecture: Morris Chang in conversation with President John L. Hennessy # https://www.youtube.com/watch?v=wEh3ZgbvBrE\nlearn and think if you don\u0026rsquo;t learn, you have nothing to think 《領航者Visionaries》張忠謀 Morris Chang (II)：我不以輝煌作為人生目標 # https://www.youtube.com/watch?v=AYUBSr3Wons\nLifelong Learning systematically ","date":"1 December 2021","externalUrl":null,"permalink":"/posts/idea-about-study/","section":"Posts","summary":"Zero to Full-Time Programmer in 5 Steps # Zero to Full-Time Programmer in 5 Steps\nstart from small consistently study every day The scoreboard that is changing my life # The scoreboard that is changing my life","title":"Study Tips","type":"posts"},{"content":"https://www.youtube.com/watch?v=j_j2MiAxUvY\u0026list=PL0MZ85B_96CEmmy0C6NF52ZCMNcY1Wryf\nMethods # Wait until the transaction is completely settled # not practical and safe\nHashlock # The transaction is partially complete\nSettle once the sender publishes the key on the blockchain.\nCons # hugo cannot refund if kevin does not response Timelock # hugo made a partial transaction that the btc can be claimed by kevin within n number of blocks if the transaction is expired, hugo can claim back the fund Cons # the claims need to execute manually HTLC (Hash Time Lock Contract) # Combine two locks: (and)\nclaim within n number of blocks the key is revealed on the blockchain Steps in a cross chain swap # Timestamp: 55:36\nExample: btc -\u0026gt; eth swap from hugo to kevin\nassume that Hugo has a key (secret that only Hugo will know) 1234 with hash 4567\nHugo and Kevin will not make transactions directly\nThey will send the fund to the contract instead\nTHe contract will hold the eth and btc for Hugo and Kevin\nThere are 2 separate contracts\none is in btc (let\u0026rsquo;s say Contract A)\nthe other is in eth (let\u0026rsquo;s say Contract B)\nhugo sent btc to the Contract A in btc network\nTransaction:\nField Value From hugo Value 10btc To kevin Hash 4567 Kevin sent eth to the Contract B in eth network Field Value From kevin Value 134eth To hugo hash 9876 Hugo observed that kevin has funded eth to Contract B. then he will submit his key 1234 in btc network, btc send to kevin by the Contract A same as kevin, he submits his key 9876 in eth network, eth sent to hugo by the Contract B Problem # semi-trusted we assume Hugo and Kevin can see each other on each chain if the access right is changed during the transaction\u0026hellip;let say hugo can revoke kevin on eth network..then the transaction fails (this happens on hyperledger as it can have access control but it is not possible on btc and eth) ","date":"28 November 2021","externalUrl":null,"permalink":"/posts/cross-chain-swap/","section":"Posts","summary":"https://www.youtube.com/watch?v=j_j2MiAxUvY\u0026list=PL0MZ85B_96CEmmy0C6NF52ZCMNcY1Wryf\nMethods # Wait until the transaction is completely settled # not practical and safe\nHashlock # The transaction is partially complete\nSettle once the sender publishes the key on the blockchain.","title":"Atomic cross chain swap between Hyperledger Fabric and Ethereum","type":"posts"},{"content":"","date":"28 November 2021","externalUrl":null,"permalink":"/tags/crosschain/","section":"Tags","summary":"","title":"crosschain","type":"tags"},{"content":"","date":"28 November 2021","externalUrl":null,"permalink":"/tags/hyperledger/","section":"Tags","summary":"","title":"hyperledger","type":"tags"},{"content":" Building BLOCKCHAIN Apps With HYPERLEDGER COMPOSER # https://www.youtube.com/watch?v=gAxK6zYrfxI\na framework of hyperledger\ntesting - mocha / chai webapp generator - Yeoman api generation - Rest Server + loopback api doc generation - swagger auth (rest api) - passport integration with IOT - red-node ","date":"28 November 2021","externalUrl":null,"permalink":"/posts/hyperledger-composer/","section":"Posts","summary":" Building BLOCKCHAIN Apps With HYPERLEDGER COMPOSER # https://www.youtube.com/watch?v=gAxK6zYrfxI\na framework of hyperledger\ntesting - mocha / chai webapp generator - Yeoman api generation - Rest Server + loopback api doc generation - swagger auth (rest api) - passport integration with IOT - red-node ","title":"Hyperledger composer","type":"posts"},{"content":"","date":"28 November 2021","externalUrl":null,"permalink":"/tags/iot/","section":"Tags","summary":"","title":"iot","type":"tags"},{"content":"Official Website\nPlaylist\na visual editor to build workflow and ui application to interact with IoT\nwire the IoTs and user (with web dashboard) read data from IoT write data to IoT (control it, on and off etc) build a flow among IoTs and users for automation build dashboard UI all can be done by drag and drop\ncan be used with blockchain in the supply chain\u0026hellip;\nnode red pulls the data from sensors and submits to blockchain automatically\nUse cases # Temperature monitor dashboard # listen to any data sent from the thermometer show the temperature on the dashboard Light switcher # send switch on and off signal to the light bulb through dashboard Automation (like IFFF and Apple Shortcut but in the IoT version) # monitoring the environment with sensors (like humidity and temperature) trigger other IoT (like turn on the washing machine) Water Utility (SCADA) # https://www.youtube.com/watch?v=FCfmWnxQkoc\ntanks and bump bump water from the well to task 1 and then bump to tank 2 alert when the tank reaches the critical level MQTT # lightweight network protocol for IoT\nHow to Get Started with MQTT What is an MQTT Broker Clearly Explained Basic Concept # IoTs subscribe to specific topic workshop/switch in MQTT Broker user publish workshop/switch to MQTT broker (via red node) IoTs receive the data from the MQTT broker and react to it (eg: turn on the switch\u0026hellip;) MQTT Broker # Open source MQTT broker: Eclipse Mosquitto\nbidirectional protocol # subscribe and publish at the same time in the same IoTs\nTLS # Basic authentication # CERT # failover / backup # retained message # When adding a new device, the value is empty as no subscribed topic is sending data. this feature will fill it with the last record\npersistent session # when IoT disconnected to the broker\nno need to resubscribe the topic automatically connect back to the broker with network connection back Client status # IoT sends birth death discount message when connected to broker\nWhen the connection is established, the broker sends a birth message to a set of topic\u0026rsquo;s subscribers There is a keep-alive timer at the broker. Broker will ping the device periodically to ensure the device is alive If the keep -alive timer is timeout, the broker will send a death message to the topic\u0026rsquo;s subscribers Also, red-node can ping the IoTs to check their status. publish the status of IoT topic when it is connected or disconnected or unexpected disconnected\nMQTT.fx = software to publish / subscribe to the broker\nwith PI # https://www.youtube.com/watch?v=WxUTYzxIDns\nmentions MQTT broker but does not mention how to setup node-red is installed on the pi itself, and sends a message to the pin directly to control the light bulb video does not tell # how to setup the MQTT (send data to a specific topic to the broker) how to receive data from the device without MQTT (he just shows how to control the light bulb) MQTT client setup # PI # https://www.youtube.com/watch?v=WxUTYzxIDns\ninstall MQTT client connect to the MQTT server publish all events to the MQTT service with a specific topic (need to write code) Device with preinstalled software # https://www.youtube.com/watch?v=KqHVkUiPRzc\nThis is not using node-red but the concept is same\njust enable MQTT function How to capture MQTT data from device at node-red # listen to all topics from MQTT and capture it to debug node trigger the event to send out (eg: clicking the wifi button) / wait for the device to publish a message to the topic (thermometer) find out the payload and topic name Enterpise Case study # How to Build Massive IoT with Klika Tech, Wirepas and AWS Building an End-to-End Industrial IoT (IIoT) Solution with AWS IoT - AWS Online Tech Talks Use case # tracking the temperature, humidity, vibration in shipping pro-logistics: open the container door, turn on and turn off the light.. control the lighting track which the conference room is used locate things in office / warehouse Flow # IoTs\ngateways\nMQTT Broker (aggregate all data)\nuser application wirepas network tools backend wirepas FW (RuuviTag)\nwirepas snap\n(AWS) wirepas service (WNT, WPE) (Dashboard) / Greengrass snap (Like Red-Node)\nAWS IoT SiteWise \u0026amp; AWS IoT Greengrass # (Depends on the device manufactory)\nOnLogic Moxa greengrass\naccess point for the IoTs to send data to AWS\nAWS IoT Message Broker # AWS version of MQTT\nAWS IoT Device Management # IoT version of System Manager\nbulk registration Patching Monitor the health / Search Integration with analytics tools Wireless mesh network with asset tracking on site # building gateways on site to let the IoTs can send data to cloud (SiteWise) the message can be repeated (extended) once they cover each other (that is why it is called a mesh network) 700000 devices in one mesh network cover Oslo ROI -\u0026gt; look at the maintanese and analytics and delivery RuuviTag # Pressure\nTemperature\nHumidity\nAccelerometer\nLocation\nI2C / SPI\nSteps # Build SiteWise Assets / Model Build SiteWise Gateway (connect with KEPServerEX) Portal -\u0026gt; SiteWise Montior -\u0026gt; Create Projects IoT Core set a rule (How to process the data) to send data to analytics in analaytics there is a channel created -\u0026gt; use pipeline to pass the raw data to lamda and process the raw data -\u0026gt; store the processed data to data store -\u0026gt; create (SQL / Container custom code) in data sets and consumer in IoT Events (Detector model) ","date":"28 November 2021","externalUrl":null,"permalink":"/posts/node-red/","section":"Posts","summary":"Official Website\nPlaylist\na visual editor to build workflow and ui application to interact with IoT\nwire the IoTs and user (with web dashboard) read data from IoT write data to IoT (control it, on and off etc) build a flow among IoTs and users for automation build dashboard UI all can be done by drag and drop","title":"Node Red","type":"posts"},{"content":"","date":"28 November 2021","externalUrl":null,"permalink":"/tags/node-red/","section":"Tags","summary":"","title":"node-red","type":"tags"},{"content":" Day1 # Setup # https://www.eksworkshop.com/020_prerequisites/ec2instance https://www.eksworkshop.com/030_eksctl/launcheks https://www.eksworkshop.com/beginner/060_helm/helm_intro/install Tasks # Deploy simple application Fargate Notes # There are 2 modes in EKS:\nEC2 Fargate They can coexist in EKS.\nAlso we can only fargate only. When we deploy plugins to the cluster, It will be deployed to the control panel. (hidden from user)\nHow to use fargate in EKS: # create a fargate profile which is associated with a namespace deploy yml to that namespace Pros and Con of fargate # can be scaled up quickly # it is a micro vm as similar as pod one fargate instance per pod lower cost of fargate # Fargate will be cheaper in some situations. no need to rent large ec2 instance. it is good if you are doing batch jobs with EKS no need to maintain the ec2 instance (patch it, upgrade the kubelet) not well integrate with other AWS services # like efs?\u0026hellip;need to further study on it.\nDay3 # Tasks # https://www.eksworkshop.com/intermediate/245_x-ray/microservices https://www.eksworkshop.com/intermediate/246_monitoring_amp_amg https://www.eksworkshop.com/intermediate/250_cloudwatch_container_insights https://www.eksworkshop.com/beginner/170_statefulset https://www.eksworkshop.com/beginner/190_efs Alternative of argo # https://www.eksworkshop.com/intermediate/260_weave_flux https://www.eksworkshop.com/intermediate/220_codepipeline ","date":"27 November 2021","externalUrl":null,"permalink":"/posts/aws-workshop/","section":"Posts","summary":"Day1 # Setup # https://www.eksworkshop.com/020_prerequisites/ec2instance https://www.eksworkshop.com/030_eksctl/launcheks https://www.eksworkshop.com/beginner/060_helm/helm_intro/install Tasks # Deploy simple application Fargate Notes # There are 2 modes in EKS:\nEC2 Fargate They can coexist in EKS.\nAlso we can only fargate only.","title":"AWS EKS Workshop","type":"posts"},{"content":" Pros with AWS Managed Blockchain # no need to build the ledger by themself focus on the smart contract a single blockchain network can effectively work cross multiple parties An Overview on Blockchain Services from AWS # https://www.youtube.com/watch?v=WAIOBeQA2QQ\nledger database (immutable transaction) consenus decentlization smart contract ease for audit Hyperledger Fabric VS eth # authentication access control private ledger ","date":"27 November 2021","externalUrl":null,"permalink":"/posts/aws-managed-blockchain/","section":"Posts","summary":"Pros with AWS Managed Blockchain # no need to build the ledger by themself focus on the smart contract a single blockchain network can effectively work cross multiple parties An Overview on Blockchain Services from AWS # https://www.","title":"AWS Managed Blockchain","type":"posts"},{"content":"","date":"27 November 2021","externalUrl":null,"permalink":"/tags/eks/","section":"Tags","summary":"","title":"eks","type":"tags"},{"content":" Control Panel Fee # AKS: 0 EKS: fixed charge 0.1 USD / Hour Integration # I think AKS is better, most of the provisioning plugins can be enabled by one click\nIn aws, you need to\ninstall plugins setup iam service account for the plugins to access AWS service application routing # For AKS,\nit is just an checkbox option For EKS, you need to\nsetup IAM OICD setup external dns setup ALB contrller auto scaler # Same as application routing -\u0026gt; auto scaler. enable and use\nin AWS, we need to setup cluster autoscaler\nvolume provisioning # AKS supports volume provisioning with Azure File (EFS) \u0026amp; Azure Disk (EBS), it has many pre-defined volume class\u0026hellip;we do not need to install any plugin\nFor EKS, we need CSI driver\nebs efs Virtual node # It is AKS things, like EKS\u0026rsquo;s fargate\u0026hellip;but I cannot find the price..\n2021-12-10 updates: # I have asked on Microsoft Q\u0026amp;A: https://docs.microsoft.com/en-us/answers/questions/659456/price-of-ake-virtual-node-and-aks-without-node-poo.html\nneed min 1 system node group can start / stop the cluster the VM will stop charging when the cluster is stop charge of other provisioned services like LB and Azure File are unknown\u0026hellip; virtual node will charge as container service (like fargate) ","date":"27 November 2021","externalUrl":null,"permalink":"/posts/eks-vs-aks/","section":"Posts","summary":"Control Panel Fee # AKS: 0 EKS: fixed charge 0.1 USD / Hour Integration # I think AKS is better, most of the provisioning plugins can be enabled by one click","title":"EKS vs AKS","type":"posts"},{"content":"","date":"27 November 2021","externalUrl":null,"permalink":"/tags/hk/","section":"Tags","summary":"","title":"hk","type":"tags"},{"content":"","date":"27 November 2021","externalUrl":null,"permalink":"/tags/movie/","section":"Tags","summary":"","title":"movie","type":"tags"},{"content":"https://youtu.be/TgsTCzLRQcs?t=1905\nMay You Stay Forever Young # Taipei golden horse film festival 2021\nTrailer\ntwo girls arrested in 721 one sexuel abused by HK police force and try to commit suicide another young teanager insists on looking for her. She is arrested as she turns back to him in a confrontation with riot police Revolution of Our Times # IMDB\nTrailer\nConnection with mashipo # How an elderly Hong Kong pro-democracy protester also fought in vain to save his village from developers\nWong understands people because he has confronted the government before in Mashipo young people help them to protect his home in mashipo\u0026hellip;so he thinks he has responsibility to protect them back The FA , who praises the police force, let him enter the Prince Edward station at 831 # still a high school student have dramatic changes after 831 (become negative\u0026hellip;) interview with his teacher and classmate Protector stayed in Legislative Council # a girl who said 齊上齊落(all together not one less), turn back bring people out from the council ppl should participate more\u0026hellip;not just support them\u0026hellip; HK is not important; HKER is more important TW use 40 years to have democracy - need patient Repressed anger as it does not help Interview with people who is already in prison or left HK # Ho Kwai-lam Aerial photography - protest confront with riot police at MongKok # what is “be water” Drifting Petals # Taipei golden horse film festival 2021\nTrailer\n","date":"27 November 2021","externalUrl":null,"permalink":"/posts/movies-about-hk/","section":"Posts","summary":"https://youtu.be/TgsTCzLRQcs?t=1905\nMay You Stay Forever Young # Taipei golden horse film festival 2021\nTrailer\ntwo girls arrested in 721 one sexuel abused by HK police force and try to commit suicide another young teanager insists on looking for her.","title":"Movies about Hong Kong","type":"posts"},{"content":"","date":"27 November 2021","externalUrl":null,"permalink":"/tags/workshop/","section":"Tags","summary":"","title":"workshop","type":"tags"},{"content":"","date":"26 November 2021","externalUrl":null,"permalink":"/tags/auth/","section":"Tags","summary":"","title":"auth","type":"tags"},{"content":" Resources # Exam Landing Page Sample Questions Exam Guide udemy course udemy mock exam whizlabs test \u0026amp; hand-on labs TODO # take notes revision whizlabs mocks (by topic) mock 1 (udemy) mock 2,3,4 (whizlabs) examtopic udemy quiz + exercise Revision # The numbers below are the page no of the udemy course\u0026rsquo;s pdf: AWS Certified Networking Specialty Slides v1.1.\nSummary # DNS (151) Advanced Networking (188) VPC Endpoint (292, 298) Site-to-site VPN (347, 401, 407) VPN Tunnels and Routing (348, 352) DX Gateway with VGW (532) DX with TGW (547) DX Billing (624) DX (630) Troubleshooting in DX (626) Gateway Load Balancer (816) Q \u0026amp; A # ALB (677) DX (625) Good to know # SG (46) BYOIP (90) VPC Traffic Mirroring (105) DHCP Option Sets (124) TGW (251) AWS Site-to-Site VPN (403) Network Load Balancer (656) Comparisons # private, public and EIP (33) IPv4, IPv6 (36) NACL, SG (48) NAT Gateway, Instance (64) VPC Peering vs Transit Gateway Cloudfront function vs lambda@edge (703) AWS global accelerator vs Cloudfront (711) Exam Essential # VPC Fundamentals (67) Advanced VPC (190) VPC Peering Endpoint (301) AWs Site-to-Site VPN (402) Direct Connect (632) Firewall (806) Gateway Load Balancer (817) Revision by topic # Playlists # global accelerator api gateway client vpn cloudformation cloudfront cloudwatch cloudhsm cognito config dx edge computing elb firehose guarduty inspector lambda organization route53 s3 sqs ssm vpc vpn waf workspace References # accelerator - custom routing accelerator - custom routing alb - limit request to clondfront only aurora - regional failover aws organization - SCP cloudfront - origin types cloudfront - signed url vs signed cookies cloudfront - troubleshooting cloudfront - will forward the response when first byte arrives from the origin cloudhsm - auto ha cloudtrail - 5 trails for region cloudtrail - log encryption with KMS cloudtrail - use integrity validation to check the log was modified, delete or change cloudwatch - namespace valid characters cloudwatch - no need to config sns to provide data every minute cloudwatch - skip metrics config - managed rules config - renaming the delivery channel config - requirements config - type of triggers config - viewing configuration compliance costs model costs model 2 dhcp - cannot modify and one vpc one dhcp options dx - dx in public regions can access any other public region dx - faq dx - lag requirement 1 dx - lag requirement 2 dx - public vif dx - public vif connectivity dx - quotas dx - requirements for virtual interfaces dx - vif dx - virtual Interfaces to Direct Connect connections or LAG bundles dx - vpn as backup ec2 - http proxy ec2 - network performance ec2 - retrieve instance metadata eip - hostname will be changed once eip attached eip - public ip change every time stop and start the instance eip - public ip will be released once eip attached eip - reverse dns record for mailserver eni - limitation flow log local zone - alb limitation local zone - supported service ms ad - limitation - not compatible with exchange and skype ms ad - requirements nacl - default rule# in nacl nat - need egress-only-internet-gateway for ipv6 to ipv6 commnuication nat - only support tcp udp and icmp nat - pricing nat - tcp connectin fail as tcp does not support ip fragmentation placement group - can add / move / remove instance from group placement group - can launch with diff instance type but not recommended placement group - can span vpcs placement group - cannot merge placement group placement group - stop and start is fine but may have insufficient capacity error without capacity reservation quicksight - private connection with rds redishift - private connectivity with enhanced vpc routing route53 - a/p failover route53 - aws services which support alias records route53 - can attach private hosted zone with overlapping namespace in same vpc route53 - dns resolution b/w on-premise and aws with ad route53 - dns server with custom domain (white-labe and reusable delegation set) route53 - health check rules route53 - system rules when forwarding less specific domain simple ad - requirement sns tgw - peering tgw - quotas tgw - route table priority vpc - aws cidr tier vpc - aws does not support broadcast in vpc vpc - ipv4 subnet cidr prefix vpc - ipv6 subnet cidr prefix vpc - jumbo frame packet drop as Don't Fragment flag is set but the network does not support higher MTU vpc - multicast support vpc - reserved address in cidr vpc - route table qutoa vpc - route table troubleshoot vpc - usage of enableDnsHostnames and enableDnsSupport vpce - service endpoint support ipv4 over tcp only vpce - tagging is suported vpce - use prefix list in security group vpn - ipv6 support on tgw but not vgw vpn - static route a/p mode (priority) vpn - troubleshooting vpn - why cannot overlap the cidr vpn -ipsec encryption algorithms waf - config count action to test (monitor mode) waf - rule statement list workspaces - requirements appliance in shared vpc cloudhub mult vpc with single customer gateway multi-vpc network infrastructure route table options shared vpc transit vpc Virtual Private Cloud Connectivity Options vpc peering with cidr overlap vpc with subnet overlapping VPC Fundamentals (17) # What is TCP/IP? # Answer application (data) transport (segment) network (packet) link (frame) physical (frame) What is OSI? (306) # Answer application presentation session transport network link physical ##### How to calculate the no of address in a CIDR (like 192.168.0.1/28)? (18) Answer 2^(32-28)=16 Can we override the main route table in VPC? (26) # Answer override it at subnet level Which IPs are reserved for a vpc? (29) # Answer 5\n0=network 1=router 2=dns 3=reserved last=broadcast What is the default behaviour of SG (inbound and outbound)? (46) # Answer inbound: block all outbound: allow all How is the NAT gateway charged? (59) # Answer hourly charge data transfer fee Can we apply SG on a NAT gateway? (59) # Answer no What is the port range NAT gateway needs for outbound connection? (59) # Answer 1024-65535 Why don\u0026rsquo;t we need cross-AZ failover in NAT ? (62) # Answer because if az down, nat and app are down too What is the most important setting to setup NAT with EC2? (63) # Answer disable the source and destination check Advanced VPC (72) # What is the limitation of adding a secondary CIDR? (79) # Answer aws defines cidr in 3 classes. 172 169 10192.168 172.16 10.0 if the primary cidr is in one of the classes, the secondary cidr must be in the same class or not in any class with an equal / more specific prefix if the primary cidr is not in one of the class, the secondary cidr must not in any class with equal / more specific prefix 5ipv4 1ipv6 How many private, public and elastic ip and sg can one ENI have? (81, 85) # Answer private - depends on the instance type public - 1 elastic - 11 eip per private ip What is the use case of dual home setup with multiple ENIs? (84) # Answer 1 eni for internal traffic 1 eni for internet traffic What are the prerequisites of BYOIP? (89, 90) # Answer you should own that ip ip should have a good record each account can bring 5 ips (ipv4 \u0026 ipv6) ROA ipv4: /24; ipv6: /48 pub; /56 pri What type of Flow Logs can we capture from a VPC? (92) # Answer vpc subnet eni Where does the Flow Logs Action field come from? (97) # Answer security group / nacl What type of record cannot be captured in a flow log? (98) # Answer traffic from aws dns / metadata dhcp, windows licence activation server What can be the source \u0026amp; target of a VPC Traffic Mirror? (102, 105) # Answer source: eni target: eni, nlb What port does the VPC Traffic Mirror require? (105) # Answer udp 4789 How to set a custom domain / dns on an EC2 instance? (113) # Answer change the dhcp-option VPC DNS and DHCP # What does enableDNSSupport do? (117) # Answer resolve the dns What does enableDNSHostname do? (117) # Answer assign hostname to the ec2 instance How to resolve the hostname in VPC peering and TGW? (132) # Answer enable dns support How to resolve the hostname in Hybrid cloud (AWS to on-premise, on-premise to AWS) (old \u0026amp; new way)? (132-150) # Answer old:\naws -\u003e on-premise: setup ad / dns server on aws; set dhcp to that server; forward query to that server in on-premise dns on-premise -\u003e aws: setup ad / dns server on aws; use that server in on-premise new: aws -\u003e on-premise: setup outbound endpoint to on-premise dns, set forward rule to that endpoint on-premise -\u003e aws: setup inbound endpoint, forward query to that endpoint in on-premise dns VPC Network Performance and Optimization # Formula of throughput (154) # What is Jumbo frame? (154) # Answer MTU \u003e 1500 (9001 / 8500) What is Path MTU Discovery? (155) # Answer check the max MTU support between 2 routes What are the advantages of using placement group - cluster (162) # Answer low latency higher bandwidth What is the bandwidth limitation between VPC, EC2 instances, VPN and DX (179-183)? # Answer vpc = 100% to igw / other region = 50% ec2 = 5gbps 10gbps (placement group) single flow vpn (tgw) 1.25gbps per tunnel and sum up to 25gbps vpn (vgw) 1.25gbps dx (vgw/tgw) = 1/10/100gbps VPC Peering # What is the limitation of VPC Peering? (207) # Answer non-transitive cannot access igw, nat, vpn, peer, gateway endpoint TGW # What is the difference between vgw and tgw? (211) # Answer vgw is non-transitive cannot access igw, nat, vpn, peer, gateway endpoint what attachments does TGW support? (211) # Answer vpn vpc tgw dxgw What is the special route in TGW? (230) # Answer custom route table for each tgw attachment VPC Endpoint # How many types of VPC endpoint? What is the difference between them? (258) # Answer interface = has eni attached gateway = use route table private link = attach to alb or nlb all has vpc endpoint policy Can on-premises network access to the gateway endpoint with VPN/DX? (273) # Answer no What kind of source can be used in a private link? (281) # Answer nlb and alb How to use hostname in interface endpoint? (288) # Answer enable private dns How to use the interface endpoint without private DNS? (290) # Answer use the dns name with \"region\" When should we use a private link? When should we use VPC Peering (297) # Answer Many clients need to access the service from different accounts (\u003e125) use peering when there are many services that need to be accessed AWS Site-to-Site VPN # What port does IPsec need? (309) # Answer udp/500 Range of public and private ASN (313) # Answer 0-65525 64512-65534 4 common BGP Param for routing (319) # Answer weight as_path med local-preference What type of VPN does AWS support Site-to-Site VPN? (327) # Answer ipsec What routing method does AWS support in Site-to-Site VPN? (329) # Answer bgp / static What port is required in VPN for NAT-T? (335) # Answer udp/4500 VPN Tunnels and Routing # How to set Active/Active tunnel (351) # Answer advertise the same prefix. use BGP and ECMP for load balancing \u003c- this will cause the traffic to go randomly advertise diff prefix. more specific prefixes will be prefered or use AS_PATH or MED to control the traffic from aws What is DPD? Port to send messages? default timeout? timeout action? (354) # Answer dead peer detection. detect the dead peer tunnel. 30s. action: clear, restart, none How to prevent a tunnel from terminating due to inactivity (355) # Answer ping the aws network each 5 seconds to prevent idle tunnel How to monitor the tunnel status? (357) # Answer set alarm with cloudwatch metrics DX # What is the fibre mode of DX? (450) # Answer single what is the data link requirement of DX (450) # Answer fibre 802.1vlan LX What are the requirements for a customer router? (450) # Answer bgp What is the port of BGP? (457) # Answer 179/tcp What is BFD? How to enable it? (458) # Answer auto failover dx is enabled by default. enable it on the customer router lower the failure detection time How long does it take for failover with and without BFD? (458, 597, 598) # Answer 300ms * 3times = 1s default 30s * 3 = 90s; DPD = method to detect the failure (for ipsec+bgp). BFD = method to lower the failure detection (for bgp) How can a user whitelist the ips range from AWS? (459) # Answer whitelist the ip in ip-range.json How many VIF can a hosted connection have? (465) # Answer 1 What Ip type does DX support? (498) # Answer ipv4 and ipv6 How many route prefixes can the public vif advertise? (503) # Answer 1000 How many route prefixes can a private vif advertise? (507) # Answer 100 What is the limitation of vif and vgw location? (507) # Answer in the same region What service cannot access inside a VPC with DX? (510) # Answer nat, nat + igw, peered vpc, gateway endpoint ,dns What is the usage of DX gateway? (517) # Answer connect dx and vpc in any regions (pri/transit vif) How many vif can a DX connection have? (526) # Answer 50(pub + pri) How many transit vif can a DX connection have? (547) # Answer 1(transit) How many DX gateways can a vif have? (530) # Answer 1 How many vif can a DX gateway connect to? (530) # Answer 30 How many vgw can a DX gateway have? (526,547) # Answer 10 How many tgw can a Dx gateway have? (538) # Answer 3 What is the charge for using Dx gateway? (533) # Answer no How many routes can be advised per TGW through the DX gateway? (539) # Answer 100 (bgp) what is ECMP (251, 556) - only support BGP # Answer load balancing between different dx connection the same dx location how to setup Active-Active with public vif and public ASN (554, 558) # Answer advertise same prefix route use ECMP (by aws) how to setup Active-Active with private vif and public ASN (558) # Answer no how to setup Active-Passive with public vif and public ASN (561) # Answer advertise same prefix route as_path / med for incoming traffic to aws local-pref for outgoing traffic to aws how to setup Active-Passive with public vif and public ASN (561) # Answer advertise more specific prefix route how to constrain the adviser routes to on-premises with public vif (567) # Answer on-premises routes advertise the bgp communities to ask aws to advertise specific routes only (region / inter-region / global) how to filter the adviser routes from aws in on-premises with public vif (567) # Answer based on the bgp communities from aws. it tells where the route comes from what is the routing policy with private vif (577, 582) # Answer advise more specific route bgp: as_path / med or local preference (same dx location) bgp-communities What is LAG? (584) # Answer aggregate the dx connection to a larger bandwidth How many DX connections can a LAG have? (584) # Answer 4 What is the bandwidth requirement of a LAG? (584) # Answer same bandwidth and on the same customer device What is the operation connection attribute in LAG? (589) # Answer no of connection that aws treating the LAG is up and running how to increase the resiliency of DX? (3 methods) (590) # Answer multi device (development) multi location (resiliency) multi device and multi dx location (max) one more -\u003e dx over vpn how to encrypt DX traffic? (2 methods) (601) # Answer dx over vpn (ipsec) macsec When will the Dx connection be charged? (Hosted and Dedicated connection) (621) # Answer dedicated: once aws create the connection for you (aws did their job, give you the LOA) hosted: once you accept the connection What will be charged in Dx? (615) # Answer data out and port charge Who will be charged for the DTO fee? (622) # Answer the resource owner to send out the traffic ELB # What Layer ALB, CLB, NLB, GLB belong to? (639) # Answer alb 7 clb 7/4 nlb 4 glb 3 What protocol does ALB, CLB, NLB, GLB support? (641) # Answer alb http https clb http https tcp tls nlb tcp udp tls glb ip Why does NLB have less latency than ALB? (652) # Answer do not need to read the packets (we only check the ip port and protocol) What kind of target can be used in the ALB and NLB target group? (653) # Answer alb: ip ec2 lambda nlb: ip ec2 What kind of protocol is supported in the health check? (653) # Answer tcp What are the connection idle timeouts of ALB, NLB, CLB? (657) # Answer alb 60 nlb 350for tcp 120 for udp cannot be configured clb 60 What routing algorithm supported by ALB, NLB, CLB (659) # Answer alb, clb: least outstanding, round-robin nbl: hash flow hash How to keep the client ip in ALB, NLB (671, 672) # Answer alb \u0026 clb: x-forwarded-For nlb: proxy protocol 2 clb: proxy protocol 1 How to keep the client on the same instance for a period of time? (661) # Answer sticky session What is Cross-Zone Load Balancing? (663) # Answer by default enabled in alb, the traffic are load balanced to all instances evenly disabled in nlb disabled in clb (api), enabled in clb (console) What is SNI and which ELB does it support? (667) # Answer sni = domain name in certmulti ssl certs in one web server alb and nlb support it What is Connection Draining (670) # Answer remove opening connections from died instance (auto scaling group) Cloudfront # What services / source can be Cloudfront\u0026rsquo;s origin? (683) # Answer any public ip s3 media package and mediastore container Why public access is needed in cloudfront\u0026rsquo;s origin (683) # Answer Cloudfront does the health check from the internet What is the origin group (687) # Answer primary \u0026 secondary of group of origin for failover one primary and one secondary origin for failover create a origin group, select primary \u0026 secondary origin, then select the group in cloudfront origin How to change custom header / behaviour in Cloudfront? (698) # Answer use lambda@edge or cloudfront actionfunction How to restrict the content to specific geolocation? (696) # Answer map the origin to geolocation the allow / block list choose either whitelist / blacklist and the countries for restriction What is AWs global accelerator? What does it work (709) # Answer use anycast(2ips). will send traffic to aws edge locations and then reach to your service through aws backbone network What is the difference between AWS global accelerator and cloudfront? (711) # Answer cloudfront only supports http / https. accelerator using a transport layer so it can be udp volip mqtt Route53 # What is the longest and shortest TTL in route53 (721) # Answer 60s 24hr What are the alias targets in route53 (724) # Answer s3 beanstalk vpce accelerator api gateway elb cloudfront other route53 record(same hosted zone) How to bind an RDS DB instance in route53 (748) # Answer cname Routing Policies in Route53? (725) # Answer simple multivalue failover latency weighted geolocation geoproximty How does Route53 perform the health check in a private VPC? (732) # Answer setup a health check with cloudwatch alert How to setup hybrid DNS (754) # Answer use route53 forwarderresolver. create outbound(forward on-premise domain query to on-premise dns) and inbound (for on-premise forwarderresolver to forward the query to aws vpc domain) endpoint How does AWS ensure the HA in route53? (807) # Answer random sharding anycast striping Network firewall # What layer SG, NACL, network firewall, WAF, shield? # Answer sg = 7 3/4 nacl = 4 3/4 network firewall 7-4 7-3 waf = 7 shield = 3 What is the VPC level in SG and NACL? (766) # Answer sg = vpc instance nacl = subnet When should we use nacl instead of sg? (768, 769) # Answer to block something (sg cannot block traffic) When should we use WAF instead of lacl ? (770) # Answer block many ips / handle ddos block ip for the application cloudfront / alb behind Stateless and stateful of sg, nacl, network firewall, shield? (773) # Answer sg = stateful nacl = stateless network firewalld = both (stateless -\u003e stateful) shield = stateless How can a SYN cookie prevent DDos in packet flooding? # Gateway Load balancer # what is the use case of gateway load balancer (813) # Answer applicane (ip ec2) what port does gateway load balancer need for GENEVE (817) # Answer GENEVE UDP 6081 Other # Centralised VPC Interface endpoint 295 # access s3 with endpoint # use a private link to access a web server (NLB/EC2) # VPC peering connections (125) # VPC peering allows using Security group (301) # how many ip can BYOIP bring in (90) # How many VPC CIDRs can a VPC have? (72) # dns of vpc .2 169.254.169.253 (117) # Why is edge location safe? (711) # What must be enabled to use route53 in vpc? # Demo (266)\nHand-on Labs # https://www.whizlabs.com/learn/course/aws-advanced-networking-speciality/195\nEC2 # 30 CloudFront # 1 2 10 ALB # 3 4 5 WAF # 11 12 ACL # 18 29 VPN # 21 Endpoint # 23 24 25 Flow Log # 26 Container # 28 VPCs Connectivities # Transit gateway peering attachments What is VPC peering? Transit Gateway vs VPC peering Solution / Situation same region cross account cross region VPC Peering OK OK OK TGW Peering OK OK OK TGW (attach VPC) OK OK NO AWS Managed VPN OK OK OK EC2 Based VPN OK OK OK Private Link is something like tgw peer but for specific service only. It can cross regions and accounts.\nScope # Resource Scope VGW VPC TGW VPC NLB VPC ALB VPC Route53 VPC NAT Subnet Common Pattern # Routing (582) Site-to-Site Connection routing # Static - Active/ Active Tunnels (349) Static - Active/ Passive Tunnels (350) Dynamic - Active/ Active Tunnels (351) TGW # Centralised NAT gateway (241) Centralised NAT instance (243) Centralised NAT instance + VPN (244) Centralised VPC interface endpoint + VPN (245) Hybrid VPN (247) Hybrid DX (248) VPC Peering # Failed cases # Page Title 207 CIDR overlap or transitive routing 208 from DX or VPN 209 to IGW Site-to-site VPN / DX with VGW # Use cases # Page Title 335 Site-to-site Connection with NAT-Traversal 361 Single Site-to-site Connection with VGW 363 Multiple Site-to-site Connection with VGW 365 Redundant VPN connections for HA Failed cases # Page Title 340 Site-to-site VPN to IGW 341 Site-to-site VPN to NAT 343 Site-to-site VPN to VPC Peering 344 Site-to-site VPN to VPC Gateway endpoint Successful cases # Page Title 342 Site-to-site VPN to NAT Instance 345 Site-to-site VPN to VPC Interface endpoint 345 Site-to-site VPN to on-premise NAT to Internet Site-to-site VPN with TGW # Page Title 362 Multiple Site-to-site Connection with TGW VPN CloudHub # 369 EC2 Based VPN # Page Title 375 Single instance 376 HA 378 Horizontal scaling - VPN EC2 per subnet 379 Horizontal scaling - Split traffic Transit VPC # Page Title 391 Transit VPC 398 global regions with single Transit Hub 398 global regions with multiple Transit Hub (GRE) 549 Direct Connect and Transit VPC DX # Use cases # Page Title 591 VPN as a backup 592 Dual Devices 593 Dual locations 594 Dual locations with DX connection backup 602 VPN over DX Failed cases # Page Title 530 DX Gateway with multiple customer sites 542 DX Gateway with multiple TGWs Successful cases # Page Title 546 DX Gateway with multiple customer sites Network Firewall # Use cases # Centralised: single firewall subnet / vpc. connect with tgw (791) Distributed: firewall subnet per vpc (790) Limitation # VPC Limit\nPeering # VPC Peering: 125\nRouting # Resource Limit Private VIFs 100 Public VIFs 1000 MTU (159) # Resource Limit VPC 9001 VPC Peering 1500 DX 9001 TGW (to DX) 8500 TGW (to VPN) 1500 VPC Endpoint 1500 NAT 1500 IGW 1500 VPN 1500 Bandwidth # Quotas for your TGW Aggregated throughput limit for VGW NAT gateways VPC (180) # Resource Limit VPC Peer no IGW no NAT 5-45 Gbps TGW depends VGW 1.25Gbps VGW (to DX) depends on DX TGW # 1 VPN can have 2 tunnels Resource Limit per resource VPC 50Gbps DX 50Gbps TGW 50Gbps VPN tunnel 1.25Gbps VPN Tunnel # 1.25Gbps\nEC2 # 100G networking in AWS, a network performance deep dive EC2 network performance rules:\nsingle flow limit within regions and other Situation Limit w/i the region 100% to other regions 50% igw 50% dx 50% Single Flow # Situation Limit w/i placement group 10Gbps other 5Gbps Aggregated # General Purpose Network Performance Situation Limit w enhanced networking 100Gbps w/o enhanced networking 25Gbps Network Driver Limit Intel 82599VF 10Gbps ENA 100Gbps EFA 100Gbps ","date":"26 November 2021","externalUrl":null,"permalink":"/posts/exam-aws-networking/","section":"Posts","summary":"Resources # Exam Landing Page Sample Questions Exam Guide udemy course udemy mock exam whizlabs test \u0026amp; hand-on labs TODO # take notes revision whizlabs mocks (by topic) mock 1 (udemy) mock 2,3,4 (whizlabs) examtopic udemy quiz + exercise Revision # The numbers below are the page no of the udemy course\u0026rsquo;s pdf: AWS Certified Networking Specialty Slides v1.","title":"AWS Certified Advanced Networking - Specialty","type":"posts"},{"content":" Trust Over IP: Hyperledger Ursa, Indy, and Aires # https://www.youtube.com/watch?v=FfuhlF9ZYPM\nNotes # prove the credential without CA trusted issuer (eg: gov) issues the identity token (eg: driving licence) -\u0026gt; send it to a crypto account (like sending btc to someone) on the blockchain, it shows a record that the account owns that licence. (like you can check your btc balance / NFS on chain explorer) anyone can request that you own that account (like an approval transaction from dapp). You can decide to share your data to what extent send the data to requester and it can be verified as it is signed by issuer blockchain is used to prove the ownership of your identities data can be stored off chain, sign it and verify with issuer\u0026rsquo;s account hacking personal accounts become useless -\u0026gt; not cost effective, you can hack a single person each time stealing / leading personal data is useless -\u0026gt; no proof, no one will accept Example # british columbia\nComprehensive review of 21 use cases of Hyperledger # https://youtu.be/VQTARQSuUFU?t=1440\nUse cases (some are still in proof of concept) # end to end tracking (supply chain, different parties can participate eg: bank insurance company consultant retail\u0026hellip;) 2.identity management - eg: allow 3rd parties to access your data with requests, prevent fraud, credit checking, hash your passport/fingerprint so you can verify it is your identity contract automation- like promotion in company (HR), consultant contract voting tokenisations - cross parties reward program(asiamiles), assets management (use in trading, convert different kinds of assets like house stock cash to token) immutable record- law enforcement investigation, accounting, credit check, insurance claim, medical history, law\u0026hellip; Hyperledger Fabric Business Use Cases # https://youtu.be/1ps4PJtFRfY?t=544\nIBM food trust # safety sustainability cost optimization (inventory / transportation ) tracking Tradelens -\u0026gt; (supply chain for trading) # costs 1.8T / year -\u0026gt; potential saving ~10% costs IBM blockchain world wire # world wire solution with blockchain use stellar protocol Singapore Exchange: Clearing Financial Transactions on Amazon Managed Blockchain # https://www.youtube.com/watch?v=nUre2ELySdo\nblockchain is used to handle the payment and settlement of securities\nthe method they used called DVP (delivery versus payment) = a way to ensure cash and securities can simultaneously exchange\nclient can be\nbuyer = buy securities with eth seller = sell securities with eth In the architecture,\nhyperledger = securities eth client = cash Transaction will come across with both ledgers\narbitrator and observer with receive activities on chain\narbitrator = mediator = handle dispute observer = regulator = do regulatory report Pros with AWS Managed Blockchain # no need to build the ledger by themself focus on the smart contract a single blockchain network can effectively work across multiple parties An Overview on Blockchain Services from AWS # https://www.youtube.com/watch?v=WAIOBeQA2QQ\nPros of blockchain network # ledger database (immutable transaction) consensus decentralisation smart contract ease for audit Hyperledger fabric VS ETH # authentication access control easy to add user easy to duel with settlement (bank) faster than centialization private ledger use case # trading supply case Enhanced Transparency and Frictionless Supply Chain with Blockchain # https://www.youtube.com/watch?v=2NqbUb4rT88\u0026list=WL\u0026index=46\nB2B Business\nbuy and sell order between Bottlers\nvisibility cross supply chain reduce manual process and communicate between sellers and buyers dispute resolution (smark contract) -\u0026gt; I think we can use TimeLock to do partial transaction easy to scale no need to build a different api for each system (mesh network n(n-1)/2) ","date":"26 November 2021","externalUrl":null,"permalink":"/posts/blockchain-use-case/","section":"Posts","summary":"Trust Over IP: Hyperledger Ursa, Indy, and Aires # https://www.youtube.com/watch?v=FfuhlF9ZYPM\nNotes # prove the credential without CA trusted issuer (eg: gov) issues the identity token (eg: driving licence) -\u0026gt; send it to a crypto account (like sending btc to someone) on the blockchain, it shows a record that the account owns that licence.","title":"Blockchain use cases","type":"posts"},{"content":"","date":"26 November 2021","externalUrl":null,"permalink":"/tags/cka/","section":"Tags","summary":"","title":"cka","type":"tags"},{"content":"","date":"26 November 2021","externalUrl":null,"permalink":"/tags/etcd/","section":"Tags","summary":"","title":"etcd","type":"tags"},{"content":" with IAM # user -\u0026gt; with API_KEY \u0026amp; API_TOKEN audience attached iam role (eg: ec2 instance) update the iam role and user mapping in configmap aws-auth OIDC # OIDC built in at the eks cluster\nOICD can request AWS IAM to issue the web identity token the token will be used to assume role and access AWS service Item Value OICD oidc.hhuge9.com IAM user hugotse OIDC user kevintse IAM role S3Admin To assume a role in EKS # Associate OIDC to AWS IAM - that why it is called associate-iam-oidc-provider. Create an role which has a trust relationshop to the service account from OIDC to assume with Web Identity Token - so who in OIDC can use web identity token to access aws service Difference between assume-role and assume-role-with-web-identity # hugotse can use aws sts assume-role to assume S3Admin kevintse can use aws sts assume-role-with-web-identity with web-identity to assume S3Admin How a pod to access aws service # EKS has some webhooks, it will monitor our changes on cluster\nYou can create a custom admission webhooks, deny / modify specfic requests of api servers\nWhen the admission webhook detects a service account associated to a pod and that account annotate with iam arn, it will\nadmission webhook -\u0026gt; send OICD token to AWS IAM AWS IAM -\u0026gt; verify the OICD token and return web identity token to the webhook admission webhook save the web identity token and mount it as a volume in the pod Application -\u0026gt; use the web identity token with aws sts assume-role-with-web-identity to access AWS Service Where does the OICD token come from? # with Service Account Token Volume Projection, any pod attached with a service account will automatically mount with a OICD token\nWays to assume role # if you are an iam user, use access key if you are in aws services (like ec2 instance), attach iam role or with oidc, use web identity token oidc user get web identity token from aws iam with oidc token oidc user use the token to assume role Permission needs to assume role # by a iam user # user hugotse have right to use assume-role (call aws sts assume-role) a role has a trusted relationship to user hugotse to assume role by a oicd user # oidc user kevintse have right to use assume-role-with-web-identity a role has a trusted relationship with oicd user kevintse to assume role with web identity ","date":"26 November 2021","externalUrl":null,"permalink":"/posts/k8s-authentication/","section":"Posts","summary":"with IAM # user -\u0026gt; with API_KEY \u0026amp; API_TOKEN audience attached iam role (eg: ec2 instance) update the iam role and user mapping in configmap aws-auth OIDC # OIDC built in at the eks cluster","title":"k8s Authentication","type":"posts"},{"content":" Things to turn off before exam # Bandwidth saving # google drive sync any backgorund updates wifi connection in other devices Prevent from autoupdate # software update (mbp, iphone) Others # firewall disable all chrome plugins backup plan # sim card - two extra network: EE, giffgaff spare computer (MBP) Tips # When use wc -\u0026gt; be careful the header, need total - 1 # Store temporary commands / note / result in a fixed location like /root/tmp # Even in remote host; we use that fixed location to store our results then get back the result from it # remote$ cat tmp remote$ exit remote$ exit local$ ssh remote cat tmp Do not use tmux # hard to copy and paste and scroll\nOne question one yml # Name with q\u0026lt;no\u0026gt;.yml like q10.yml is the yml of 10th question\nWrite all resources on same yml in same question # Set .vimrc # set sw=2 ts=2 et set hidden Use alias # export do=\u0026#39;--dry-run=client -o yaml\u0026#39; export now=\u0026#39;--sort-by=\u0026#34;{.metadata.creationTimestamp}\u0026#34;\u0026#39; Use variable in command # n=namespace nn=name i=image s=server Then you will find that we can reuse many command history. like\nCreate po yml # k run -n $n $nn --image $i $do Get po # k get po $nn -n $n Test service account # k auth can-i --as=system:serviceaccount:$n:$nn When working on next question, start a new vim (kill the old session) # Keep a single vim session in bash # back and fore from vim and bash many times is needed using multiple vim session are so confusing that also why working on single yaml per question is encouraged Create (touch /opt/xxxxx/q11/answer) the answer file first # needed to submit anser to specfic file just touch the file -\u0026gt; open it with vim -\u0026gt; put it in background (c-z) ","date":"26 November 2021","externalUrl":null,"permalink":"/posts/exam-cka/","section":"Posts","summary":"Things to turn off before exam # Bandwidth saving # google drive sync any backgorund updates wifi connection in other devices Prevent from autoupdate # software update (mbp, iphone) Others # firewall disable all chrome plugins backup plan # sim card - two extra network: EE, giffgaff spare computer (MBP) Tips # When use wc -\u0026gt; be careful the header, need total - 1 # Store temporary commands / note / result in a fixed location like /root/tmp # Even in remote host; we use that fixed location to store our results then get back the result from it # remote$ cat tmp remote$ exit remote$ exit local$ ssh remote cat tmp Do not use tmux # hard to copy and paste and scroll","title":"Note for CKA Exam","type":"posts"},{"content":" Control Panel svr1 svr2 svr3 svr1 # Stop k8s # systemctl stop kubelet crictl rm -f $(crictl ps -q) start the etcd in single node # vim /etc/kubernetes/manifests/etcd.yaml - --force-new-cluster=true Add svr2 etcd to cluster # kubectl exec -it etcd-svr1.hhuge9.com -nkube-system -- sh -c \u0026#39;ETCDCTL_API=3 etcdctl --endpoints 127.0.0.1:2379 --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key --cacert=/etc/kubernetes/pki/etcd/ca.crt member add svr2.hhuge9.com --peer-urls=\u0026#34;https://192.168.0.102:2380\u0026#34;\u0026#39; Save the output # export ETCD_NAME=\u0026#34;svr2.hhuge9.com\u0026#34; export ETCD_INITIAL_CLUSTER=\u0026#34;svr1.hhuge9.com=http://192.168.0.101:2380,svr2.hhuge9.com=http://192.168.0.102:2380\u0026#34; export ETCD_INITIAL_CLUSTER_STATE=existing systemctl start kubelet svr2 # Stop k8s and remove etcd data # systemctl stop kubelet crictl rm -f $(crictl ps -q) rm -rf /var/lib/etcd Update etcd config (with the output in svr1) # vim /etc/kubernetes/manifests/etcd.yaml - --initial-cluster=svr1.hhuge9.com=https://192.168.0.101:2380,svr2.hhuge9.com=https://192.168.0.102:2380 - --initial-cluster-state=existing systemctl start kubelet ","date":"26 November 2021","externalUrl":null,"permalink":"/posts/etcd/","section":"Posts","summary":"Control Panel svr1 svr2 svr3 svr1 # Stop k8s # systemctl stop kubelet crictl rm -f $(crictl ps -q) start the etcd in single node # vim /etc/kubernetes/manifests/etcd.yaml - --force-new-cluster=true Add svr2 etcd to cluster # kubectl exec -it etcd-svr1.","title":"Recover etcd","type":"posts"},{"content":"","date":"21 October 2021","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":" Procedures # create private key create csr (distinguished_name) sign csr by CA (x509v3) Commands # Gen private key # openssl genrsa \\ -out ca.key Self signed crt # openssl req -x509 \\ -config openssl.cnf -new \\ -out ca.crt \\ -key ca.key gen csr # openssl req \\ -config openssl.cnf -new \\ -out server.csr \\ -key server.key sign crt # openssl x509 \\ -extfile openssl.cnf \\ -extensions svr_cert \\ -in server.csr \\ -out server.crt \\ -CA ca.crt \\ -CAkey ca.key \\ -CAcreateserial When the CA is first time to sign a cert, you need to create serial file ca.crl with -CAcreateserial Next time we sign a new cert, we will need to update the serial file with -CAserial ca.crl Outputs # Inspect the cert / key # openssl rsa -in ca.key -noout -text openssl x509 -in ca.crt -noout -text extract the pub key # openssl rsa -in ca.key -pubout openssl x509 -in ca.crt -noout -pubkey Verify # crt is issue by ca # openssl verify -CAfile ca.crt server.crt key and crt are match (modulus) # openssl rsa -in ca.key -nout -modulus | openssl md5 openssl x509 -in ca.crt -nout -modulus | openssl md5 trust self signned ca # we need to place the ca.crt to a location and call command below to trusted the cert (the programme will create a soft link)\nwe can man update-ca-certificate to find the location. eg: /usr/local/share/ca-certificates\ncp ca.crt /usr/local/share/ca-certificates update-ca-certificate trusted the cert in firefox and chrome acutally work but we need to restart the pc..\ntwo output should be same\nCAxxxx \u0026lt;- Capital in CA, lowercase in next word -in -out \u0026lt;- represent the module\nConfig # cp /etc/ssl/openssl.cnf openssl.cnf vi openssl.cnf Remove all empty and commended lines and start update the openssl.cnf\nHOME = . oid_section = new_oids [ new_oids ] tsa_policy1 = 1.2.3.4.1 tsa_policy2 = 1.2.3.4.5.6 tsa_policy3 = 1.2.3.4.5.7 [ ca ] [ CA_default ] policy = policy_match [ policy_match ] countryName = match stateOrProvinceName = match organizationName = match organizationalUnitName = optional commonName = supplied emailAddress = optional [ policy_anything ] countryName = optional stateOrProvinceName = optional localityName = optional organizationName = optional organizationalUnitName = optional commonName = supplied emailAddress = optional [ req ] default_bits = 2048 default_keyfile = privkey.pem distinguished_name = req_distinguished_name attributes = req_attributes string_mask = utf8only [ req_distinguished_name ] 0.organizationName = Organization Name (eg, company) 0.organizationName_default = hhuge9.com commonName = Common Name (e.g. server FQDN or YOUR name) commonName_max = 64 [ req_attributes ] [ v3_req ] basicConstraints = CA:FALSE keyUsage = critical, digitalSignature, keyEncipherment [ v3_ca ] basicConstraints = critical,CA:true keyUsage = critical, keyCertSign subjectKeyIdentifier=hash authorityKeyIdentifier=keyid:always,issuer [ svr_cert ] basicConstraints=CA:FALSE keyUsage = critical, digitalSignature, keyEncipherment extendedKeyUsage = serverAuth subjectKeyIdentifier=hash authorityKeyIdentifier=keyid,issuer [ usr_cert ] basicConstraints=CA:FALSE keyUsage = critical, digitalSignature, keyEncipherment extendedKeyUsage = clientAuth subjectKeyIdentifier=hash authorityKeyIdentifier=keyid,issuer The most important setting is session: v3_req v3_ca svr_cert and usr_cert\nNormally, Cert need req_distinguished_name session only.\nFor example, we use domain in CN (Common Name) in web server cert\nIn k8s, we will use CN as username, O (Orgnazation) as group for auth\nIt can be done by commands without config.\nThose information is defined in req stage. (not sign stage)\nIf you want to use config, you can:\n[ req_distinguished_name ] 0.organizationName = Organization Name (eg, company) 0.organizationName_default = hhuge9.com commonName = Common Name (e.g. server FQDN or YOUR name) commonName_max = 64 Or in command, we can\nopenssl req \\ -config openssl.cnf -new \\ -out server.csr \\ -key server.key \\ -subj \u0026#39;/O=hhuge9.com/CN=redis.hhuge9.com\u0026#39; But when we want to create a CA, cert for TLS auth. We need x503v3 extension to define additional information.\nThat is why we see -config and -extensions in sign command.\nTo check the documentation of the extension, we can man openssl x509 \u0026gt; go to bottom \u0026gt; Also see x509v3_config \u0026gt; man x509v3_config\nCSR do not need the extension. -config is optional.\nThis three are most important:\nFor non CA\nbasicConstraints = CA:FALSE keyUsage = critical, digitalSignature, keyEncipherment For CA\nbasicConstraints = CA:TRUE keyUsage = critical, keyCertSign For server cert\nextendedKeyUsage = serverAuth For client cert\nextendedKeyUsage = clientAuth If we want to contraint the cert be used by specfic host, we can add subjectAltName\nsubjectAltName = @alt_name [ alt_name ] DNS.0 = redis-master.hhuge9.com DNS.1 = redis01.hhuge9.com IP.0 = 192.168.2.100 ","date":"21 October 2021","externalUrl":null,"permalink":"/posts/openssl/","section":"Posts","summary":"Procedures # create private key create csr (distinguished_name) sign csr by CA (x509v3) Commands # Gen private key # openssl genrsa \\ -out ca.key Self signed crt # openssl req -x509 \\ -config openssl.","title":"How to create ssl cert for tls auth","type":"posts"},{"content":"","date":"21 October 2021","externalUrl":null,"permalink":"/categories/k8s/","section":"Categories","summary":"","title":"k8s","type":"categories"},{"content":"","date":"21 October 2021","externalUrl":null,"permalink":"/tags/openssl/","section":"Tags","summary":"","title":"openssl","type":"tags"},{"content":"","date":"21 October 2021","externalUrl":null,"permalink":"/categories/openssl/","section":"Categories","summary":"","title":"openssl","type":"categories"},{"content":"","date":"9 October 2021","externalUrl":null,"permalink":"/tags/chrome/","section":"Tags","summary":"","title":"chrome","type":"tags"},{"content":"I am preparing the CKA exam. The exam requires us to complete tasks using web terminal on Chrome.\nMy concern is that Ctrl+w is a common shortcut in terminal (delete word) but it also is the \u0026ldquo;close tab\u0026rdquo; shortcut of chrome. (Windows)\nThere is a workaround on this issue. Comment under this article mentioned that we can define ctrl+w in chrome://extensions/shortcuts to override the default behavior on ctrl+w. This will prevent the tab be closed when hit ctrl+w accidently.\n","date":"9 October 2021","externalUrl":null,"permalink":"/posts/disable-ctrl-w-on-chrome/","section":"Posts","summary":"I am preparing the CKA exam. The exam requires us to complete tasks using web terminal on Chrome.\nMy concern is that Ctrl+w is a common shortcut in terminal (delete word) but it also is the \u0026ldquo;close tab\u0026rdquo; shortcut of chrome.","title":"Disable Ctrl+w on Chrome","type":"posts"},{"content":"","date":"9 October 2021","externalUrl":null,"permalink":"/categories/other/","section":"Categories","summary":"","title":"other","type":"categories"},{"content":"https://www.w3.org/TR/NOTE-datetime\n2019-07-16T12:00:00Z = utc time\nYYYY-MM-DDThh:mm:ss.sTZD\nTZD can be Z or +hh:mm or -hh:mm\nZ = UTC\n+08:00 = UTC+8\n","date":"17 September 2021","externalUrl":null,"permalink":"/posts/time-format/","section":"Posts","summary":"https://www.w3.org/TR/NOTE-datetime\n2019-07-16T12:00:00Z = utc time\nYYYY-MM-DDThh:mm:ss.sTZD\nTZD can be Z or +hh:mm or -hh:mm\nZ = UTC\n+08:00 = UTC+8","title":"Time Format","type":"posts"},{"content":"Facing this warning in a repo\nwarning: ignoring broken ref refs/remotes/origin/HEAD https://stackoverflow.com/questions/45811971/warning-ignoring-broken-ref-refs-remotes-origin-head\nAfter reading this article, seem the reference of refs/remotes/origin/HEAD is broken.\n$ git symbolic-ref refs/remotes/origin/HEAD refs/remotes/origin/master I have changed the branch from master to main before. master branch is no longer exist. I think that is the reason of the warning.\ngit symbolic-ref refs/remotes/origin/HEAD refs/remotes/origin/main Then the warning is fixed.\n","date":"25 August 2021","externalUrl":null,"permalink":"/posts/broken-git-ref/","section":"Posts","summary":"Facing this warning in a repo\nwarning: ignoring broken ref refs/remotes/origin/HEAD https://stackoverflow.com/questions/45811971/warning-ignoring-broken-ref-refs-remotes-origin-head\nAfter reading this article, seem the reference of refs/remotes/origin/HEAD is broken.\n$ git symbolic-ref refs/remotes/origin/HEAD refs/remotes/origin/master I have changed the branch from master to main before.","title":"Broken Git Ref","type":"posts"},{"content":"","date":"25 August 2021","externalUrl":null,"permalink":"/tags/git/","section":"Tags","summary":"","title":"git","type":"tags"},{"content":"","date":"25 August 2021","externalUrl":null,"permalink":"/categories/git/","section":"Categories","summary":"","title":"git","type":"categories"},{"content":"Recently find that it is too slow to start a new bash.\nAfter debugging, I realize that nvm is the main reason for that.\nFinally, disabled nvm to solve the problem.\n","date":"25 August 2021","externalUrl":null,"permalink":"/posts/slow-bash-startup/","section":"Posts","summary":"Recently find that it is too slow to start a new bash.\nAfter debugging, I realize that nvm is the main reason for that.\nFinally, disabled nvm to solve the problem.","title":"Slow Bash Startup","type":"posts"},{"content":"I used the emacs keybind to maniplicate the string on command line but it does not work at chrome on macos.\nI am unhappy with that until I find the solution on this post\nhttps://stackoverflow.com/questions/20146972/is-there-a-way-to-make-alt-f-and-alt-b-jump-word-forward-and-backward-instead-of sudo mkdir -p ~/Library/Keybindings/ sudo vi ~/Library/Keybindings/DefaultKeyBinding.dict { \u0026#34;~d\u0026#34; = \u0026#34;deleteWordForward:\u0026#34;; \u0026#34;^w\u0026#34; = \u0026#34;deleteWordBackward:\u0026#34;; \u0026#34;~f\u0026#34; = \u0026#34;moveWordForward:\u0026#34;; \u0026#34;~b\u0026#34; = \u0026#34;moveWordBackward:\u0026#34;; } ","date":"20 August 2021","externalUrl":null,"permalink":"/posts/move-forward-backward-in-chrome/","section":"Posts","summary":"I used the emacs keybind to maniplicate the string on command line but it does not work at chrome on macos.\nI am unhappy with that until I find the solution on this post","title":"Move Forward Backward in Chrome","type":"posts"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/tags/import/export/","section":"Tags","summary":"","title":"import/export","type":"tags"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"}]